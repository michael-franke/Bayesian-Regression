[
  {
    "objectID": "practice-sheets/02d-linReg-multiple-catPredictors.html",
    "href": "practice-sheets/02d-linReg-multiple-catPredictors.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models)."
  },
  {
    "objectID": "practice-sheets/02d-linReg-multiple-catPredictors.html#exercises-metric-and-categorical-predictors",
    "href": "practice-sheets/02d-linReg-multiple-catPredictors.html#exercises-metric-and-categorical-predictors",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercises: metric and categorical predictors",
    "text": "Exercises: metric and categorical predictors\nHere is an aggregated data set dolphin_agg2 for you.\n\n# aggregate\ndolphin_agg2 <- dolphin %>% \n  filter(correct == 1) %>% \n  group_by(exemplar, group, condition) %>% \n  dplyr::summarize(MAD = median(MAD, na.rm = TRUE),\n                   RT = median(RT, na.rm = TRUE)) %>% \n  mutate(log_RT = log(RT))\n\n\n\n\n\n\n\nExercise 2a\nRun a model predicting MAD based on standardized log_RT, group, condition, and their three-way interaction. Set a seed = 999.\n\n\n\n\n\nShow solution\n# standardize\ndolphin_agg2$log_RT_s <- scale(dolphin_agg2$log_RT, scale = TRUE)\n\n# model\nmodel3 = brm(\n  MAD ~ log_RT_s * group * condition, \n  data = dolphin_agg2,\n  iter = 2000,\n  chains = 4,\n  seed = 999\n  )\n\n\n\n\n\n\n\n\nExercise 2b\nLook at the output. Extract posterior means and 95% CrIs for the following predictor level combinations. One row corresponds to one concrete combination of levels. (Tip: check your results by plotting them against the data)\n\nCombination1: log_RT_s == 0; group == click; condition == Atypical\nCombination2: log_RT_s == 0; group == touch; condition == Atypical\nCombination3: log_RT_s == 1; group == touch; condition == Typical\nCombination4: log_RT_s == 2; group == touch; condition == Atypical\n\n\n\n\n\n\nShow solution\nposteriors3a <- model3 %>%\n  spread_draws(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`,\n               `b_log_RT_s:grouptouch:conditionTypical`\n               ) %>% \n  mutate(Combination1 = b_Intercept + (0 * b_log_RT_s),\n         Combination2 = b_Intercept + (0 * b_log_RT_s) + b_grouptouch,\n         Combination3 = b_Intercept + (1 * b_log_RT_s) + b_grouptouch + \n           b_conditionTypical + `b_grouptouch:conditionTypical` + (1 * `b_log_RT_s:grouptouch`) + \n           (1 * `b_log_RT_s:conditionTypical`) + (1 * `b_log_RT_s:grouptouch:conditionTypical`),\n         Combination4 = b_Intercept + (2 * b_log_RT_s) + b_grouptouch + \n           (2 * `b_log_RT_s:grouptouch`)) %>% \n  dplyr::select(Combination1, Combination2,\n                Combination3, Combination4) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  group_by(parameter) %>% \n  summarise(mean_posterior = mean(posterior),\n            `95lowerCrI` = HDInterval::hdi(posterior, credMass = 0.95)[1],\n            `95higherCrI` = HDInterval::hdi(posterior, credMass = 0.95)[2])\n\nposteriors3a\n\n\n\n\n\n\n\n\nExercise 2c\nDefine the following priors and run the model3 again:\n\nlog_RT_s: student-t (df = 3, mean = 0, sd = 30)\ngrouptouch: student-t (df = 3, mean = 100, sd = 200)\nconditionTypical: student-t (df = 3, mean = 0, sd = 200)\nlog_RT_s:grouptouch: normal (mean = 0, sd = 30)\nlog_RT_s:conditionTypical: normal (mean = 0, sd = 30)\ngrouptouch:conditionTypical: student-t (df = 3, mean = 0, sd = 200)\nlog_RT_s:grouptouch:conditionTypical: student-t (df = 3, mean = 0, sd = 30)\n\n\n\n\n\n\nShow solution\npriors_model3 <- c(\n   set_prior(\"student_t(3,0,30)\", class = \"b\", coef = \"log_RT_s\"),\n   set_prior(\"student_t(3,100,200)\", class = \"b\", coef = \"grouptouch\"),\n   set_prior(\"student_t(3,0,200)\", class = \"b\", coef = \"conditionTypical\"),\n   set_prior(\"normal(0,30)\", class = \"b\", coef = \"log_RT_s:grouptouch\"),\n   set_prior(\"normal(0,30)\", class = \"b\", coef = \"log_RT_s:conditionTypical\"),\n   set_prior(\"student_t(3,0,200)\", class = \"b\", coef = \"grouptouch:conditionTypical\"),\n   set_prior(\"student_t(3,0,30)\", class = \"b\", coef = \"log_RT_s:grouptouch:conditionTypical\")\n)\n\n# model\nmodel3b = brm(\n  MAD ~ log_RT_s * group * condition, \n  data = dolphin_agg2,\n  iter = 2000,\n  chains = 4,\n  prior = priors_model3\n  )\n\n\n\n\n\n\n\n\nExercise 2d\nCompare the two posterior estimates from model3 and model3b. What has changed?\n\n\n\n\n\nShow solution\n# extract posteriors for model2\nposteriors3a <- model3 %>%\n  spread_draws(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% \n  select(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  group_by(parameter)\n\n# extract posteriors for model2b\nposteriors3b <- model3b %>%\n  spread_draws(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% \n  select(b_Intercept, b_log_RT_s,\n               b_grouptouch, b_conditionTypical,\n               `b_log_RT_s:grouptouch`, `b_log_RT_s:conditionTypical`,\n               `b_grouptouch:conditionTypical`, `b_log_RT_s:grouptouch:conditionTypical`) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  group_by(parameter)\n\n# plot posteriors for model2\nggplot(posteriors3a, aes(x = posterior, y = parameter)) + \n    # plot density \n    geom_halfeyeh(.width = 0.95) +\n    # add axes titles\n    xlab(\"\\nMAD\") +\n    ylab(\"\") +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n                 lty = \"dashed\") +\n    scale_x_continuous(limits = c(-250, 250))\n  \n# plot posteriors for model2b\nggplot(posteriors3b, aes(x = posterior, y = parameter)) + \n    # plot density \n    geom_halfeyeh(.width = 0.95) +\n    # add axes titles\n    xlab(\"\\nMAD\") +\n    ylab(\"\") +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n                 lty = \"dashed\") +\n    scale_x_continuous(limits = c(-250, 250))\n\n# ANSWER:The model output does not change much. Overall, the posteriors are a little tighter and closer to zero for model3b\n\n\n\n\n\n\n\n\nExercise 2a\nSuppose you have the following aggregated data set and want to run the following linear model: AUC ~ condition\n\n# aggregate\ndolphin_agg3 <- dolphin %>% \n  filter(correct == 1) %>% \n  group_by(subject_id, condition) %>%\n  dplyr::summarize(AUC = median(AUC, na.rm = TRUE)) \n\ndolphin_agg3$AUC_s <- scale(dolphin_agg3$AUC, scale = TRUE)\n\nDeviation code the effect of condition, such that the Intercept gives you the grand average of AUC_s and the coefficient for condition gives you the difference between Atypical + Typical exemplars. Check last week’s reading of Bodo Winter’s book again (or google).\nSpecify an agnostic prior for the effect of condition and run the model from above (set seed = 333).\n\n\n\n\n\nShow solution\n# contrast code condition\nc <- contr.treatment(2)\n\n# divide by 2 for it to represent the difference\nmy.coding <- matrix(rep(1/2, 2), ncol = 1)\nmy.simple <- c - my.coding\n\n# make factor\ndolphin_agg3$condition <- as.factor(dolphin_agg3$condition)\n# associate with contrast\ncontrasts(dolphin_agg3$condition) = my.simple\n\npriors_agnostic <- c(\n   # Atypical < Typical\n   set_prior(\"normal(0, 3)\", class = \"b\", coef = \"condition2\")\n)\n\nmodel4 = brm(\n  AUC_s ~ condition, \n  data = dolphin_agg3,\n  iter = 2000,\n  chains = 4,\n  seed = 333,\n  prior = priors_agnostic\n  )\n\n\n\n\n\n\n\n\nExercise 2f\nNow suppose you have three people who want to encode their subjective beliefs about whether and how group and condition affect AUC_s. To keep your solutions comparable, we assume prior beliefs are normally distributed and there are three types of beliefs:\n\nA strong belief in a directional relationship: The person assumes that there is a difference between two conditions (A>B). The mean of the assumed differences is 3 units of AUC_s with a SD of 0.5.\nAn agnostic belief in a directional relationship: Both A>B and B>A are plausible, but uncertainty is high. The mean of the most plausible distribution is 0 with a SD of 3, i.e. a rather wide distribution, allowing effects in both directions.\n\nHere are three researchers and their prior beliefs:\nMichael holds strong prior beliefs that Typical exemplars exhibit less curvature than Atypical exemplars.\nNina is agnostic about the effect of condition on AUC_s.\nAs opposed to Michael, Jones holds strong prior beliefs that Typical exemplars exhibit MORE curvature than Atypical exemplars.\nSpecify priors for Michael, Nina, and Jones, and run models (set seed = 323) for all of these scenarios. Look at the results (maybe plot the posteriors if that helps you) and briefly describe how the priors affected the posteriors.\n\n\n\n\n\nShow solution\npriors_Michael <- c(\n   # Atypical > Typical\n   set_prior(\"normal(-3, 0.5)\", class = \"b\", coef = \"condition2\")\n)\n\npriors_Nina <- c(\n   # Atypical < Typical\n   set_prior(\"normal(0, 3)\", class = \"b\", coef = \"condition2\")\n)\n\npriors_Jones <- c(\n      set_prior(\"normal(3, 0.5)\", class = \"b\", coef = \"condition2\")\n)\n\n\n# model\nmodel5_Michael = brm(\n  AUC_s ~ condition, \n  data = dolphin_agg3,\n  iter = 2000,\n  chains = 4,\n  seed = 333,\n  prior = priors_Michael\n  )\n\nmodel5_Nina = brm(\n  AUC_s ~ condition, \n  data = dolphin_agg3,\n  iter = 2000,\n  chains = 4,\n  seed = 333,\n  prior = priors_Nina\n  )\n\nmodel5_Jones = brm(\n  AUC_s ~ condition, \n  data = dolphin_agg3,\n  iter = 2000,\n  chains = 4,\n  seed = 333,\n  prior = priors_Jones\n  )\n\n# extract posteriors\n\n## Michael\nposteriors_Michael <- model5_Michael %>%\n  spread_draws(b_condition2) %>% \n  select(b_condition2) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  mutate(model = \"Michael\")\n  \n\n## Nina\nposteriors_Nina <- model5_Nina %>%\n  spread_draws(b_condition2) %>% \n  select(b_condition2) %>% \n  gather(key = \"parameter\", value = \"posterior\")  %>% \n  mutate(model = \"Nina\")\n\n## Jones\nposteriors_Jones <- model5_Jones %>%\n  spread_draws(b_condition2) %>% \n  select(b_condition2) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  mutate(model = \"Jones\")\n\n# add to one df\nposteriors_all <- rbind(posteriors_Michael, posteriors_Nina, posteriors_Jones)\n\n# plot posteriors for all models\nggplot(posteriors_all, aes(x = posterior, y = parameter, fill = model, color = model)) + \n    # plot density \n    geom_halfeyeh(.width = 0.95, alpha = 0.4) +\n    facet_grid(model~ .) +\n    # add axes titles\n    xlab(\"\\nAUC\") +\n    ylab(\"\") +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf, color = \"black\",\n                 lty = \"dashed\") +\n    scale_x_continuous(limits = c(-1.5, 0.5))\n\n#ANSWER: The agnostic Nina serves as a baseline. With a weakly informative prior, the likelihood dominates the posterior.\n# Michael has a strong preconception that Atypical exemplars elicit larger AUC values and in comparison to Jones, the posterior distribution is slightly shifted more toward negative values.\n# Jones has a strong preconception that Typical exemplars elicit larger AUC values and in comparison to Jones and Michael, the posterior distribution is slightly shifted AWAY from negative values.\n# Here, the priors, although encoding strong prior beliefs, have only little impact on the posteriors but do shift posteriors toward the priors."
  },
  {
    "objectID": "practice-sheets/00-preamble.html",
    "href": "practice-sheets/00-preamble.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nCode\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors()[c(1,3,4,5,2,6:14),\"hex\", drop = TRUE]\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}"
  },
  {
    "objectID": "practice-sheets/03b-GLM-exercises.html",
    "href": "practice-sheets/03b-GLM-exercises.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nCode\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors()[c(1,3,4,5,2,6:14),\"hex\", drop = TRUE]\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\nExercise 1: logistic regression\nUse the following data frame:\n\n# set up data frame\ndolphin <- aida::data_MT\ndolphin_agg <- dolphin %>% \n  filter(correct == 1) %>% \n  mutate(straight = as.factor(ifelse(prototype_label == \"straight\", 1, 0)),\n         log_RT_s = scale(log(RT)))\n\n\n\n\n\n\n\nExercise 1a\nPlot straight (straight == 1) vs. non-straight (straight == 0) trajectories (y-axis) against log_RT_s and color-code by group.\n\n\n\n\n\nShow solution\ndolphin_agg$straight_numeric <- as.numeric(as.character(dolphin_agg$straight))\n\nggplot(data = dolphin_agg) +\n  geom_point(aes(x = log_RT_s, y = straight_numeric, color = group), \n             # we add a little bit of jitter to make the points better visible\n             position = position_jitter(height = 0.02), alpha = 0.2) \n\n\n\n\n\n\n\n\nExercise 1b\nRun the appropriate generalized linear model in brms that predicts straight vs. non-straight trajectories based on group, log_RT_s, and their two-way interaction.\n\n\n\n\n\nShow solution\nGlmMdl <- brm(straight ~ log_RT_s * group, \n                 dolphin_agg, cores = 4,\n              family = \"bernoulli\",\n              seed = 123)\nGlmMdl\n\n\n\n\n\n\n\n\nExercise 1c\nDescribe the model predictions based on the posterior means of the population coefficients.\n\n\n\n\n\nShow solution\n# Answer: The model predicts that the log odds for the mean log_RT_s in the click group (Intercept = reference level) is 0.86. With every unit of log_RT_s these log odds become smaller by 0.23. The model predicts that the log odds for the mean log_RT_s in the touch group is 1.56 (0.86 + 0.70), i.e. much higher than in the click group. With every unit of log_RT_s these log odds become smaller by 0.22.\n\n# The baseline difference between click and touch group is compelling with more straight trajectories in the touch group. The effect of log_RT_s is also compelling with less straight trajectories for slower responses. This relationship is not compellingly modulated between the touch and the click group (virtually identical).\n\n\n\n\n\n\n\n\nExercise 1d\nExtract the posteriors means and 95% CrIs for the relationships between straight, log_RT_s and group for representative range of log_RT_s values. Plot the logistic regression lines for both groups into one graph. Color code the regression lines according to group.\n\n\n\n\n\nShow solution\n# extract posterior means for model coefficients\npredicted_values <- GlmMdl %>%\n  spread_draws(b_Intercept, b_log_RT_s, b_grouptouch, `b_log_RT_s:grouptouch`) %>%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-3, 7, 0.2))) %>% \n  unnest(log_RT) %>%\n  # transform into proportion space\n  mutate(pred_click = plogis(b_Intercept + b_log_RT_s * log_RT),\n         pred_touch = plogis(b_Intercept + b_log_RT_s * log_RT +\n                               b_grouptouch + `b_log_RT_s:grouptouch` * log_RT)\n         ) %>%\n  group_by(log_RT) %>%\n  summarise(pred_click_m = mean(pred_click, na.rm = TRUE),\n            pred_click_low = quantile(pred_click, prob = 0.025),\n            pred_click_high = quantile(pred_click, prob = 0.975),\n            pred_touch_m = mean(pred_touch, na.rm = TRUE),\n            pred_touch_low = quantile(pred_touch, prob = 0.025),\n            pred_touch_high = quantile(pred_touch, prob = 0.975)\n            ) \n\n# plot predicted values against data\nggplot(data = predicted_values) +\n  geom_hline(yintercept = c(0,1), lty = \"dashed\", color = \"grey\") +\n  geom_point(data = dolphin_agg,\n             aes(x = log_RT_s, y = straight_numeric, color = group), \n             position = position_jitter(height = 0.02), alpha = 0.2) +\n  geom_ribbon(aes(x = log_RT, ymin = pred_click_low, ymax = pred_click_high), alpha = 0.2) +\n  geom_ribbon(aes(x = log_RT, ymin = pred_touch_low, ymax = pred_touch_high), alpha = 0.2) +\n  geom_line(aes(x = log_RT, y = pred_click_m), color = \"#E69F00\", size = 2) +\n  geom_line(aes(x = log_RT, y = pred_touch_m), color = \"#56B4E9\", size = 2) +\n  ylab(\"Predicted prob of straight trajs\") +\n  ylim(-0.3,1.3) +\n  xlim(-3,7)\n\n\n\n\n\n\n\n\nExercise 1e\nAssume we want to predict correct responses based on condition. We look at the touch group only. Set up a data frame and plot the data as a point plot. (Remember how to jitter the data points)\n\n\n\n\n\nShow solution\n# set up data frame\ndolphin_agg2 <- dolphin %>% \n filter(group == \"touch\")\n\ndolphin_agg2$correct_numeric <- as.numeric(as.character(dolphin_agg2$correct))\n\nggplot(data = dolphin_agg2) +\n  geom_point(aes(x = condition, y = correct_numeric, color = condition), \n             # we add a little bit of jitter to make the points better visible\n             position = position_jitter(height = 0.02, width = 0.1), alpha = 0.2) \n\n\n\n\n\n\n\n\nExercise 1f\nRun the appropriate generalized linear model in brms that predicts correct responses based on condition. Extract the posterior means and 95% CrIs for the effect of condition on correct and plot them as points and whiskers into one plot superimposed on the data.\n\n\n\n\n\nShow solution\nGlmMdl2 <- brm(correct ~ condition, \n                 dolphin_agg2, cores = 4,\n              family = \"bernoulli\")\nGlmMdl2\n\n# extract posterior means for model coefficients\npredicted_values <- GlmMdl2 %>%\n  spread_draws(b_Intercept, b_conditionTypical) %>%\n  # transform into proportion space\n  mutate(Atypical = plogis(b_Intercept),\n         Typical = plogis(b_Intercept + b_conditionTypical)\n         ) %>%\n  select(Atypical, Typical) %>% \n  gather(parameter, posterior) %>% \n  group_by(parameter) %>%\n  summarise(mean = mean(posterior, na.rm = TRUE),\n            lower = quantile(posterior, prob = 0.025),\n            upper = quantile(posterior, prob = 0.975)\n            ) \n\n# plot predicted values against data\nggplot(data = predicted_values) +\n  geom_point(data = dolphin_agg2, aes(x = condition, y = correct_numeric, color = condition), \n             # we add a little bit of jitter to make the points better visible\n             position = position_jitter(height = 0.02, width = 0.1), alpha = 0.2) +\n  geom_errorbar(aes(x = parameter, ymin = lower, ymax = upper), \n                width = 0.1, color = \"black\") +\n  geom_point(aes(x = parameter, y = mean, fill = parameter),\n             size = 4, color = \"black\", pch = 21) +\n  ylab(\"Predicted prob of correct responses\")\n\n\n\n\nExercise 2: Poisson regression\nWe will continue to use dolphin_agg in this exercise.\n\n\n\n\n\n\nExercise 2b\nPlot the relationship between xpos_flips and log_RT_s in a scatterplot and visually differentiate between conditions as you see fit.\n\n\n\n\n\nShow solution\nggplot(data = dolphin_agg) +\n  geom_point(aes(x = log_RT_s, y = xpos_flips, color = condition), \n             # we add a little bit of jitter to make the points better visible\n             position = position_jitter(height = 0.2), alpha = 0.2) +\n  ylim(-1,8) +\n  xlim(-5,10)\n\n\n\n\n\n\n\n\nExercise 2b\nRun an appropriate generalized regression model for xflips with brms to predict xpos_flips based on log_RT_s, condition, and their two-way interaction.\n\n\n\n\n\nShow solution\nGlmMdl3 <- brm(xpos_flips ~ log_RT_s * condition, \n                 dolphin_agg, cores = 4,\n              family = \"poisson\")\nGlmMdl3\n\n\n\n\n\n\n\n\nExercise 2c\nExtract the posterior means and 95% CrIs across a range of representative values of log_RT_s (see walkthrough) for both conditions and plot them against the data (as done before in walkthrough and exercise 1).\n\n\n\n\n\nShow solution\npredicted_Poisson_values <- GlmMdl3 %>%\n  spread_draws(b_Intercept, b_log_RT_s, b_conditionTypical, `b_log_RT_s:conditionTypical`) %>%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-5, 10, 0.5))) %>% \n  unnest(log_RT) %>%\n  mutate(pred_atypical = exp(b_Intercept + b_log_RT_s * log_RT),\n         pred_typical = exp(b_Intercept + b_log_RT_s * log_RT +\n                              b_conditionTypical + `b_log_RT_s:conditionTypical` * log_RT)) %>%\n  group_by(log_RT) %>%\n  summarise(pred_atypical_m = mean(pred_atypical, na.rm = TRUE),\n            pred_atypical_low = quantile(pred_atypical, prob = 0.025),\n            pred_atypical_high = quantile(pred_atypical, prob = 0.975),\n            pred_typical_m = mean(pred_typical, na.rm = TRUE),\n            pred_typical_low = quantile(pred_typical, prob = 0.025),\n            pred_typical_high = quantile(pred_typical, prob = 0.975)) \n\n\nggplot(data = predicted_Poisson_values, aes(x = log_RT)) +\n  geom_point(data = dolphin_agg, aes(x = log_RT_s, y = xpos_flips, color = condition), \n             position = position_jitter(height = 0.2), alpha = 0.2) +\n  geom_ribbon(aes(ymin = pred_atypical_low, ymax = pred_atypical_high), alpha = 0.1) +\n  geom_ribbon(aes(ymin = pred_typical_low, ymax = pred_typical_high), alpha = 0.1) +\n  geom_line(aes(y = pred_atypical_m), color = \"#E69F00\", size = 2) +\n  geom_line(aes(y = pred_typical_m),color = \"#56B4E9\", size = 2) +\n  ylab(\"Predicted prob of xflips\") +\n  ylim(-1,10) +\n  xlim(-3,6)\n\n\n\n\nExercise 3: Logistic regression with binomial likelihood\nBinary logistic regression assumes that the outcome variable comes from a Bernoulli distribution which is a special case of a binomial distribution where the number of trial \\(n = 1\\) and thus the outcome variable can only be 1 or 0. In contrast, binomial logistic regression assumes that the number of the target events follows a binomial distribution with \\(n\\) trials and probability \\(q\\). Read up on Binomial data with brms here: https://www.rensvandeschoot.com/tutorials/generalised-linear-models-with-brms/\nTake the following subset of the dolphin data frame that only contains correct responses (= 1).\n\n# set up data frame\ndolphin_sub <- dolphin %>% \n  filter(correct == 1) %>% \n  mutate(straight = (ifelse(prototype_label == \"straight\", 1, 0)),\n         log_RT_s = scale(log(RT)))\n\n\n\n\n\n\n\nExercise 3a\nFor each subject_id in each group, aggregate the mean log_RT_s, the number of trials that are classified as straight trajectories, and the total number of trials. Plot the proportion of trials that are classified as straight (vs. all trials) trajectories for each subject.\n\n\n\n\n\nShow solution\n# set up data frame\ndolphin_agg3 <- dolphin_sub %>% \n  group_by(subject_id, group) %>% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight),\n            total = n()) \n\n# plot predicted values against data\nggplot(data = dolphin_agg3) +\n  geom_point(aes(x = log_RT_s, y = straights/total, color = group), size = 2, alpha = 0.5) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5)\n\n\n\n\n\n\n\n\nExercise 3b\nFormulate a binomial logistic regression model to predict the proportion of straight trajectories based on log_RT_s, group, and their two-way interaction. Note that these proportional data are not assumed to be generated by a Bernoulli distribution, but a binomial distribution. Take that into account by setting family = binomial(link = \"logit\").\nExtract posterior means and 95% CrIs for the effect of log_RT_s for both groups and plot them across a representative range of log_RT_s (as done before in this week).\n\n\n\n\n\nShow solution\n# We specify both the number of target events (straights) and the total number of trials (total) wrapped in trials(), which are separated by |. In addition, the family should be “binomial” instead of “bernoulli”.\nGlmMdl4 <- brm(\n  straights | trials(total) ~ log_RT_s * group,  \n  data = dolphin_agg3, \n  family = binomial(link = \"logit\"))\n\nsummary(GlmMdl4)\n\n# extract posteriors means and 95% CrIs\npredicted_values <- GlmMdl4 %>%\n  spread_draws(b_Intercept, b_log_RT_s, b_grouptouch, `b_log_RT_s:grouptouch`) %>%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-3, 3, 0.2))) %>% \n  unnest(log_RT) %>%\n  # transform into proportion space\n  mutate(pred_click = plogis(b_Intercept + b_log_RT_s * log_RT),\n         pred_touch = plogis(b_Intercept + b_log_RT_s * log_RT +\n                               b_grouptouch + `b_log_RT_s:grouptouch` * log_RT)\n         ) %>%\n  group_by(log_RT) %>%\n  summarise(pred_click_m = mean(pred_click, na.rm = TRUE),\n            pred_click_low = quantile(pred_click, prob = 0.025),\n            pred_click_high = quantile(pred_click, prob = 0.975),\n            pred_touch_m = mean(pred_touch, na.rm = TRUE),\n            pred_touch_low = quantile(pred_touch, prob = 0.025),\n            pred_touch_high = quantile(pred_touch, prob = 0.975)\n            ) \n\n# plot predicted values against data\nggplot(data = predicted_values) +\n  geom_point(data = dolphin_agg3,\n             aes(x = log_RT_s, y = straights / total, color = group),\n             alpha = 0.2, size = 2) +\n  geom_ribbon(aes(x = log_RT, ymin = pred_click_low, ymax = pred_click_high), alpha = 0.2) +\n  geom_ribbon(aes(x = log_RT, ymin = pred_touch_low, ymax = pred_touch_high), alpha = 0.2) +\n  geom_line(aes(x = log_RT, y = pred_click_m), color = \"#E69F00\", size = 2) +\n  geom_line(aes(x = log_RT, y = pred_touch_m), color = \"#56B4E9\", size = 2) +\n  ylab(\"Predicted prob of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5)\n\n\n\n\n\n\n\n\nExercise 3c\nNow compare the results from this analysis to the results from the model 1b above which you plotted in 1d. How do the model results differ and why could that be? (Feel free to explore the data to understand what is going on)\n\n\n\n\n\nShow solution\n# Answer: The model in 1b suggested a negative coefficient of reaction time, i.e. slower responses lead to less straight trajectories. The model here suggests a positive coefficient for reaction time, i.e. slower responses lead to more straight trajectories. Given the data, the model, and the priors, this effect is compelling for at least the click group. \n\n# A major difference in the two analyses is that the former analysis looked at all data and disregarded that responses came from clusters of sources. For example, responses that come from one and the same participant are dependent on each other because participants might differ in characteristics relevant to the task, like how fast they move and how many times they move to the target in a straight trajectory. The latter analysis aggregated participants behavior by looking at the proportion of straight trajectories within each subject, thus one data point corresponds to one participant, resulting in data points being independent (at least regarding the participant identity). If all participants showed a negative effect of reaction time on the likelihood of straight trajectories, but participants systematically differ in terms of their baseline correlation between reaction time and likelihood of producing straight trajectories in the opposite direction (positive relationship), we might get discrepancies between these different models. What we ultimately need is to take multiple levels of the data into account simultaneously, which is the topic of next week."
  },
  {
    "objectID": "practice-sheets/04a-MCMC-diagnostics.html",
    "href": "practice-sheets/04a-MCMC-diagnostics.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Load relevant packages and “set the scene.”\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nCode\n# install packages from CRAN (unless installed)\npckgs_needed <- c(\n  \"tidyverse\",\n  \"brms\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\"\n)\npckgs_installed <- installed.packages()[,\"Package\"]\npckgs_2_install <- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx <- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors()[c(1,3,4,5,2,6:14),\"hex\", drop = TRUE]\n\n# setting theme colors globally\nscale_colour_discrete <- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete <- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\ndolphin <- aida::data_MT\nmy_scale <- function(x) c(scale(x))\n\nThis tutorial provides demonstrations of how to check the quality of MCMC samples obtained from brms model fits.\n\nA good model\nTo have something to go on, here are two model fits, one of this is good, the other is … total crap. The first model fits a smooth line to the average world temperature. (We need to set the seed here to have reproducible results.)\n\nfit_good <- brm(\n  formula = avg_temp ~ s(year), \n  data = aida::data_WorldTemp,\n  seed = 1969\n) \n\nThe good model is rather well behaved. Here is a generic plot of its posterior fits and traceplots:\n\nplot(fit_good)\n\n\n\n\nTraceplots look like madly-in-love caterpillars doing their thing.\nWe can check \\(\\hat{R}\\) and effective sample sizes also in the summary of the model:\n\nsummary(fit_good)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ s(year) \n   Data: aida::data_WorldTemp (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     3.56      1.08     1.96     6.17 1.00      907     1689\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     8.31      0.02     8.27     8.35 1.00     3732     2652\nsyear_1      14.55      2.25    10.11    19.08 1.00     2182     2371\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.01     0.30     0.36 1.00     3823     2962\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nInterestingly, there is one warning message about one divergent transition. We are recommended to check the pairs() plot, so here goes:\n\npairs(fit_good)\n\n\n\n\nThis is actually not too bad. (Wait until you see a terrible case below!)\nWe can try to fix this problem with a single divergent transition by doing as recommended by the warning message, namely increasing the adapt_delta parameter in the control structure:\n\nfit_good_adapt <- brm(\n  formula = avg_temp ~ s(year), \n  data = aida::data_WorldTemp,\n  seed = 1969,\n  control = list(adapt_delta=0.9),\n) \n\nsummary(fit_good_adapt)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ s(year) \n   Data: aida::data_WorldTemp (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     3.59      1.09     1.97     6.30 1.00     1003     1613\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     8.31      0.02     8.27     8.35 1.00     3632     2619\nsyear_1      14.59      2.40     9.96    19.34 1.00     2425     2510\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.01     0.30     0.36 1.00     3588     2868\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThat looks better, but what did we just do? — When the sampler “warms up”, it tries to find good parameter values for the case at hand. The adapt_delta parameter is the minimum amount of accepted proposals (where to jump next) before “warm up” counts as done and successfull. So with a small problem like this, just making the adaptation more ambitious may have have solved the problem. It has also, however, made the sampling slower, less efficient.\nA powerful interactive tool for exploring a fitted model (diagnostics and more) is shinystan:\n\nshinystan::launch_shinystan(fit_good_adapt)\n\n\n\nA terrible model\nThe main (maybe only) reason for serious problems with the NUTS sampling is this: sampling issues arise for bad models. So, let’s come up with a really stupid model.\nHere’s a model that is like the previous but adds a second predictor, which is a normal (non-smoothed) regression coefficient that is almost identical to the original year information. You may already intuit that this cannot possibly be a good idea; the model is notionally deficient. So, we exepct nightmares during sampling:\n\nfit_bad <- brm(\n  formula = avg_temp ~ s(year) + year_perturbed, \n  data = aida::data_WorldTemp |> mutate(year_perturbed = rnorm(1,year,0.001)),\n  seed = 1969\n) \n\nsummary(fit_bad)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ s(year) + year_perturbed \n   Data: mutate(aida::data_WorldTemp, year_perturbed = rnor (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     3.60      1.09     2.00     6.23 1.00     1129     1529\n\nPopulation-Level Effects: \n                       Estimate        Est.Error          l-95% CI\nIntercept      1881743732622.60 5163778452754.78 -7906966272547.49\nyear_perturbed   -1075283223.00    2950733535.76    -8054763769.55\nsyear_1                   14.56             2.34             10.10\n                        u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      14095822306960.40 2.21        5       12\nyear_perturbed     4518271021.89 2.21        5       12\nsyear_1                    19.35 1.00     2691     2419\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.01     0.30     0.36 1.00     4448     3051\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIndeed, that looks pretty bad. We managed to score badly on all major accounts:\n\nlarge \\(\\hat{R}\\)\nextremely poor efficient sample size\nridiculously far ranging posterior estimates for the main model components\ntons of divergent transitions\nmaximum treedepth reached more often than hipster touches their phone in a week\n\nSome of these caterpillars look like they are in a vicious rose war:\n\nplot(fit_bad)\n\n\n\n\nWe also see that that the intercept of and the slope for year_perturbed are the main troublemakers (in terms of traceplots).\nInterestingly, a simple posterior check doesn’t look half-bad:\n\npp_check(fit_bad)\n\n\n\n\nBut now, have a look at the pairs() plot:\n\npairs(fit_bad)\n\n\n\n\nAha, there we see a clear problem! The joint posterior for the intercept and the slope for year_perturbed looks like a line. This means that these parameters could in princple do the same “job”.\nThis suggests a possible solution stragey. The model is too unconstrained. It can allow these two parameters meander to wherever they want (or so it seems). We could therefore try honing them in by specifying priors, like so:\n\nfit_bad_wPrior <- brm(\n  formula = avg_temp ~ s(year) + year_perturbed, \n  data = aida::data_WorldTemp |> mutate(year_perturbed = rnorm(1,year,0.001)),\n  seed = 1969,\n  prior = prior(\"student_t(1,0,5)\", coef = \"year_perturbed\")\n) \n\nsummary(fit_bad_wPrior)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ s(year) + year_perturbed \n   Data: mutate(aida::data_WorldTemp, year_perturbed = rnor (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     3.56      1.08     1.99     6.24 1.00     1041     1569\n\nPopulation-Level Effects: \n               Estimate Est.Error   l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       -489.39  68742.92 -104969.69 99623.76 1.01     1649      453\nyear_perturbed     0.28     39.28     -56.92    59.99 1.01     1649      453\nsyear_1           14.60      2.40      10.04    19.45 1.00     1937     2511\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.01     0.30     0.36 1.00     4167     3295\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWell, alright! That isn’t too bad anymore. But it is still clear from the posterior pairs plot that this model has two parameters that steal each other’s show. The model remains a bad model … for our data.\n\npairs(fit_bad)\n\n\n\n\nHere’s what’s wrong: year_perturbed is a constant! The model is a crappy model of the data, because the data is not what we thought it would be. Check it out:\n\naida::data_WorldTemp |> mutate(year_perturbed = rnorm(1,year,0.001))\n\n# A tibble: 269 × 5\n    year anomaly uncertainty avg_temp year_perturbed\n   <dbl>   <dbl>       <dbl>    <dbl>          <dbl>\n 1  1750  -1.41        NA        7.20          1750.\n 2  1751  -1.52        NA        7.09          1750.\n 3  1753  -1.07         1.3      7.54          1750.\n 4  1754  -0.614        1.09     8.00          1750.\n 5  1755  -0.823        1.24     7.79          1750.\n 6  1756  -0.547        1.28     8.06          1750.\n 7  1757  -0.438        1.31     8.17          1750.\n 8  1758  -2.42         1.76     6.19          1750.\n 9  1759  -1.53         2.25     7.08          1750.\n10  1760  -2.46         2.75     6.14          1750.\n# … with 259 more rows\n\n\nSo, we basically ran a model with two intercepts!?!\nLet’s try again:\n\ndata_WorldTemp_perturbed <- aida::data_WorldTemp |> \n    mutate(year_perturbed = rnorm(nrow(aida::data_WorldTemp),year, 50))\ndata_WorldTemp_perturbed\n\n# A tibble: 269 × 5\n    year anomaly uncertainty avg_temp year_perturbed\n   <dbl>   <dbl>       <dbl>    <dbl>          <dbl>\n 1  1750  -1.41        NA        7.20          1664.\n 2  1751  -1.52        NA        7.09          1710.\n 3  1753  -1.07         1.3      7.54          1708.\n 4  1754  -0.614        1.09     8.00          1726.\n 5  1755  -0.823        1.24     7.79          1793.\n 6  1756  -0.547        1.28     8.06          1771.\n 7  1757  -0.438        1.31     8.17          1691.\n 8  1758  -2.42         1.76     6.19          1765.\n 9  1759  -1.53         2.25     7.08          1764.\n10  1760  -2.46         2.75     6.14          1728.\n# … with 259 more rows\n\n\nThat’s more like what we thought it was: year_perturbed is supposed to be noisy version of the actual year. So, let’s try again, leaving out the smoothing, just for some more chaos-loving fun:\n\nfit_bad_2 <- brm(\n  formula = avg_temp ~ year + year_perturbed, \n  data = data_WorldTemp_perturbed,\n  seed = 1969,\n  prior = prior(\"student_t(1,0,5)\", coef = \"year_perturbed\")\n) \n\nsummary(fit_bad_2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ year + year_perturbed \n   Data: data_WorldTemp_perturbed (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         -3.54      0.59    -4.68    -2.39 1.00     4299     2956\nyear               0.01      0.00     0.01     0.01 1.00     3669     2607\nyear_perturbed    -0.00      0.00    -0.00     0.00 1.00     3809     2807\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.41      0.02     0.37     0.44 1.00     1548     1548\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThere are no warnings, so this model must be good, right? – No!\nIf we check the pairs plot, we see that we now have introduced a fair correlation between the two predictor variables.\n\npairs(fit_bad_2)\n\n\n\n\nWe should just not have year_perturbed; it’s nonsense, and it shows in the diagnostics.\nYou can diagnose more using shinystan:\n\nshinystan::launch_shinystan(fit_bad)"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html",
    "href": "practice-sheets/02b-catPreds-tutorial.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\nGeneralized (non-)linear mixed effect models are a powerful statistical tool that gains increasing popularity for data analysis in cognitive science and many other disciplines. This tutorial will provide an overview of different categorical variable coding schemes used in mixed effect models. We will look at two example data sets from factorial-design experiments with categorical predictors and a continuous dependent variable which we will analyze using a Bayesian approach."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#dataset",
    "href": "practice-sheets/02b-catPreds-tutorial.html#dataset",
    "title": "Bayesian regression: theory & practice",
    "section": "Dataset",
    "text": "Dataset\nThe first part of this tutorial is based on a data set from an experiment by Winter and Grawunder (2012) You can get the dataset by running:\n\npoliteness_df <- faintr::politeness\n\n# get a look at the data set\nhead(politeness_df)\n\n# A tibble: 6 × 5\n  subject gender sentence context pitch\n  <chr>   <chr>  <chr>    <chr>   <dbl>\n1 F1      F      S1       pol      213.\n2 F1      F      S1       inf      204.\n3 F1      F      S2       pol      285.\n4 F1      F      S2       inf      260.\n5 F1      F      S3       pol      204.\n6 F1      F      S3       inf      287.\n\n\nThe data contains records of the voice pitch of speakers in different social contexts (polite and informal). They investigated whether the mean voice pitch differs across the factor gender of the speakers (F and M) and across the factor contexts - resulting in four different condition combinations (gender X context). Such a design is called factorial design and the single combinations are called design cells."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#explore-data-visually",
    "href": "practice-sheets/02b-catPreds-tutorial.html#explore-data-visually",
    "title": "Bayesian regression: theory & practice",
    "section": "Explore Data visually",
    "text": "Explore Data visually\nBefore we dive into any statistical analyses of our dataset it is helpful to get a rough idea of what the data looks like. For example, we can start by exploring the dataset visually.\n\n\n\n\n\nFurthermore, we can compute some basic statistics - e.g. the mean of the different design cells, before we turn to more complex linear models. We can also compute the overall mean pitch across all the conditions - the grand mean. These values will be helpful for a sanity check when interpreting the linear models later on.\n\ntibble_means <- politeness_df %>%\n  group_by(context, gender) %>%\n  summarize(mean = mean(pitch))\nhead(tibble_means)\n\n# A tibble: 4 × 3\n# Groups:   context [2]\n  context gender  mean\n  <chr>   <chr>  <dbl>\n1 inf     F       261.\n2 inf     M       144.\n3 pol     F       233.\n4 pol     M       133.\n\nmean(tibble_means$mean)\n\n[1] 192.8605"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#dummy-treatment-coding",
    "href": "practice-sheets/02b-catPreds-tutorial.html#dummy-treatment-coding",
    "title": "Bayesian regression: theory & practice",
    "section": "Dummy (Treatment) Coding",
    "text": "Dummy (Treatment) Coding\nDummy coding, or treatment coding, is the default coding scheme used by R. Understanding the name ‘treatment coding’ helps understanding what this coding scheme does: imagine a medical experiment with a single control group (who obtain a placebo) and different experimental groups each of which gets a different treatment (e.g., different drugs), and where we want to compare each treatment group to the single, pivotal control group. Consequently, dummy coded variables are estimated by comparing all levels of the variable to a reference level. The intercept of a linear model containing dummy-coded variables is the mean of the reference level.\nOur variables only have two levels, so the effect of gender could be estimated by treating female as the reference level and estimating the effect of being male compared to the reference level – so basically estimating the difference in pitch it takes to “get from female to male”. Similarly, we can estimate the effect of context: the informal context can be treated as the reference level and the effect of politeness can be estimated against it. By default, the first level of a factor is treated as the reference level (for unordered factors that is the first string in alphanumeric order) - but principally, there is no difference as to which level should be used as the reference level. It makes sense to choose the level which is in some sense the ‘control’ in your experimental design.\nBecause R uses dummy-coding by default, we can look at the default numerical coding right away. The function contrasts() displays the contrast matrix for the respective variable:\n\ncontrasts(politeness_df$gender)\n\n  M\nF 0\nM 1\n\ncontrasts(politeness_df$context)\n\n    pol\ninf   0\npol   1\n\n\nBut if we wish to explicitly assign a dummy (treatment) coding to a variable, we may do so by a built-in R function:\n\ncontrasts(politeness_df$gender) <- contr.treatment(2) # insert the number of levels here\n# check\ncontrasts(politeness_df$gender)\n\n  2\nF 0\nM 1\n\n\nSo both variables \\(x_1\\) and \\(x_2\\) can take either the value 0 or 1 (because we dummy-code both categorical variables; see below for more). We already defined the referenc levels of the single variables, now we can define the overall reference level of our model (by combining the two individual reference levels) – it is the mean pitch of female speakers in informal contexts.\nHaving set all the basics, we can now turn to computing a linear model of the mean pitch as predicted by the factors gender and context:\n\n# here, we only use fixed effects\nlm.dummy.FE <- brm(\n  pitch ~ gender * context,\n  data = politeness_df,\n  cores = 4,\n  iter = 1000\n)\nlm.dummy.FE.coefs <- fixef(lm.dummy.FE)[,1] %>% as.numeric() # get the estimated coefficients\nsummary(lm.dummy.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ gender * context \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            260.96      7.95   245.63   276.88 1.00     1077     1346\ngender2             -116.44     11.38  -138.76   -94.49 1.00      941     1502\ncontextpol           -27.77     11.33   -49.84    -5.88 1.00     1011     1217\ngender2:contextpol    16.38     15.88   -14.91    47.14 1.01      809     1108\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.12      2.91    30.93    42.29 1.01     2098     1593\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow how do we interpret the estimated coefficients?\nLet us recall the regression equation that is hidden behind this output: \\[y = \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 + \\beta_3*x_1x_2\\]\nIn order to help us interpret the output, R assigns string names to the estimated coefficients using the names we used in the generic formula. The (Intercept) corresponds to \\(\\beta_0\\), genderM corresponds to \\(\\beta_1\\), contextpol corresponds to \\(\\beta_2\\) and genderM:contextpol (the interaction term) to \\(\\beta_3\\).\nFurther, let us recall the numerical coding of our variables: for \\(x_1\\) (gender) a 0 means female, a 1 means male; for \\(x_2\\) (context) a 0 means informal, a 1 means polite. So the computed values are the estimates for conditions differing from the respective reference conditions - i.e. when the respective \\(x\\) is a 1.\nTo get an estimate of a certain design cell (\\(y_i\\)) - let’s start with the mean pitch of female speakers (0 for \\(x_1\\)) in informal contexts (0 for \\(x_2\\)) - we just insert the corresponding numeric values for the corresponding \\(x\\) and the estimated value for the corresponding \\(\\beta\\). Thus we get:\n\ny1 = lm.dummy.FE.coefs[1] + lm.dummy.FE.coefs[2]*0 +\n  lm.dummy.FE.coefs[3]*0 + lm.dummy.FE.coefs[4]*(0)\ny1\n\n[1] 260.9607\n\n\nHence, the mean pitch of female speakers in informal context corresponds to the intercept. As a sanity check, we can recall that for dummy coded variables the model intercept is just the mean of the reference cell (in our case, female speakers in informal contexts!).\nLet’s now calculate the mean pitch of male speakers (1 for \\(x_1\\)) in informal contexts (0 for \\(x_2\\)):\n\ny2 = lm.dummy.FE.coefs[1] + lm.dummy.FE.coefs[2]*1 +\n  lm.dummy.FE.coefs[3]*0 + lm.dummy.FE.coefs[4]*(1*0)\ny2\n\n[1] 144.5167"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#simple-contrast-coding",
    "href": "practice-sheets/02b-catPreds-tutorial.html#simple-contrast-coding",
    "title": "Bayesian regression: theory & practice",
    "section": "Simple (Contrast) Coding",
    "text": "Simple (Contrast) Coding\nAnother common coding scheme is the simple coding (also called contrast coding). Simple coded variables are also compared to a reference level (just like dummy-coded ones). However, the intercept of a simple coded model is the grand mean – the mean of all cells (i.e. the mean of female-informal & female-polite & male-informal & male-polite cells).\nGenerally, this kind of coding can be created by subtracting \\(1/k\\) from the dummy coding contrast matrix, where \\(k\\) is the number of levels a variable has (in our case, both have two). Hence, the reference level will always only have negative values in the contrast matrix. The general rule is that the contrasts within a column have to add up to 0. R does not provide a built-in function for simple coding, but we can easily create the respective matrix ourselves by subtracting \\(1/k\\) (i.e. 1/2) from the dummy coding matrix:\n\n# manual creation of contrasts\ncontr.matrix <- matrix( rep(0.5, 2))\ndummy.matrix <- contr.treatment(2)\ncontr.coding <- dummy.matrix - contr.matrix\n\n# we should duplicate the values to not overwrite previous contrasts\npoliteness_df <- politeness_df %>%\n  mutate(context_contr = context,\n         gender_contr = gender)\ncontrasts(politeness_df$context_contr) <- contr.coding\ncontrasts(politeness_df$gender_contr)  <- contr.coding\n\nHence now the gender is coded as -0.5 for female and 0.5 for male; context is coded as -0.5 for informal and 0.5 for polite.\nLet’s again look at our regression model:\n\nlm.contr.FE <- brm(\n  pitch ~ gender_contr * context_contr,\n  data = politeness_df,\n  cores = 4,\n  iter =  1000\n)\nlm.contr.FE.coefs <- fixef(lm.contr.FE)[,1] %>% as.numeric() # get vector of estimated coefficients\nsummary(lm.contr.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ gender_contr * context_contr \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                      192.87      3.98   185.01   200.72 1.00     3023\ngender_contr2                 -108.16      7.88  -123.42   -92.57 1.01     2740\ncontext_contr2                 -19.36      7.94   -34.33    -3.73 1.00     2468\ngender_contr2:context_contr2    15.84     15.48   -14.97    47.13 1.00     2953\n                             Tail_ESS\nIntercept                        1464\ngender_contr2                    1199\ncontext_contr2                   1405\ngender_contr2:context_contr2     1438\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.07      2.85    30.92    42.18 1.00     2599     1642\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn order to compute the mean pitch of a specific cell, we proceed just as with dummy-coded variables and insert the respective estimates and values for \\(x\\). Let us start with female speakers (\\(x_1\\) is -0.5) in informal contexts (\\(x_2\\) is -0.5):\n\ny1 = lm.contr.FE.coefs[1] + lm.contr.FE.coefs[2]*(-0.5) + lm.contr.FE.coefs[3]*(-0.5) + lm.contr.FE.coefs[4]*(-0.5)*(-0.5)\ny1\n\n[1] 260.5951\n\n\nWe get the same result as before (as we should - the estimates should not depend on a coding scheme but only on the data). As a sanity check, we can again look at the intercept – it matches the grand mean we computed in the beginning of this tutorial – as it should.\nYour turn! Compute the pitch means for the other three conditions.\n\n# your code here"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#deviation-sum-coding",
    "href": "practice-sheets/02b-catPreds-tutorial.html#deviation-sum-coding",
    "title": "Bayesian regression: theory & practice",
    "section": "Deviation (Sum) Coding",
    "text": "Deviation (Sum) Coding\nDeviation coding (also called sum coding) is the most popular coding scheme and is often considered the best choice to get a clear picture of presence (or absence) of an effect and a clear random effects interpretation.\nIt is slightly different from the previous schemes. It compares the mean of the predicted variable for a specific condition to the grand mean. So the estimates do not tell you the difference between the reference level and another level anymore. The intercept of linear models with sum coded variables is the grand mean.\nR has a built-in function for creating sum coded variables:\n\n# again create a new variable\npoliteness_df %>%\n  mutate(context_dev = context,\n         gender_dev = gender) -> politeness_df\ncontrasts(politeness_df$context_dev) <- contr.sum(2) # insert number of levels\ncontrasts(politeness_df$gender_dev)  <- contr.sum(2)\n\nNow the gender is coded as s 1 for female and -1 for male; context is coded as 1 for informal and -1 for polite.\nBelow we fit a model with the sum-coded variables:\n\nlm.dev.FE <- brm(pitch ~ context_dev * gender_dev,\n                data = politeness_df,\n                cores = 4,\n                iter = 1000)\nlm.dev.FE.coefs <- fixef(lm.dev.FE)[,1] %>% as.numeric()\nsummary(lm.dev.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ context_dev * gender_dev \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                  192.93      3.98   184.83   200.91 1.00     2369\ncontext_dev1                 9.71      4.10     1.39    17.87 1.00     2587\ngender_dev1                 53.97      3.87    46.42    61.50 1.01     2770\ncontext_dev1:gender_dev1     4.06      4.16    -3.86    12.03 1.00     2255\n                         Tail_ESS\nIntercept                    1608\ncontext_dev1                 1598\ngender_dev1                  1491\ncontext_dev1:gender_dev1     1445\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.23      2.98    30.96    42.62 1.00     1955     1537\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe coefficients denote now the difference between the grand mean (i.e. intercept) and the mean of the respective condition.\nWe apply the same idea to estimate the pitch means for specific cases: E.g. for female speakers in informal contexts we do:\n\ny1 = lm.dev.FE.coefs[1] + lm.dev.FE.coefs[2]*1 + lm.dev.FE.coefs[3]*1 + lm.dev.FE.coefs[4]*1*1\ny1\n\n[1] 260.6811\n\n\nSince the intercept is now the grand mean and not a specific reference level, let us think about the interpretation of the single estimates. The estimate of e.g. the context effect now denotes the value by which the mean pitch in informal (estimate * 1, remember our coding!) or polite contexts (estimate * -1) differs from the grand mean. So if we wish to calculate the mean pitch in polite contexts (across genders), we would do:\n\nyPol = lm.dev.FE.coefs[1] + lm.dev.FE.coefs[2] * (-1)\nyPol\n\n[1] 183.2212\n\n\nThis means that the single estimates are in some sense ‘independent’ of each other (in contrast to e.g. dummy-coded variables where the estimates are bound to the reference levels of two variables) and give us insight if a specific factor is credibly different from 0. Similarly, if we wish to calculate the mean pitch of male speakers, we would calculate:\n\nyM = lm.dev.FE.coefs[1] + lm.dev.FE.coefs[3] * (-1)\nyM\n\n[1] 138.9629"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#helmert-coding",
    "href": "practice-sheets/02b-catPreds-tutorial.html#helmert-coding",
    "title": "Bayesian regression: theory & practice",
    "section": "Helmert Coding",
    "text": "Helmert Coding\nIn this coding scheme, a level of a variable is compared to its subsequent levels. In our dataset, e.g. for gender the level female is compared to the subsequent level male.\nGenerally, to create such a coding, in order to compare the first level to the subsequent levels you would assign \\((k-1)/k\\) to the first level and \\(-1/k\\) to all subsequent levels where \\(k\\) is the total number of levels. To compare the second level to the subsequent levels you would assign 0 to the first level, \\((i-1)/i\\) to the second level and \\(-i/1\\) to all subsequent levels where \\(i = k-1\\) and so on. The difference of this coding scheme to previous ones is more clear for variales with >2 levels (see below). The intercept of a linear model corresponds to the grand mean.\nR does not have a built-in function for standard Helmert coding, so we do it manually:\n\n# with politeness data\nhelm.matrix <- matrix(c(0.5, -0.5))\npoliteness_df <-\n politeness_df %>%\n mutate(gender_helm = gender,\n         context_helm = context)\ncontrasts(politeness_df$gender_helm)  <- helm.matrix\ncontrasts(politeness_df$context_helm) <- helm.matrix\n\nThe linear model looks like this:\n\nlm.helmert.FE <- brm(pitch ~ context_helm * gender_helm,\n                data = politeness_df,\n                cores = 4,\n                iter = 1000)\nlm.helmert.FE.coefs <- fixef(lm.helmert.FE)[,1] %>% as.numeric()\nsummary(lm.helmert.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ context_helm * gender_helm \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                    192.93      4.06   185.01   201.08 1.00     2085\ncontext_helm1                 19.22      8.05     3.06    35.15 1.00     2048\ngender_helm1                 108.21      7.75    92.33   123.53 1.00     2094\ncontext_helm1:gender_helm1    15.41     16.09   -16.81    47.08 1.00     2194\n                           Tail_ESS\nIntercept                      1516\ncontext_helm1                  1413\ngender_helm1                   1333\ncontext_helm1:gender_helm1     1454\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.28      3.03    31.02    42.85 1.00     2081     1616\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe contrast estimate for the first level and the remaining levels is calculated by taking the mean of the dependent variable for the first level and subtracting the mean of the dependent variable for the remaining levels (in our case, just the mean of the second level). In other words, if we look at the context coefficient it denotes the difference between the mean of the polite and informal context means."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#mixing-coding-schemes",
    "href": "practice-sheets/02b-catPreds-tutorial.html#mixing-coding-schemes",
    "title": "Bayesian regression: theory & practice",
    "section": "Mixing Coding Schemes",
    "text": "Mixing Coding Schemes\nIf you have several categorical predictor variables, it is also possible (and often useful!) to use different coding schemes for the different variables. It might, for example, make sense to use dummy coding for a variable which has a control and a treatment condition, and to use e.g. simple coding for a variable which has two ‘equal’ levels.\nFor example, we could use dummy-coding for context and simple-coding for gender.\nWhen you mix coding schemes or define your own schemes there might be no pre-defined answers to questions as to what the sigle coefficients or the intercept mean. But knowing how the different schemes work, you can easily find this out!\nLet us explore how the interpretation of the model changes if we mix coding schemes:\n\nlm.mixedCode.FE <- brm(pitch ~ context * gender_contr,\n                data = politeness_df,\n                cores = 4,\n                iter = 1000)\nlm.mixedCode.FE.coefs <- fixef(lm.mixedCode.FE)[,1] %>% as.numeric()\nsummary(lm.mixedCode.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ context * gender_contr \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                  202.86      5.63   192.34   213.92 1.00     2434\ncontextpol                 -19.77      8.24   -35.64    -3.57 1.00     2242\ngender_contr2             -115.75     10.86  -137.77   -94.63 1.00     1442\ncontextpol:gender_contr2    15.45     15.83   -14.92    46.28 1.00     1688\n                         Tail_ESS\nIntercept                    1622\ncontextpol                   1582\ngender_contr2                1190\ncontextpol:gender_contr2     1423\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.14      3.02    30.94    42.60 1.00     2163     1580\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nGenerally, the interpretation is just the combination of what we have learned about the individual coding schemes. Recall that the intercept of a dummy-coded model is the mean of the reference level – since we dummy-coded context, the refernce level would be informal context. But it is not the intercept yet! We have the second predictor in our model – the simple coded gender. In simple coded models the intercept is the mean across the levels of the variable. Now the intercept of our model with the two different predictors is the mean pitch in informal contexts - across genders.\nFollowing this logic, the context estimate denotes the difference between the informal and polite contexts - still across genders. The gender estimate denoted the difference between the mean pitch and female speakers if multiplied by the value -0.5 (recall our coding above); and the mean pitch and male speakers if multiplied by the value 0.5."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#dummy-treatment-coding-1",
    "href": "practice-sheets/02b-catPreds-tutorial.html#dummy-treatment-coding-1",
    "title": "Bayesian regression: theory & practice",
    "section": "Dummy (Treatment) Coding",
    "text": "Dummy (Treatment) Coding\nGenerally, the coding schemes work in the very same way independently of the number of levels. The only difference to our old data set is that now we need two numeric variables coding the contrasts between the levels of one variable. If we look at the default (dummy) coding of e.g. the variable List we see a contrast matrix with two columns, each denoting the comparisons between two levels:\n\ncontrasts(latinsquare$SOA)\n\n       medium short\nlong        0     0\nmedium      1     0\nshort       0     1\n\ncontrasts(latinsquare_df$List)\n\n   L2 L3\nL1  0  0\nL2  1  0\nL3  0  1\n\n\nSo now the recoding of the categorical variable takes two numeric variables: e.g. \\(x_{1_2}\\) and \\(x_{1_3}\\), where both can take the values 0 or 1; the single levels are denoted by the combination of the two numeric variables. Again there is a reference level - List 1 - described by \\(x_{1_2}\\) and \\(x_{1_3}\\) being 0. \\(x_{1_2}\\) being 1 describes the difference between the reference level and List 2; \\(x_{1_3}\\) being 1 describes the difference between the reference level and List 3. Correspondingly, there is and individual \\(\\beta\\) for each numeric variable estimated in the regression model. The coding of the SOA factor works just the same way. The interactions between specific levels are described by combining the respective numeric variables \\(x\\). So the model we are fitting is described by:\n\\[y = \\beta_0 + \\beta_1 * x_{1_2} + \\beta_2 * x_{1_3} + \\beta_3 * x_{2_2} + \\beta_4 * x{2_3} + \\beta_5 * x_{1_2}x_{2_2} + \\beta_6 * x_{1_3}x_{2_2} + \\beta_7 * x_{1_2}x_{2_3}  + \\beta8 * x_{1_3}x_{2_3}\\]\n\nlm3.dummy.FE <- brm(RT ~ List * SOA,\n                   data = latinsquare_df,\n                   cores = 4,\n                   iter = 1000)\nlm3.dummy.FE.coefs <- fixef(lm3.dummy.FE)[,1] %>% as.numeric()\nsummary(lm3.dummy.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List * SOA \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          530.54     11.48   508.11   552.33 1.00     1006     1343\nListL2              12.31     16.31   -19.95    45.20 1.00     1036     1161\nListL3              -1.18     15.83   -31.23    30.00 1.00      975     1141\nSOAmedium            6.45     15.84   -24.71    37.80 1.00     1017     1383\nSOAshort            13.46     15.77   -17.46    44.50 1.01     1137     1323\nListL2:SOAmedium     7.02     22.86   -36.66    52.38 1.00     1109     1264\nListL3:SOAmedium   -20.72     22.52   -64.19    23.98 1.00     1027     1089\nListL2:SOAshort    -21.98     23.08   -69.09    20.36 1.00     1218     1361\nListL3:SOAshort    -20.67     22.18   -64.06    22.78 1.00     1198     1117\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.57      2.85    41.41    52.59 1.00     2067     1465\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSince both predictors are dummy-coded the intercept represents the reference level - the mean RT for List 1 and a long SOA. Following the same procedure as for the two-level variables you could calculate the estimated mean RTs for specific conditions."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#simple-contrast-coding-1",
    "href": "practice-sheets/02b-catPreds-tutorial.html#simple-contrast-coding-1",
    "title": "Bayesian regression: theory & practice",
    "section": "Simple (Contrast) Coding",
    "text": "Simple (Contrast) Coding\nSimple coding only slightly differs from dummy-coding – the intercept of the model is the grand mean, not the mean RT of the reference level. Otherwise, the coefficients still denote the difference between the reference level and other specific levels.\n\nlatinsquare_df %>%\n  mutate(List_contr = factor(List),\n         SOA_contr = factor(SOA)) -> latinsquare_df\ndummy.matrix3 <- contr.treatment(3)\ncontr.matrix3 <- matrix(c(1/3, 1/3, 1/3, 1/3, 1/3, 1/3), ncol=2)\ncontrasts(latinsquare_df$List_contr) <- dummy.matrix3 - contr.matrix3\ncontrasts(latinsquare_df$SOA_contr) <-  dummy.matrix3 - contr.matrix3\n\n\nlm3.simple.FE <- brm(RT ~ List_contr * SOA_contr,\n                   data = latinsquare_df,\n                   cores = 4,\n                   iter = 1000)\nlm3.simple.FE.coefs <- fixef(lm3.simple.FE)[,1] %>% as.numeric()\nsummary(lm3.simple.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_contr * SOA_contr \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                534.65      3.86   527.10   542.28 1.00     2318\nList_contr2                7.71      9.82   -11.91    26.51 1.00     1832\nList_contr3              -14.77      9.67   -33.59     3.92 1.00     1636\nSOA_contr2                 1.92      9.51   -16.71    20.01 1.00     2282\nSOA_contr3                -0.51      9.62   -19.41    18.01 1.00     2362\nList_contr2:SOA_contr2     4.56     23.51   -41.17    49.62 1.00     1547\nList_contr3:SOA_contr2   -22.79     23.39   -68.03    22.43 1.00     1413\nList_contr2:SOA_contr3   -24.04     23.99   -69.89    23.34 1.00     1550\nList_contr3:SOA_contr3   -22.53     23.87   -68.93    26.21 1.00     1529\n                       Tail_ESS\nIntercept                  1447\nList_contr2                1661\nList_contr3                1626\nSOA_contr2                 1811\nSOA_contr3                 1872\nList_contr2:SOA_contr2     1750\nList_contr3:SOA_contr2     1421\nList_contr2:SOA_contr3     1395\nList_contr3:SOA_contr3     1671\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.63      2.91    41.09    52.66 1.00     1819     1386\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#deviation-sum-coding-1",
    "href": "practice-sheets/02b-catPreds-tutorial.html#deviation-sum-coding-1",
    "title": "Bayesian regression: theory & practice",
    "section": "Deviation (Sum) Coding",
    "text": "Deviation (Sum) Coding\nWith increasing number of levels within the factors the complexity and messiness of interpreting the differences between levels against each other increases considerably. Hence it might make a lot of sense to use the deviation coding scheme which provides estimates of effects comapred to the grand mean. We again can use the R built-in function to assign deviation coding to our three-level factors:\n\nlatinsquare_df %>%\n  mutate(List_dev = List,\n         SOA_dev = SOA) -> latinsquare_df\ncontrasts(latinsquare_df$List_dev) <- contr.sum(3) # insert number of levels\ncontrasts(latinsquare_df$SOA_dev) <- contr.sum(3)\n\nFor e.g. SOA our numeric variables now denote the effect of long SOA compared to the grand mean when \\(x_{2_2}\\) is a 1 and \\(x_{2_3}\\) is a 0; they denote the effect of medium SOA compared to the grand mean when \\(x_{2_2}\\) is a 0 and \\(x_{2_3}\\) is a 1; the effect of short SOA is never compared to the grand mean since it is always assigned a -1.\n\nlm3.dev.FE <- brm(RT ~ List_dev * SOA_dev,\n                   data = latinsquare_df,\n                 cores = 4,\n                 iter = 1000)\nlm3.dev.FE.coefs <- fixef(lm3.dev.FE)[,1] %>% as.numeric()\nsummary(lm3.dev.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_dev * SOA_dev \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            534.49      3.89   526.82   542.23 1.00     2524     1510\nList_dev1              2.33      5.46    -8.37    12.89 1.00     1474     1352\nList_dev2             10.05      5.34    -0.63    20.62 1.00     1465     1553\nSOA_dev1              -0.62      5.45   -11.47     9.92 1.00     1932     1500\nSOA_dev2               1.50      5.63   -10.18    12.63 1.00     1968     1396\nList_dev1:SOA_dev1    -6.51      7.68   -21.18     9.19 1.00     1666     1631\nList_dev2:SOA_dev1    -1.14      7.93   -16.86    14.54 1.00     1425     1525\nList_dev1:SOA_dev2    -1.78      7.55   -16.56    12.70 1.00     1804     1710\nList_dev2:SOA_dev2    10.58      7.83    -4.29    25.85 1.00     1592     1264\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.49      2.80    41.40    52.37 1.00     2219     1445\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThink about what the single estimates mean!"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#helmert-coding-1",
    "href": "practice-sheets/02b-catPreds-tutorial.html#helmert-coding-1",
    "title": "Bayesian regression: theory & practice",
    "section": "Helmert Coding",
    "text": "Helmert Coding\nFor recap: In this coding scheme, a level of a variable is compared to its subsequent levels. What does this mean for the three-level factors?\n\nlatinsquare_df %>%\n  mutate(List_helm = List,\n         SOA_helm = SOA) -> latinsquare_df\nhelm.matrix3 <- matrix(c(2/3, -1/3, -1/3, 0, 1/2, -1/2 ), ncol = 2)\ncontrasts(latinsquare_df$List_helm) <- helm.matrix3\ncontrasts(latinsquare_df$SOA_helm) <- helm.matrix3\n\n\nlm3.helm.FE <- brm(RT ~ List_helm * SOA_helm,\n                   data = latinsquare_df,\n                  cores = 4,\n                  iter = 1000)\nlm3.helm.FE.coefs <- fixef(lm3.helm.FE)[,1] %>% as.numeric()\nsummary(lm3.helm.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_helm * SOA_helm \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept              534.62      4.02   527.08   542.37 1.00     4337\nList_helm1               3.38      8.71   -14.06    21.29 1.00     4153\nList_helm2              22.49      9.43     3.90    41.67 1.00     4177\nSOA_helm1               -0.93      8.08   -16.40    15.48 1.00     4516\nSOA_helm2                2.98      9.08   -15.17    20.05 1.00     4569\nList_helm1:SOA_helm1   -14.94     17.32   -48.48    19.96 1.00     4332\nList_helm2:SOA_helm1   -13.04     20.49   -53.46    27.64 1.00     4377\nList_helm1:SOA_helm2   -14.19     20.36   -53.70    25.11 1.00     5201\nList_helm2:SOA_helm2    28.50     23.11   -16.79    71.97 1.00     3660\n                     Tail_ESS\nIntercept                1493\nList_helm1               1257\nList_helm2               1621\nSOA_helm1                1565\nSOA_helm2                1467\nList_helm1:SOA_helm1     1375\nList_helm2:SOA_helm1     1589\nList_helm1:SOA_helm2     1508\nList_helm2:SOA_helm2     1453\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.57      2.75    41.54    52.49 1.00     3405     1546\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe main effect estimates denote the differences between the mean of List1 and the mean of (List2 + List3); and between the mean of List2 and the mean of List3. Respectively, they denote the differences between the mean of SOA long and the mean of (medium + short); and between the mean of SOA medium and the mean of short. The intercept is the grand mean."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#reverse-helmert-coding",
    "href": "practice-sheets/02b-catPreds-tutorial.html#reverse-helmert-coding",
    "title": "Bayesian regression: theory & practice",
    "section": "Reverse Helmert Coding",
    "text": "Reverse Helmert Coding\nThe reverse Helmert coding scheme (also called difference coding) is quite similar to the Helmert coding, but compares the mean of a level to its previous levels. Since we basically reverse the coding we used in the previous scheme, we also ‘reverse’ the contrast matrix to create such a coding.\n\nlatinsquare_df %>%\n  mutate(List_rhelm = List,\n         SOA_rhelm = SOA) -> latinsquare_df\nrhelm.matrix3 <- matrix(c(-1/2, 1/2, 0, -1/3, -1/3, 2/3 ), ncol = 2)\ncontrasts(latinsquare_df$List_rhelm) <- rhelm.matrix3\ncontrasts(latinsquare_df$SOA_rhelm) <- rhelm.matrix3\n\n\nlm3.rhelm.FE <- brm(RT ~ List_rhelm * SOA_rhelm,\n                   data = latinsquare_df,\n                   iter = 1000,\n                   cores = 4)\nlm3.rhelm.FE.coefs <- fixef(lm3.rhelm.FE)[,1] %>% as.numeric()\nsummary(lm3.rhelm.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_rhelm * SOA_rhelm \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                534.55      3.76   527.17   541.83 1.00     4673\nList_rhelm1                7.79      9.67   -10.73    27.07 1.00     4045\nList_rhelm2              -18.53      8.03   -34.91    -2.09 1.00     3392\nSOA_rhelm1                 2.14      9.09   -15.65    19.54 1.00     4343\nSOA_rhelm2                -1.47      8.30   -16.98    14.77 1.00     4341\nList_rhelm1:SOA_rhelm1     6.12     22.93   -39.70    50.41 1.00     4243\nList_rhelm2:SOA_rhelm1   -24.63     20.38   -64.38    13.86 1.00     4589\nList_rhelm1:SOA_rhelm2   -25.76     19.54   -62.98    14.41 1.00     3873\nList_rhelm2:SOA_rhelm2     2.13     16.72   -30.49    36.18 1.00     5767\n                       Tail_ESS\nIntercept                  1516\nList_rhelm1                1336\nList_rhelm2                1229\nSOA_rhelm1                 1353\nSOA_rhelm2                 1383\nList_rhelm1:SOA_rhelm1     1644\nList_rhelm2:SOA_rhelm1     1509\nList_rhelm1:SOA_rhelm2     1141\nList_rhelm2:SOA_rhelm2     1426\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.42      2.82    41.24    52.44 1.00     3219     1627\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn our example, the first estimate denotes the difference between the mean of List2 and the mean of List1; the second - the difference between the mean of List3 and the mean of (List1 + List2). The main effects of SOA can be interpreted similarly."
  },
  {
    "objectID": "practice-sheets/02b-catPreds-tutorial.html#mixed-schemes-dummy-and-deviation-coding",
    "href": "practice-sheets/02b-catPreds-tutorial.html#mixed-schemes-dummy-and-deviation-coding",
    "title": "Bayesian regression: theory & practice",
    "section": "Mixed Schemes: Dummy and Deviation Coding",
    "text": "Mixed Schemes: Dummy and Deviation Coding\nJust like with two-level factors, we might wish to use different coding schemes for different predictors. It might make sense to use dummy coding for a variable which has a control and two different treatment conditions, and to use deviation coding for a variable which has ‘equal’ levels.\nFor example, we could use dummy-coding for List and deviation-coding for SOA.\n\nlm3.mixedCode.FE <- brm(RT ~ List * SOA_dev,\n                   data = latinsquare_df,\n                   cores = 4,\n                   iter = 1000)\nlm3.mixedCode.FE.coefs <- fixef(lm3.mixedCode.FE)[,1] %>% as.numeric()\nsummary(lm3.mixedCode.FE)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List * SOA_dev \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         536.86      6.43   524.71   550.08 1.00     1899     1367\nListL2              7.64      9.18   -10.80    25.65 1.00     2113     1456\nListL3            -14.67      9.54   -33.18     4.45 1.00     2127     1621\nSOA_dev1           -7.17      9.44   -25.35    11.35 1.01     1199     1390\nSOA_dev2           -0.11      9.31   -17.94    18.74 1.01     1126     1365\nListL2:SOA_dev1     5.53     13.45   -21.04    32.30 1.00     1284     1336\nListL3:SOA_dev1    14.37     13.65   -13.08    39.29 1.00     1401     1361\nListL2:SOA_dev2    11.89     13.03   -12.84    37.04 1.00     1183     1341\nListL3:SOA_dev2    -6.65     13.24   -33.02    18.72 1.00     1266     1474\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.50      2.85    41.37    52.43 1.00     2126     1370\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nTowards the end of this tutorial, the main take-away is this: when you look at the estimates of (any) model, you could ask yourself a couple of questions like these to make sure you understand what was calculated:\n\nWhat does the intercept represent?\nWhat do the single estimates mean?\nWhat do they tell me about my hypotheses?\n\nOf course, you will also encounter experimental designs which use a two-level and a three-level categorical predictors – but the conceptual basics regarding how to choose the contrasts and how to interpret linear models are the same."
  },
  {
    "objectID": "practice-sheets/05b-hierarchical-models-exercises.html",
    "href": "practice-sheets/05b-hierarchical-models-exercises.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Load relevant packages and “set the scene.”\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models)."
  },
  {
    "objectID": "practice-sheets/05b-hierarchical-models-exercises.html#exercise-1-logistic-regression",
    "href": "practice-sheets/05b-hierarchical-models-exercises.html#exercise-1-logistic-regression",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercise 1: Logistic regression",
    "text": "Exercise 1: Logistic regression\nConsider the following model formula for the dolphin data set:\n\nbrms::bf(MAD ~ condition + \n     (condition || subject_id) +\n     (condition || exemplar))\n\nMAD ~ condition + (condition || subject_id) + (condition || exemplar) \n\n\n\n\n\n\n\n\nExercise 1a\nWhy is the random effect structure of this model questionable? Can we meaningfully estimate all parameters? (Tip: Think about what group levels vary across predictor levels)\n\n\n\n\n\nShow solution\n# Answer: `condition` is not crossed with `exemplar`. An exemplar is either typical or atypical, thus a random slope does not make sense.\n\n\n\n\n\n\n\n\nExercise 1b\nUse the following data frame:\n\n# set up data frame\ndolphin_correct <- dolphin %>% \n  filter(correct == 1) %>% \n  mutate(log_RT_s = my_scale(log(RT)),\n         AUC_s = my_scale(AUC))\n\nRun a multilevel model that predicts AUC_s based on condition. Specify maximal random effect structures for exemplars and subject_ids (ignore correlations between intercepts and slopes for now). Specify a seed = 98.\nIf you encounter “divergent transition” warning, make them go away by refitting the model appropriately (Tip: Brms gives very useful, actionable advice)\n(This might take a couple of minutes, get used to it ;)\n\n\n\n\n\nShow solution\n# refit with upped adapt_delta and max_treedepth\nxmdl_AUC2 <- brm(AUC_s ~ condition +\n                  (condition || subject_id) +\n                  (1 | exemplar),\n                data = dolphin_correct,\n                control=list(adapt_delta=0.99, max_treedepth=15), \n                seed = 98\n                )\nxmdl_AUC2\n\n\n\n\n\n\n\n\nExercise 1c\nYou want to run a multilevel model that predicts log_RT_s based on group. You want to account for group-level variation of both subject_id and exemplar. What kind of groupings can be meaningfully estimated, given the dataset and the experimental design. You can check the crossing of different vectors with xtabs() for example.\n\n\n\n\n\nShow solution\n# check crossing\nxtabs(~ group + subject_id, dolphin_correct)\n# individual subject_ids contributed data only to one group because it is a between-subject design\n# --> we need varying intercepts only, i.e. a different base-rate for subjects\n\nxtabs(~ group + exemplar, dolphin_correct)\n# each exemplar contributes data to both groups\n# --> we can integrate varying intercepts and slopes for exemplars\n\n\n\n\n\n\n\n\nExercise 1d\nRun a multilevel model that predicts log_RT_s based on group and add maximal random effect structures licensed by the experimental design (ignore possible random intercept-slope interactions for now).\nSpecify weakly informative priors as you see fit.\n\n\n\n\n\nShow solution\npriors <- c(\n  #priors for all fixed effects (group)\n  set_prior(\"student_t(3, 0, 3)\", class = \"b\"),\n  #prior for the Intercept\n  set_prior(\"student_t(3, 0, 3)\", class = \"Intercept\"),\n  #prior for all SDs including the varying intercepts and slopes\n  set_prior(\"student_t(3, 0, 3)\", class = \"sd\")\n)\n\nxmdl <- brm(log_RT_s ~ group + \n              (1 | subject_id) +\n              (group || exemplar),\n            prior = priors,\n            data = dolphin_correct)\nxmdl\n\n\n\n\n\n\n\n\nExercise 1e\nExtract the posterior means and 95% CrIs of touch vs. click log_RT_s and plot them.\n\n\n\n\n\nShow solution\n# Extract the posteriors\nposteriors <- xmdl %>%\n  spread_draws(b_Intercept, \n               b_grouptouch) %>%\n  # calculate posteriors for each individual level\n  mutate(click = b_Intercept,\n         touch = b_Intercept + b_grouptouch) %>% \n  select(click, touch) %>% \n  gather(key = \"parameter\", value = \"posterior\") %>% \n  group_by(parameter) %>% \n  summarise(mean_posterior = mean(posterior),\n            `95lowerCrI` = HDInterval::hdi(posterior, credMass = 0.95)[1],\n            `95higherCrI` = HDInterval::hdi(posterior, credMass = 0.95)[2])\n\n# plot\nggplot(data = posteriors, \n       aes(x = parameter, y = mean_posterior,\n           color = parameter, fill = parameter)) + \n  geom_errorbar(aes(ymin = `95lowerCrI`, ymax = `95higherCrI`),\n                width = 0.2, color = \"grey\") +\n  geom_line(aes(group = 1), color = \"black\") +\n  geom_point(size = 4) +\n  labs(x = \"group\",\n       y = \"posterior log(RT) (scaled)\")\n\n\n\n\n\n\n\n\nExercise 1f\nAdd the posterior estimates for different exemplars to the plot. (Tip: Check code from the previous “tutorial” to extract the random effect estimates.)\n\n\n\n\n\nShow solution\n# extract the random intercepts for exemplars\nrandom_intc_matrix <- ranef(xmdl)$exemplar[, , \"Intercept\"] %>% \n  round(digits = 2) \n\n# extract the by-exemplar random slopes for group\nrandom_slope_matrix <- ranef(xmdl)$exemplar[, , \"grouptouch\"] %>% \n  round(digits = 2)\n\n# random intercepts to dataframe\nrandom_intc_df <- data.frame(exemplar = row.names(random_intc_matrix), random_intc_matrix) %>% \n  select(exemplar, Estimate) %>% \n  rename(rintercept = Estimate)\n\n# combine with random slope matrix\nrandom_slope_df <- data.frame(exemplar = row.names(random_slope_matrix), random_slope_matrix) %>% \n  select(exemplar, Estimate) %>% \n  rename(rslope = Estimate) %>% \n  full_join(random_intc_df) %>% \n  # add population parameters and group-specific parameters\n  mutate(click_population = fixef(xmdl)[1],\n         touch_population = fixef(xmdl)[1] + fixef(xmdl)[2],\n         click = rintercept + click_population,\n         touch = rintercept + rslope + touch_population) %>% \n  select(exemplar, touch, click) %>% \n  gather(parameter, mean_posterior, -exemplar)\n  \n\n# combine with plot\nggplot(data = posteriors, \n       aes(x = parameter, y = mean_posterior,\n           color = parameter, fill = parameter)) + \n   # add random estimates\n  geom_point(data = random_slope_df, \n             alpha = 0.4,\n             size = 2,\n             position = position_jitter(width = 0.01)\n             ) +\n  # add lines between random estimates\n  geom_line(data = random_slope_df, \n            aes(group = exemplar),\n            color = \"grey\", alpha = 0.3) +\n  # add population-level estimates\n  geom_errorbar(aes(ymin = `95lowerCrI`, ymax = `95higherCrI`),\n                width = 0.2, color = \"grey\") +\n  geom_line(aes(group = 1), size = 2, color = \"black\") +\n  geom_point(size = 4, pch = 21, color = \"black\") +\n  labs(x = \"group\",\n       y = \"posterior log(RT) (scaled)\")"
  },
  {
    "objectID": "practice-sheets/05b-hierarchical-models-exercises.html#exercise-2-poisson-regression",
    "href": "practice-sheets/05b-hierarchical-models-exercises.html#exercise-2-poisson-regression",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercise 2: Poisson regression",
    "text": "Exercise 2: Poisson regression\n\n\n\n\n\n\nExercise 2a\nRun a multilevel poisson regression predicting xpos_flips based on group, log_RT_s, and their two-way interaction. Specify maximal random effect structures for exemplars and subject_ids licensed by the design (ignore correlations between intercepts and slopes for now). (Tip: allow groupings to differ regarding the interaction effect if licensed by the design.) Specify weakly informative priors.\n\n\n\n\n\nShow solution\npriors <- c(\n  #priors for all fixed effects\n  set_prior(\"student_t(3, 0, 3)\", class = \"b\"),\n  #prior for all SDs including the varying intercepts and slopes for both groupings\n  set_prior(\"student_t(3, 0, 3)\", class = \"sd\")\n)\n\npoisson_mdl <- brm(xpos_flips ~ group * log_RT_s +\n                     (log_RT_s || subject_id) +\n                     (group * log_RT_s || exemplar),\n                   data = dolphin_correct,\n                   prior = priors,\n                   family = \"poisson\")\n\npoisson_mdl\n\n\n\n\n\n\n\n\nExercise 2b\nExtract and plot the population level estimates for both click and touch group as a regression line into a scatter plot (x = b_log_RT_s, y = xpos_flips).\n\n\n\n\n\nShow solution\n# extract posterior means for model coefficients\npredicted_Poisson_values <- poisson_mdl %>%\n  spread_draws(b_Intercept, b_log_RT_s, \n               b_grouptouch, `b_grouptouch:log_RT_s`\n               ) %>%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-5, 10, 0.5))) %>% \n  unnest(log_RT) %>%\n  mutate(click = exp(b_Intercept + b_log_RT_s*log_RT),\n         touch = exp(b_Intercept + b_log_RT_s*log_RT +\n                            b_grouptouch + `b_grouptouch:log_RT_s`*log_RT)) %>%\n  select(log_RT, click, touch) %>% \n  gather(group, posterior, -log_RT) %>% \n  group_by(log_RT, group) %>%\n  summarise(pred_m = mean(posterior, na.rm = TRUE),\n            pred_low = quantile(posterior, prob = 0.025),\n            pred_high = quantile(posterior, prob = 0.975)\n            ) \n\n# plot population level\nggplot(data = predicted_Poisson_values, aes(x = log_RT, y = pred_m)) +\n  geom_point(data = dolphin_correct, aes(x = log_RT_s, y = xpos_flips, color = group), \n             position = position_jitter(height = 0.2), alpha = 0.2) +\n  geom_line(aes(y = pred_m, color = group), size = 2) +\n  facet_grid(~group) +\n  ylab(\"Predicted prob of xflips\") +\n  ylim(-1,10) +\n  xlim(-5,10)\n\n\n\n\n\n\n\n\nExercise 2c\nExtract the respective subject-specific estimates from the model and plot them into the same plot (use thinner lines).\n\n\n\n\n\nShow solution\n# extract the random effects for subject_id\n\n# intercepts\nrandom_intc_matrix <- ranef(poisson_mdl)$subject_id[, , \"Intercept\"] %>% \n  round(digits = 3)\n\n# slopes\nrandom_slope_matrix <- ranef(poisson_mdl)$subject_id[, , \"log_RT_s\"] %>% \n  round(digits = 3)\n\n# to df\nrandom_intc_df <- data.frame(subject_id = row.names(random_intc_matrix), random_intc_matrix) %>% \n  select(subject_id, Estimate) %>% \n  rename(rintercept = Estimate)\n\n# wrangle into one df \nrandom_slope_df <- data.frame(subject_id = row.names(random_slope_matrix), random_slope_matrix) %>% \n  select(subject_id, Estimate) %>% \n  rename(rslope = Estimate) %>% \n  full_join(random_intc_df) %>% \n  expand_grid(group = c(\"click\", \"touch\")) %>% \n  # add population parameters and group-specific parameters\n  mutate(adjusted_int = ifelse(group == \"click\",\n           rintercept + fixef(poisson_mdl)[1],\n           rintercept + fixef(poisson_mdl)[1] + fixef(poisson_mdl)[2]),\n         adjusted_slope = ifelse(group == \"click\",\n           rslope + fixef(poisson_mdl)[3],\n           rslope + fixef(poisson_mdl)[3] + fixef(poisson_mdl)[4])) %>% \n  mutate(log_RT = list(seq(-5, 10, 0.5))) %>% \n  unnest(log_RT) %>%\n  select(subject_id, log_RT, group, \n         adjusted_int, adjusted_slope) %>% \n  group_by(subject_id, log_RT, group) %>%\n  mutate(pred_m = exp(adjusted_int + adjusted_slope*log_RT))\n\n# plot the individual regression lines on top of the population estimate\nggplot(data = predicted_Poisson_values, aes(x = log_RT, y = pred_m)) +\n  geom_point(data = dolphin_correct, aes(x = log_RT_s, y = xpos_flips), \n             position = position_jitter(height = 0.2), alpha = 0.01) +\n  geom_line(aes(y = pred_m, color = group), size = 2) +\n  geom_line(data = random_slope_df, \n            aes(x = log_RT, y = pred_m, group = subject_id, color = group),\n            size = 0.5, alpha = 0.2) +\n  facet_grid(~group) +\n  ylab(\"Predicted prob of xflips\") +\n  ylim(-1,10) +\n  xlim(-5,10)"
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html",
    "href": "practice-sheets/01b-simple-regression.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models)."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#data-wrangling",
    "href": "practice-sheets/01b-simple-regression.html#data-wrangling",
    "title": "Bayesian regression: theory & practice",
    "section": "Data wrangling",
    "text": "Data wrangling\n\n# aggregate\ndolphin_agg <- dolphin |> \n  filter(correct == 1) |> \n  group_by(subject_id) |> \n  dplyr::summarize(\n            AUC = median(AUC, na.rm = TRUE),\n            MAD = median(MAD, na.rm = TRUE)) \n  \n# let's have a look\nhead(dolphin_agg)\n\n# A tibble: 6 × 3\n  subject_id     AUC    MAD\n       <dbl>   <dbl>  <dbl>\n1       1001  55200. 111.  \n2       1002  59596.  87.9 \n3       1003 -17772  -34.1 \n4       1004  -3600.  -3.83\n5       1005  54054   95.0 \n6       1006  60396. 155."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#visual-assessment",
    "href": "practice-sheets/01b-simple-regression.html#visual-assessment",
    "title": "Bayesian regression: theory & practice",
    "section": "Visual assessment",
    "text": "Visual assessment\nBefore we start thinking about statistical inference, we always want to get a feel for the data visually. You basically always want to plot the data.\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_point(size = 3, alpha = 0.3) \n\n\n\n\nThis graph displays the distribution of AUC and MAD values.\nLooking at the plot, we can see that there is a strong relationship between AUC and MAD. And that makes a lot of sense. The larger the cursor strives toward the competitor, the larger is the overall area under the curve. Heureka! Our hypothesis is confirmed.\nBut wait! As Bayesians, we would like to translate the data into an expression of evidence: do the data provide evidence for our research hypotheses? Also, notice that there is some variability. We want precise estimates of potential effects. We also want a measure of how certain we can be about these estimates."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#bayesian-linear-regression-with-brms",
    "href": "practice-sheets/01b-simple-regression.html#bayesian-linear-regression-with-brms",
    "title": "Bayesian regression: theory & practice",
    "section": "Bayesian linear regression with brms",
    "text": "Bayesian linear regression with brms\nThe brms package allows us to run Bayesian regression models, both simple and rather complex. It uses a sampling method, so its output will be vectors of (corellated) samples from the posterior distribution of the model’s parameters. (We will learn how this sampling method works later).\nSo, to quantify evidence and uncertainty with posterior samples, let’s run a simple linear regression model using brms. We use the R notation that some of your might already be familiar with when using lm(). We specify a formula in which AUC is predicted by MAD.\nAUC ~ MAD\nWhen you run this code, the brms package generates Stan code and runs the Stan program in the background. Stan code is executed in C++, and the model will be ‘compiled’ (you get information about this in the console output). We will learn later what this compilation does (spoiler: it computes gradients for all stochastic nodes in the model). The only thing that is relevant for you at the moment is this: This compilation can take quite a while (especially for complex models) before anything happens.\n\n# specify the model \nmodel1 = brm(\n  # model formula\n  AUC ~ MAD, \n  # data\n  data = dolphin_agg\n  )\n\n\nsummary(model1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: AUC ~ MAD \n   Data: dolphin_agg (Number of observations: 108) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   517.00   1853.56 -3135.22  4107.37 1.00     4020     3072\nMAD         454.87     16.04   423.36   485.96 1.00     3986     3036\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma 17182.05   1183.01 15019.53 19655.38 1.00     4334     3058\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe output of such a model looks very familiar if you have worked with lm() before. We want to look at what is here called “Population-Level Effects”, which is a small table in this case. The first column contains the names of our coefficients; the Estimate column gives us the posterior mean of these coefficients; the Est.Error give us the standard error; the l-95%and u-95% give us the lower and upper limit of the 95% Credible Interval (henceforth CrI). The column Rhat (R^) which is a diagnostic of chain convergence and should not diverge much from 1 (rule of thumb: should by <1.1). Again, more about that later. The Bulk_ESS and Tail_ESS columns give us numbers of “useful” samples. This number should be sufficiently high. If its not, brms will give you a convenient warning (more about that later, so don’t worry for now). If that happens, you need to increase the chains and / or the number of iterations in order to increase the overall number of samples (again, don’t worry for now).\nIf we need the main summary output in a tidy tibble format, we can use this function from the tidybayes package:\n\ntidybayes::summarise_draws(model1)\n\n# A tibble: 5 × 10\n  variable    mean  median      sd     mad      q5     q95  rhat ess_b…¹ ess_t…²\n  <chr>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>   <dbl>   <dbl>\n1 b_Inter…   517.    529.  1.85e+3 1.83e+3 -2523.   3581.   1.00   4020.   3072.\n2 b_MAD      455.    455.  1.60e+1 1.54e+1   429.    482.   1.00   3986.   3036.\n3 sigma    17182.  17127.  1.18e+3 1.19e+3 15399.  19182.   1.00   4334.   3058.\n4 lprior     -22.3   -22.3 3.03e-2 2.88e-2   -22.4   -22.3  1.00   4093.   2614.\n5 lp__     -1218.  -1218.  1.23e+0 1.01e+0 -1221.  -1217.   1.00   2056.   2659.\n# … with abbreviated variable names ¹​ess_bulk, ²​ess_tail\n\n\nThe model output suggests that the posterior mean of the Intercept is around 517 . The coefficient for MAD is estimated to be about 455.\nTo see how good a fit this is, we should manually draw this line into the graph from above.\n\n# extract model parameters:\nmodel_intercept <- summary(model1)$fixed[1,1]\nmodel_slope <- summary(model1)$fixed[2,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(intercept = model_intercept, slope = model_slope, color = project_colors[2], size  = 1) +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1])\n\n\n\n\nLooking at the graph, it does make sense, right? The red line seems to capture the main trend pretty well.\nNow is there a relationship between AUC and MAD? What would it mean if there was no relationship between these two measures? Well no relationship would mean a slope of 0. How would that look like?\n\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(intercept = model_intercept, slope = model_slope, color = project_colors[2], size = 1) +\n  geom_abline(intercept = model_intercept, slope = 0, color = project_colors[3], size = 1, lty = \"dashed\") +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1])\n\n\n\n\nThese lines look quite different indeed. But Bayesian data analysis does not give us only one single line. It gives us infinitely many lines, weighted by plausibility. Let’s explore this universe of weighted predictions."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#extracting-posterior-distributions-and-plotting-them",
    "href": "practice-sheets/01b-simple-regression.html#extracting-posterior-distributions-and-plotting-them",
    "title": "Bayesian regression: theory & practice",
    "section": "Extracting posterior distributions and plotting them",
    "text": "Extracting posterior distributions and plotting them\nWe can interpret and visualize our coefficients immediately. We can create a data frame with all posterior samples for each parameter and plot those distributions for all coefficients. Let’s first see what coefficients there are with the get_variables() function from the tidybayes package.\n\n# inspect parameters\ntidybayes::get_variables(model1)\n\n [1] \"b_Intercept\"   \"b_MAD\"         \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\nEverything that is preceded by a b_ is a population level coefficients, i.e. our predictors. Now let’s wrangle this data frame to get what we need. You don’t have to entirely understand the following code, but make sure you understand it well enough to recycle it later on.\n\n# wrangle data frame\nposteriors1 <- model1 |>\n  tidybayes::spread_draws(b_MAD, b_Intercept) |>\n  select(b_MAD, b_Intercept)\n\nposteriors1\n\n# A tibble: 4,000 × 2\n   b_MAD b_Intercept\n   <dbl>       <dbl>\n 1  465.       -872.\n 2  454.        165.\n 3  470.       -845.\n 4  476.        157.\n 5  435.        908.\n 6  452.       -598.\n 7  469.      -2528.\n 8  462.      -1183.\n 9  446.        520.\n10  443.      -2153.\n# … with 3,990 more rows\n\n\nNow that we know how to extract posterior samples, let’s actually take a bunch of these samples and plot them as lines into our scatter plot from above. In this code chunk we generate a subsample of 100 parameter pairs.\n\n# wrangle data frame\nposteriors2 <- model1 |>\n  # parameter 'ndraws' requests 100 random subsamples\n  tidybayes::spread_draws(b_MAD, b_Intercept, ndraws = 100) |>\n  select(b_MAD, b_Intercept)\n  \n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(data = posteriors2,\n              aes(intercept = b_Intercept, slope = b_MAD), \n              color = project_colors[2], size  = 0.1, alpha = 0.4) +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1]) +\n  theme_aida()\n\n\n\n\n\n\n\n\nGiven our model, assumptions and data, these are 100 plausible regression lines. As you can see they are very similar.\nUsing this pipeline we can also calculate the mean of the posteriors and any kind of Credible Interval (CrI). We first extract the posterior and bring them into a tidy form. Let’s only look at the coefficient for MAD here.\n\nposteriors3 <- model1 |>\n   # use the gather_draws() function for \"long data\"\n   tidybayes::gather_draws(b_MAD) |> \n   # change names of columns\n   rename(parameter = .variable,\n          posterior = .value) |> \n   # select only those columns that are relevant\n   select(parameter, posterior)\n\nhead(posteriors3)\n\n# A tibble: 6 × 2\n# Groups:   parameter [1]\n  parameter posterior\n  <chr>         <dbl>\n1 b_MAD          465.\n2 b_MAD          454.\n3 b_MAD          470.\n4 b_MAD          476.\n5 b_MAD          435.\n6 b_MAD          452.\n\n\nAnd then calculate the mean, the lower and the upper bound of a 90% CrI, using the function tidybayes::hdi().\n\n# get posteriors for the relevant coefficients\nposteriors3_agg <- posteriors3 |> \n  group_by(parameter) |> \n  summarise(\n    `90lowerCrI`   = tidybayes::hdi(posterior, credMass = 0.90)[1],\n    mean_posterior = mean(posterior),\n    `90higherCrI`  = tidybayes::hdi(posterior, credMass = 0.90)[2])\n\nposteriors3_agg \n\n# A tibble: 1 × 4\n  parameter `90lowerCrI` mean_posterior `90higherCrI`\n  <chr>            <dbl>          <dbl>         <dbl>\n1 b_MAD             424.           455.          486.\n\n\nNow we use this newly created data frame to plot the posterior distributions of all population-level coefficients. Again, we use our new best friend, the tidybayes package which offers some sweet extensions to ggplot’s geom_ family of functions. We also add a reference point to compare the posteriors against. A common and reasonable reference point is 0. Remember a slope coefficient of zero would correspond to a flat regression line.\n\n# plot the regression coefficients\nposteriors1 |> \n  pivot_longer(cols = everything(), names_to = \"parameter\", values_to = \"posterior\") |> \n  ggplot(aes(x = posterior, y = parameter, fill = parameter)) + \n    # plot density w/ 90% credible interval\n    tidybayes::stat_halfeye(.width = 0.9) +\n    # add axes titles\n    xlab(\"\") +\n    ylab(\"\") +\n    # adjust the x-axis \n    scale_x_continuous(limits = c(-100,600)) +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n                 lty = \"dashed\") +\n    theme(legend.position=\"none\")\n\n\n\nposteriors3_agg[1,2]\n\n# A tibble: 1 × 1\n  `90lowerCrI`\n         <dbl>\n1         424.\n\n\nHere you see density plots for our critical coefficients of the model. We care mostly about the slope coefficient (b_MAD) (the posterior of which is shown in red). Values between about 423.9779831 and about 486.4311161 are plausible (at the 90% level) and they are indicated by the thick black line in the density plot for this coefficient. The mean of the distribution is indicated by the thick black dot.\nThat’s helpful because we can relate this distribution to relevant values, for example the value 0 (dashed line). If you look at the coefficient, you can see that the posterior distribution does not include the value zero or any small-ish “Region of Practical Equivalence” around it. In fact, the posterior is really far away from zero. Thus, if we believe in the data and the model, we can be very certain that this coefficient is not zero. In other words, we would be very certain that there is a positive relationship between AUC and MAD (and in turn that ‘no relationship’ is not a very plausible scenario).\nThe brms package allows us to quickly evaluate how many posterior samples fall into a certain value range. Just for fun, let’s calculate the amount of posterior samples that are larger than 450. The following code chunk does this for us:\n\nhypothesis(model1, 'MAD > 450')\n\nHypothesis Tests for class b:\n       Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob\n1 (MAD)-(450) > 0     4.87     16.04   -21.32    31.86       1.71      0.63\n  Star\n1     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nThe results tell us that more than 60% of all posterior samples are larger than 450. It also tells us the evidence ratio (more on this later), which is the odds of the hypothesis in question (here ’MAD > 450)."
  },
  {
    "objectID": "practice-sheets/01b-simple-regression.html#exercises-for-simple-regression-modeling",
    "href": "practice-sheets/01b-simple-regression.html#exercises-for-simple-regression-modeling",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercises for simple regression modeling",
    "text": "Exercises for simple regression modeling\n\n\n\n\n\n\nExercise 1a\nMassage the data and create a new dataset that contains only correct responses and only the mean values of the RT and the AUC measurement for each participant (subject_id). Print out the head of the dataset.\n\n\n\n\n\nShow solution\n# aggregate\ndolphin_agg <- dolphin |> \n  filter(correct == 1) |> \n  group_by(subject_id) |> \n  dplyr::summarize(AUC = mean(AUC, na.rm = TRUE),\n            RT = mean(RT, na.rm = TRUE))\n  \n# let's have a look\nhead(dolphin_agg)\n\n\nWe know from the previous exercises (walkthrough) that the area-under-the-curve (AUC) is related to the maximum absolute deviation (MAD). But what about reaction times (RTs)? Isn’t it plausible that RTs are also related to AUC? The further I curve away from the target with the cursor, the longer it takes me to arrive at the target, right?\n\n\n\n\n\n\nExercise 1b\nPlot the relationship between RT and AUC in a scatterplot.\n\n\n\n\n\nShow solution\n# plot\nggplot(data = dolphin_agg, \n       aes(x = RT, \n           y = AUC)) + \n  geom_point(size = 3, alpha = 0.3)\n\n\n\n\n\n\n\n\nExercise 1c\nRun a linear regression using brms. AUC is the dependent variable (i.e. the measure) and RT is the independent variables (i.e. the predictor). The formula writes: AUC ~ RT\n\n\n\n\n\nShow solution\n# specify the model \nmodel1 <- brm(\n  # model formula\n  AUC ~ RT, \n  # data\n  data = dolphin_agg\n  )\n\nsummary(model1)\n\n\n\n\n\n\n\n\nExercise 1d\nLook at the model output. Think of it in terms of a line in the scatterplot from (1b). Where does the regression line cross the y-axis, what is the slope of the line? Draw a scatterplot of AUC against RT and add the predicted values as a line.\n\n\n\n\n\nShow solution\n# extract model parameters:\nmodel_intercept <- summary(model1)$fixed[1,1]\nmodel_slope <- summary(model1)$fixed[2,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = RT, \n           y = AUC)) + \n  geom_abline(aes(intercept = model_intercept, slope = model_slope),\n              color = project_colors[2], size = 2) +\n  geom_point(size = 3, alpha = 0.3)\n\n\nThat doesn’t really look like a tight linear relationship, right? If there is any relationship, AUC values become lower with longer reaction times (the line has a negative slope).\n\n\n\n\n\n\nExercise 1e\nNow create a new data frame which contains the extracted posteriors for b_RT from the model output (use the spread_draws() function). Print out the head of the new dataset.\n\n\n\n\n\nShow solution\n# get posteriors for the relevant coefficients\nposteriors1 <- model1 |>\n  # use the spread_draws() function of tidybayes for all relevant parameters\n  spread_draws(b_RT) |>\n  # select only those columns that are relevant\n  select(b_RT) |> \n  # bring into long format\n  gather(key = \"parameter\", value = \"posterior\")\n  \nhead(posteriors1)\n\n\n\n\n\n\n\n\nExercise 1f\nPlot the results with the `geom_halfeyeh() function. Add a vertical line at zero.\n\n\n\n\n\nShow solution\n# plot the regression coefficients\n  ggplot(posteriors1, aes(x = posterior, y = parameter)) + \n    # plot density \n    tidybayes::stat_halfeye(.width = 0.95) +\n    # add axes titles\n    xlab(\"\\nb_RT posterior distribution\") +\n    ylab(\"\") +\n    # adjust the x-axis \n    scale_x_continuous(expand = c(0, 0), limits = c(-100,100)) +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n                 lty = \"dashed\")\n\n\nNow: What is the picture telling us? Is there reason to believe in a relationship between AUC and RT? Think about it!\nThere is no compelling support for a belief in a relationship between AUC and RT. The value zero (no relationship) is contained in the 95% CrI and a non-trivial amount of posterior samples is larger than 0."
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html",
    "href": "practice-sheets/03a-GLM-tutorial.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\nThis tutorial covers common types of generalized linear regression models (GLMs):\nThe shared form of all of these GLMs is the following “feed-forward computation” (here illustrated for a single datum of the predicted variable \\(y\\) for a vector \\(x\\) of predictor variables and a vector of coefficients \\(\\beta\\):\nLink function and likelihood function may have additional free parameters, \\(\\theta_{\\text{LF}}\\) and \\(\\theta_{\\text{LH}}\\), to be fitted alongside the regression coefficients.\nSimple linear regression is the special case of this scheme where the link function is just the identity function and the likelihood is given by $y \\sim \\mathcal{N}(\\eta; \\sigma)$. Different types of regression are used to account for different kinds predicted variable \\(y\\):"
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#explanation",
    "href": "practice-sheets/03a-GLM-tutorial.html#explanation",
    "title": "Bayesian regression: theory & practice",
    "section": "Explanation",
    "text": "Explanation\nIn logistic regression, the response variable \\(y\\) is binary, i.e., we want to predict the probability \\(p\\) with which one of two possible outcomes (henceforth: the reference outcome) occurs. The likelihood function for this case is the Bernoulli distribution. This requires a link function \\(LF\\) that maps real-valued linear predictor values \\(\\xi\\) onto the unit interval. A common choice is the logistic function:\n\\[\n\\text{logistic}(\\xi) = \\frac{1}{1+ \\exp(-\\xi)} = \\eta\n\\]\n\n\n\n\n\nThe logistic regression model is then defined as:\n\\[\n\\begin{align*}\n\\xi  &= x \\cdot \\beta       && \\color{gray}{\\text{[linear predictor]}} \\\\\n\\eta &= \\text{logistic}(\\xi) && \\color{gray}{\\text{[predictor of central tendency]}} \\\\\ny & \\sim \\text{Bernoulli}(\\eta) && \\color{gray}{\\text{[likelihood]}}\n\\end{align*}\n\\]\nThe linear predictor values \\(\\xi\\) can be interpreted directly, as the log odds-ratio of the predicted probability \\(\\eta\\). This is because the inverse of the logistic function is the logit function, which has the following form:\n\\[\n\\text{logit}(\\eta) = \\log \\frac{\\eta}{1-\\eta} = \\xi\n\\]\n\nlogit = function(x) return( log(x/(1-x)) )\nggplot(data.frame(x = c(0.001,1-0.001)), aes(x)) +\n         stat_function(fun = logit, color = project_colors[2], size = 2) +\n  labs(label = \"logit function\", x = latex2exp::TeX(\"$\\\\eta$\"), y = latex2exp::TeX(\"$\\\\xi$ = logistic($\\\\eta$)\")) +\n  ggtitle(\"logit function\")\n\n\n\n\nThat also means that a difference in linear predictor parameters, e.g., in a logistic regression with a single binary categorical predictor variable (group A vs. group B), can be interpreted directly as something like the “evidence ratio” or “Bayes factor”. It is the log of the factor by which to transform log odds-ratios (e.g., changing beliefs from \\(\\eta_1\\) to \\(\\eta_2\\):\n\\[\n\\begin{align*}\n& \\xi_1 - \\xi_2 = \\log \\frac{\\eta_1}{1-\\eta_1} - \\log \\frac{\\eta_2}{1-\\eta_2} = \\log \\left ( \\frac{\\eta_1}{1-\\eta_1} \\frac{1-\\eta_2}{\\eta_2}\\right ) \\\\\n\\Leftrightarrow & \\frac{\\eta_1}{1-\\eta_1} = \\exp (\\xi_1 - \\xi_2) \\ \\frac{\\eta_2}{1-\\eta_2}\n\\end{align*}\n\\]\nFor the purposes of understanding which priors are weakly or strongly informative, a unit difference in the linear predictor can be interpreted as a log Bayes factor (changing prior odds to posterior odds). So a unit difference in the predictor value corresponds to a Bayes factor of around 2.72."
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#example",
    "href": "practice-sheets/03a-GLM-tutorial.html#example",
    "title": "Bayesian regression: theory & practice",
    "section": "Example",
    "text": "Example\nOur hypothesis is that typical examples are easier to classify, so they should have higher accuracy than atypical ones. We are also interested in additional effects of group on accuracy.\nAs usual, we begin by plotting the relevant data.\n\nsum_stats <- dolphin |> \n  group_by(group, condition) |> \n  tidyboot::tidyboot_mean(correct) |> \n  rename(accuracy = empirical_stat)\n  \nsum_stats\n\n# A tibble: 4 × 7\n# Groups:   group [2]\n  group condition     n accuracy ci_lower  mean ci_upper\n  <chr> <chr>     <int>    <dbl>    <dbl> <dbl>    <dbl>\n1 click Atypical    318    0.874    0.840 0.875    0.909\n2 click Typical     689    0.964    0.948 0.964    0.977\n3 touch Atypical    330    0.909    0.876 0.909    0.940\n4 touch Typical     715    0.941    0.924 0.941    0.958\n\nsum_stats |> \n  ggplot(aes(x = condition, y = accuracy, group = group, color = group)) +\n  geom_line(size = 1, position = position_dodge(0.2)) +\n  geom_point(size = 3, position = position_dodge(0.2)) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), \n                width = 0.1, size = 0.35, position = position_dodge(0.2))\n\n\n\n\nVisually, there might be a hint that typical trials had higher accuracy, but we cannot judge with the naked eye whether this is substantial.\nA logistic regression, regressing correct against group * condition, may tell us more. To run the logistic regression, we must tell the brms:brm() that we want to treat 0 and 1 as categories. To be sure, and also to directly dictate which of the two categories is the reference level, we use a factor (of strings) with explicit ordering.\n\nfit_logistic <- brm(\n  formula = correct ~ group * condition,\n  data = dolphin |> \n    mutate(correct = factor(ifelse(correct, \"correct\", \"incorrect\"),\n                            levels = c(\"incorrect\", \"correct\"))),\n  family = bernoulli()\n)\n\nsummary(fit_logistic)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: correct ~ group * condition \n   Data: mutate(dolphin, correct = factor(ifelse(correct, \" (Number of observations: 2052) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                       1.95      0.17     1.63     2.29 1.00     2686\ngrouptouch                      0.37      0.26    -0.15     0.88 1.00     2028\nconditionTypical                1.34      0.27     0.82     1.87 1.00     2046\ngrouptouch:conditionTypical    -0.88      0.38    -1.63    -0.12 1.00     1747\n                            Tail_ESS\nIntercept                       2873\ngrouptouch                      2171\nconditionTypical                2201\ngrouptouch:conditionTypical     2137\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nTo test whether typical examples had credibly higher accuracy, the faintr package can be used like so:\n\ncompare_groups(\n  fit_logistic,\n  higher = condition == \"Typical\",\n  lower  = condition == \"Atypical\"\n)\n\nOutcome of comparing groups: \n * higher:  condition == \"Typical\" \n * lower:   condition == \"Atypical\" \nMean 'higher - lower':  0.8966 \n95% HDI:  [ 0.5356 ; 1.256 ]\nP('higher - lower' > 0):  1 \nPosterior odds:  Inf \n\n\nBased on these results, we may conclude that, given the model and the data, we should believe that typical examples had higher accuracy.\n\n\n\n\n\n\nExercise 1a\nTest whether there is reason to believe, given model and data, that the touch group was more accurate than the click group. (After all, the click group could change their minds until the very last moment.)\n\n\n\n\n\nShow solution\ncompare_groups(\n  fit_logistic,\n  higher = group == \"click\",\n  lower  = group == \"touch\"\n)\n\n# there is no reason to believe (given model and data) that this conjecture is true\n\n\n\n\n\n\n\n\nExercise 1b\nIf you look back at the plot of accuracy, it looks as if the change from atypical to typical condition does not have the same effect, at least not at the same level of strength, for the click and the touch group, i.e., it seems that there is an interaction between these two variables (group and condition). Use the function brms::hypothesis() to examine the interaction term of the model fit. What do you conclude from this?\n\n\n\n\n\nShow solution\nbrms::hypothesis(fit_logistic, \"grouptouch:conditionTypical < 0\")\n\n# given model and data, it is very plausible to believe that there is an interaction between these two variables."
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#explanation-1",
    "href": "practice-sheets/03a-GLM-tutorial.html#explanation-1",
    "title": "Bayesian regression: theory & practice",
    "section": "Explanation",
    "text": "Explanation\nIn multinomial regression the predicted variable is categorical with more than two levels: \\(c_1, \\dots, c_k\\), \\(k > 2\\). We want to predict probabilities for each category \\(p_1, \\dots, p_k\\) (with some linear predictors, more on this in a moment). To obtain the probabilities, we estimate a set of weights (so-called logits): \\(s_1, \\dots, s_k\\). By default, we set \\(s_1 = 0\\). (We only need \\(k-1\\) numbers to define a \\(k\\)-place probability vector (given that it must sum to one).) For all \\(1 \\le i \\le k\\), we define the probability \\(p_i\\) of category \\(i\\) via the following (so-called soft-max operation):\n\\[\np_i = \\frac{\\exp s_i}{ \\sum_{j=1}^k \\exp s_j}\n\\]\nThis entails that for every \\(1 < i \\le k\\), the score \\(s_i\\) can be interpreted as the log-odds of category \\(c_i\\) over the reference category \\(c_1\\):\n\\[\ns_i = \\log \\frac{p_i}{p_1}\n\\]\nFinally, we do not just estimate any-old vector of logits, but we assume that each logit \\(s_i\\) (\\(1 < i \\le k\\)) is estimated as a linear predictor (based on the usual linear regression predictor coefficients, appropriate to the type of the \\(l\\) explanatory variables):\n\\[\ns_i = \\beta^i_0 + \\beta^i_1 x_1 + \\beta^i_2 x_2 + \\dots + \\beta^i_l x_l\n\\]\nTwo things are important for interpreting the outcome of a multinomial regression fit:\n\neach category (beyond the reference category) receives its own (independent) set of regression coefficients;\nthe linear predictor predictor \\(s_i\\) for category \\(c_i\\) can be interpreted as the log-odds of the \\(i\\)-th category over the first, reference category."
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#example-1",
    "href": "practice-sheets/03a-GLM-tutorial.html#example-1",
    "title": "Bayesian regression: theory & practice",
    "section": "Example",
    "text": "Example\nOur next research question is slightly diffuse: we want to explore whether the distribution of trajectory types is affected by whether the correct target was on the right or the left. We only consider three types of categories (curved, straight and ‘change of mind’) and prepare the data to also give us the information whether the ‘correct’ target was left or right.\n\ndolphin_prepped <-\n  dolphin |>\n  mutate(\n    prototype_label = case_when(\n     prototype_label %in% c('curved', 'straight') ~ prototype_label,\n     TRUE ~ 'CoM'\n    ),\n    prototype_label = factor(prototype_label,\n                             levels = c('straight', 'curved', 'CoM')),\n    target_position = ifelse(category_left == category_correct, \"left\", \"right\")\n    )\n\nThe relevant data now looks as follows:\n\ndolphin_prepped |> \n  select(prototype_label, target_position)\n\n# A tibble: 2,052 × 2\n   prototype_label target_position\n   <fct>           <chr>          \n 1 straight        left           \n 2 straight        right          \n 3 curved          right          \n 4 curved          left           \n 5 CoM             left           \n 6 CoM             right          \n 7 CoM             right          \n 8 straight        left           \n 9 straight        left           \n10 straight        left           \n# … with 2,042 more rows\n\n\nThe counts and proportions we care about are these:\n\nsum_stats <- dolphin_prepped |> \n  count(target_position, prototype_label) |>\n  group_by(target_position) |> \n  mutate(proportion = n / sum(n))\n\nsum_stats\n\n# A tibble: 6 × 4\n# Groups:   target_position [2]\n  target_position prototype_label     n proportion\n  <chr>           <fct>           <int>      <dbl>\n1 left            straight          751     0.734 \n2 left            curved            136     0.133 \n3 left            CoM               136     0.133 \n4 right           straight          793     0.771 \n5 right           curved             93     0.0904\n6 right           CoM               143     0.139 \n\n\nAnd here is a plot that might be useful to address your current issue:\n\nsum_stats |> \n  ggplot(aes(x = prototype_label, y = proportion, fill = prototype_label)) +\n  geom_col() +\n  facet_grid(. ~ target_position)\n\n\n\n\nIt is hard to say from visual inspection alone, whether there are any noteworthy differences. We might consider the following:\n\nConjecture: the /difference/ in probability between straight vs curved is higher when the target is on the right than when it is on the left.\n\nThis is not a real “research hypothesis” but a conjecture about the data. Let’s still run a multinomial regression model to test address this conjecture.\n\nfit_multinom <- brm(\n  formula = prototype_label ~ target_position,\n  data = dolphin_prepped,\n  family = categorical()\n)\n\nThe summary of this model fit is a bit unwieldy:\n\nsummary(fit_multinom)\n\n Family: categorical \n  Links: mucurved = logit; muCoM = logit \nFormula: prototype_label ~ target_position \n   Data: dolphin_prepped (Number of observations: 2052) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                              Estimate Est.Error l-95% CI u-95% CI Rhat\nmucurved_Intercept               -1.71      0.09    -1.89    -1.53 1.00\nmuCoM_Intercept                  -1.71      0.09    -1.89    -1.53 1.00\nmucurved_target_positionright    -0.44      0.15    -0.73    -0.16 1.00\nmuCoM_target_positionright       -0.00      0.13    -0.26     0.25 1.00\n                              Bulk_ESS Tail_ESS\nmucurved_Intercept                4527     3013\nmuCoM_Intercept                   4129     3192\nmucurved_target_positionright     4184     2752\nmuCoM_target_positionright        3861     3098\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFor better visibility here is a plot of the posteriors over relevant model parameters.\n\n# there MUST be a nicer way of doing this, but ...\nordered_names <- c(\n  \"b_mucurved_Intercept\", \n  \"b_muCoM_Intercept\",\n  \"b_mucurved_target_positionright\",\n  \"b_muCoM_target_positionright\"\n)\n\nfit_multinom |> \n  tidybayes::tidy_draws() |> \n  pivot_longer(cols = starts_with(\"b_\")) |> \n  select(name, value) |> \n  mutate(name = factor(name, levels = rev(ordered_names))) |> \n  ggplot(aes(x = value, y = name)) +\n  tidybayes::stat_halfeye() +\n  geom_vline(aes(xintercept = 0), color = project_colors[3], alpha= 1, size = 1)\n\n\n\n\n\n\n\n\n\n\nExercise 2a\nLook at the names of the coefficients in the fit summary to find out:What is the reference level for the categorical predictor variable?\n\n\n\n\n\nShow solution\n# It's the 'left' position, because there is a coefficient for the 'right' position.\n\n\n\n\n\n\n\n\nExercise 2b\nLook at the names of the coefficients in the fit summary to find out: What is the reference level of the categories to be predicted in the multinomial model?\n\n\n\n\n\nShow solution\n# The reference category is 'straight' because we have regression coeffiecient for all but the 'straight' category.\n\n\n\n\n\n\n\n\nExercise 2c\nCan you extract information about our conjecture from this plot (or the summary of the model fit)?\n\n\n\n\n\nShow solution\n# Yes! Our conjecture is about the difference in probability of the 'straight' vs he 'curved' category. This difference is directly encoded in regression coefficients. Concretely, the coefficient 'mucurved_Intercept' gives us the log odds of the 'straight' vs' the 'curved' category for the 'left'-position cases. The difference of log odds for the 'right'-position cases is simply the coefficient 'mucurved_target_positionright'. The is credibly smaller than zero (by a margin), so we may conclude that model and data provide support for our conjecture.\n\n\n\n\n\n\n\n\nExercise 2d\nUse the postrior means of the regression coefficients to compute the corresponding scores \\(s_i\\) and class probabilities \\(c_i\\). Compare these to the observed frequencies.\n\n\n\n\n\nShow solution\n# extract mean posteriors\nposterior_means <- fit_multinom |> tidybayes::summarise_draws() |> \n  select(variable, mean) |> \n  pivot_wider(names_from = variable, values_from = mean)\n\nas.numeric(posterior_means, names = colnames(posterior_means))  \n\nscores_left <- c(\n  0,\n  posterior_means[1,\"b_mucurved_Intercept\"] |> as.numeric(),\n  posterior_means[1,\"b_muCoM_Intercept\"] |> as.numeric()\n)\n\nscores_right <- c(\n  0,\n  posterior_means[1,\"b_mucurved_Intercept\"] |> as.numeric() + posterior_means[1,\"b_mucurved_target_positionright\"] |> as.numeric(),\n  posterior_means[1,\"b_muCoM_Intercept\"] |> as.numeric() + posterior_means[1,\"b_muCoM_target_positionright\"] |> as.numeric()\n)\n\nprobabilities_left <- prop.table(exp(scores_left))\nprobabilities_right <- prop.table(exp(scores_right))\n\nsum_stats |> ungroup() |> \n  mutate(prediction = c(probabilities_left, probabilities_right))"
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#explanation-2",
    "href": "practice-sheets/03a-GLM-tutorial.html#explanation-2",
    "title": "Bayesian regression: theory & practice",
    "section": "Explanation",
    "text": "Explanation\nWhen \\(k>2\\) categories have a natural ordering, the problem of predicting probabilities for each category can be simplified by taking this ordering into account. A common choice of link function for this case is the cumulative logit function which takes the linear predictor and a vector \\(\\delta\\) of \\(k-1\\) thresholds as arguments to return a probability vector, here denoted as \\(\\eta\\), whose components are defined like so:\n\\[\n\\eta_i = \\text{cumulative-logit}(\\xi; \\delta) = \\begin{cases}\n\\text{logistic}(\\delta_1 - \\xi) & \\text{if } i=1 \\\\\n\\text{logistic}(\\delta_{i} - \\xi) - \\eta_i & \\text{if } i>1 \\\\\n\\end{cases}\n\\] To see what is going on, consider the a case with three categories. Fix the two threshold \\(\\delta_1=-0.75\\) and \\(\\delta_2=1.6\\) just for illustration. Now assume that we have a case there the linear predictor value \\(\\eta\\) is zero. The cumulative logit function above then entails the category probabilities as shown in this plot, as the length of the colored bar segments:\n\n\n\n\n\nIf the linear predictor \\(\\eta\\) is estimated to be bigger than zero, this intuitively means that we shift all of the threshold to the left (by the same amount). For example, the plot below shows the case of \\(\\eta=1\\) where the probability of the first category decreases while that of the third increases.\n\n\n\n\n\nIn sum, the cumulative-logit model for ordinal regression, is defined as follows:\n\\[\n\\begin{align*}\n\\xi  &= X \\beta       && \\color{gray}{\\text{[linear predictor]}} \\\\\n\\eta &= \\text{cumulative-logit}(\\xi; \\delta) && \\color{gray}{\\text{[predictor of central tendency]}} \\\\\ny & \\sim \\text{Categorical}(\\eta) && \\color{gray}{\\text{[likelihood]}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#example-2",
    "href": "practice-sheets/03a-GLM-tutorial.html#example-2",
    "title": "Bayesian regression: theory & practice",
    "section": "Example",
    "text": "Example\nThe kind of mouse-trajectories, as categorized in variable prototype_label, are plausibly ordered by the “amount of deviation”. The following therefore tries to predict the ordered category prototype_label from the numerical measure MAD. Here is a plot of how this would look like:\n\n# prepare data by making 'prototype_label' an ordered factor\ndolphin_prepped2 <- dolphin_prepped |> \n    mutate(prototype_label = factor(prototype_label, ordered = T))\n\n# plotting the ordered categories as a function of MAD\ndolphin_prepped2 |> \n  ggplot(aes(x = MAD, y = prototype_label, \n             color = prototype_label)) +\n  geom_jitter(alpha = 0.3,height = 0.3, width = 0)\n\n\n\n\nTo run an ordinal regression model, we specify family = cumulative(). This runs the default cumulative-logit model introduced at the beginning of the session.\n\nfit_ordinal <- brm(\n  formula = prototype_label ~ MAD,\n  data = dolphin_prepped2,\n  family = cumulative()\n)\n\nThe summary output for this fitted model gives information about the slope of the predictor variable MAD as usual. But it also supplies information about two (!) intercepts: these are the cutoff points for the different categories in the cumulative normal link function.\n\nsummary(fit_ordinal)\n\n Family: cumulative \n  Links: mu = logit; disc = identity \nFormula: prototype_label ~ MAD \n   Data: dolphin_prepped2 (Number of observations: 2052) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]     4.03      0.18     3.69     4.40 1.00     2369     2533\nIntercept[2]     9.47      0.52     8.47    10.56 1.00     1807     1787\nMAD              0.02      0.00     0.02     0.03 1.00     2268     2401\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nConcretely, this means that if MAD is $x$, we compute the linear predictor \\(\\xi = \\beta_{\\text{MAD}} x\\) (without intercept!), and then “imagine around it” a standard normal distribution, i.e., consider the normal distribution $\\mathcal{\\xi, 1}$. The probabilities of the three categories are given by the areas under the curve of this normal distribution, namely:\n\nprobability category 1: area from \\(-\\infty\\) to first threshold,\nprobability category 2: area between first and second threshold,\nprobability category 3: area between second threshold and \\(\\infty\\).\n\nWe can operator with the linear regression coefficients as usual, e.g., asking whether there is any reason to believe, given model and data, that the higher MAD, the higher the probability of seeing a more ‘uncertain’ trajectory type.\n\nfit_ordinal |> \n  tidybayes::gather_draws(b_MAD) |> \n  ggplot(aes(x = .value, y = .variable)) +\n  tidybayes::stat_halfeye() +\n  ylab(\"\") + xlab(\"\") + ggplot2::xlim(0,0.03)\n\n\n\n\n\nbrms::hypothesis(fit_ordinal, \"MAD > 0\")\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1  (MAD) > 0     0.02         0     0.02     0.03        Inf         1    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities."
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#explanation-3",
    "href": "practice-sheets/03a-GLM-tutorial.html#explanation-3",
    "title": "Bayesian regression: theory & practice",
    "section": "Explanation",
    "text": "Explanation\nThe Poisson distribution is the common choice for count data. It is defined as:\n\\[\n\\text{Poisson}(k ; \\lambda) = \\frac{\\lambda^k \\ \\exp( -\\lambda)} {k!}\n\\]\nThe link function is the exponential function (so the inverse link function is the logarithmic function). The Poisson regression model is defined as:\n\\[\n\\begin{align*}\n\\xi  &= X \\beta       && \\color{gray}{\\text{[linear predictor]}} \\\\\n\\eta_i &= \\exp(\\xi_i) && \\color{gray}{\\text{[predictor of central tendency]}} \\\\\ny_i & \\sim \\text{Poisson}(\\eta_i) && \\color{gray}{\\text{[likelihood]}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "practice-sheets/03a-GLM-tutorial.html#example-3",
    "href": "practice-sheets/03a-GLM-tutorial.html#example-3",
    "title": "Bayesian regression: theory & practice",
    "section": "Example",
    "text": "Example\nThere are examples in the next exercise sheet. For a tutorial on Poisson regression specifically geared towards linguists see here."
  },
  {
    "objectID": "practice-sheets/02a-predictives.html",
    "href": "practice-sheets/02a-predictives.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models)."
  },
  {
    "objectID": "practice-sheets/02a-predictives.html#new-stuff",
    "href": "practice-sheets/02a-predictives.html#new-stuff",
    "title": "Bayesian regression: theory & practice",
    "section": "New Stuff",
    "text": "New Stuff\nHere is the mouse-tracking data set we used previously for simple linear regression.\n\ndolphin <- aida::data_MT\n# aggregate\ndolphin_agg <- dolphin |> \n  filter(correct == 1) |> \n  group_by(subject_id) |> \n  dplyr::summarize(\n            AUC = median(AUC, na.rm = TRUE),\n            MAD = median(MAD, na.rm = TRUE)) \n\nHere is a plot to remind ourselves.\n\n# plot temperature data\n\ndolphin_agg |> \n  ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(color = project_colors[2])\n\n\n\n\n\n\n\n\n\n\nExercise 3a\nObtain a model fit for AUC ~ MAD with a prior for the slope coefficient as a Student-t distribution with 1 degree of freedom, mean 0 and standard deviation 500.\n\n\n\n\n\nShow solution\nfit_dolphin_agg <- brm(\n  AUC ~ MAD, \n  data = dolphin_agg,\n  prior = prior(student_t(1,0,500), class = \"b\")\n  )\n\n\nHere is how we can extract and plot three samples from the posterior predictive distribution. So, these are three “fake” data sets of the same size and for the same MAD values as in the original data.\n\n# extract & plot posterior predictives\npost_pred <- tidybayes::predicted_draws(\n  object = fit_dolphin_agg,\n  newdata = dolphin_agg |> select(MAD),\n  value = \"AUC\",\n  ndraws = 3\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_pred |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_point() + \n  facet_grid(. ~ run)\n\n\n\n\n\n\n\n\n\n\nExercise 3b\nChange the input to the parameter newdata so that we get three samples for MAD values 400, 500 and 600.\n\n\n\n\n\nShow solution\n# extract & plot posterior predictives\npost_pred2 <- tidybayes::predicted_draws(\n  object = fit_dolphin_agg,\n  newdata = tibble(MAD = c(400, 500, 600)),\n  value = \"AUC\",\n  ndraws = 3\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_pred2 |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_point() + \n  facet_grid(. ~ run)\n\n\nWe can also extract predictions for the linear predictor values like so:\n\n# extract & plot posterior linear predictors\n\npost_lin_pred <- tidybayes::linpred_draws(\n  object = fit_dolphin_agg,\n  newdata = dolphin_agg |> select(MAD),\n  value = \"AUC\",\n  ndraws = 3\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_lin_pred |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_line() + \n  facet_grid(. ~ run)\n\n\n\n\n\n\n\n\n\n\nExercise 3c\nExtract 30 samples of linear predictor lines and plot them all in one plot. Make the line plots gray and use a low alpha value (slight transparency).\n\n\n\n\n\nShow solution\npost_lin_pred2 <- tidybayes::linpred_draws(\n  object = fit_dolphin_agg,\n  newdata = dolphin_agg |> select(MAD),\n  value = \"AUC\",\n  ndraws = 30\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_lin_pred2 |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_line(aes(group = run), color = \"gray\", alpha = 0.2)\n\n\nFinally, let’s look at a posterior predictive check, based on the distribution of actual / predicted AUC values:\n\npp_check(fit_dolphin_agg, ndraws = 20)\n\n\n\n\n\n\n\n\n\n\nExercise 3d\nRepeat all the steps from the prior predictive point of view for model fit_dolphin_agg,\n\n\n\n\n\nShow solution\nfit_dolphin_agg_prior <- stats::update(fit_dolphin_agg, sample_prior = 'only')\n\npost_pred <- tidybayes::predicted_draws(\n  object = fit_dolphin_agg_prior,\n  newdata = dolphin_agg |> select(MAD),\n  value = \"AUC\",\n  ndraws = 3\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_pred |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_point(alpha = 0.5) + \n  facet_grid(. ~ run)\n\n# extract & plot posterior predictives\npost_pred2 <- tidybayes::predicted_draws(\n  object = fit_dolphin_agg_prior,\n  newdata = tibble(MAD = c(400, 500, 600)),\n  value = \"AUC\",\n  ndraws = 3\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_pred2 |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_point(alpha = 0.8) + \n  facet_grid(. ~ run)\n\n# extract & plot posterior linear predictors\n\npost_lin_pred <- tidybayes::linpred_draws(\n  object = fit_dolphin_agg_prior,\n  newdata = dolphin_agg |> select(MAD),\n  value = \"AUC\",\n  ndraws = 3\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_lin_pred |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_line() + \n  facet_grid(. ~ run)\n\npost_lin_pred2 <- tidybayes::linpred_draws(\n  object = fit_dolphin_agg_prior,\n  newdata = dolphin_agg |> select(MAD),\n  value = \"AUC\",\n  ndraws = 30\n) |> \n  ungroup() |> \n  mutate(run = str_c(\"sample \", factor(.draw))) |> \n  select(run, MAD, AUC) \n\npost_lin_pred2 |> ggplot(aes(x = MAD, y = AUC)) +\n  geom_point(data = dolphin_agg, color = project_colors[2], alpha = 0.3) +\n  geom_line(aes(group = run), color = \"gray\", alpha = 0.2)"
  },
  {
    "objectID": "practice-sheets/02c-catPreds-execises.html",
    "href": "practice-sheets/02c-catPreds-execises.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models)."
  },
  {
    "objectID": "practice-sheets/02c-catPreds-execises.html#exercise-1",
    "href": "practice-sheets/02c-catPreds-execises.html#exercise-1",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nExercise 1a\nUse brm() to run a linear regression model for the data set dolphin_prepped and the formula:\nlog RT ~ group * condition * prototype_label\nSet the prior for all population-level slope coefficients to a reasonable, weakly-informative but unbiased prior.\n\n\n\n\n\nShow solution\nfit <- brm(\n  formula = log(RT) ~ group * condition * prototype_label,\n  prior   = prior(student_t(1, 0, 3), class = \"b\"),\n  data    = dolphin_prepped\n  )\n\n\n\n\n\n\n\n\nExercise 1b\nPlot the posteriors for population-level slope coefficients using the tidybayes package in order to:\n\ndetermine which combination of factor levels is the default cell\ncheck which coefficients have 95% CIs that do not include zero\ntry to use this latter information to address any of our research hypotheses (stated above)\n\n\n\n\n\n\nShow solution\ntidybayes::summarise_draws(fit)\ntidybayes::gather_draws(fit, `b_.*`, regex = TRUE) |>\n  filter(.variable != \"b_Intercept\") |>\n  ggplot(aes(y = .variable, x = .value)) +\n  tidybayes::stat_halfeye() +\n  labs(x = \"\", y = \"\") +\n  geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n               lty = \"dashed\")\n\n# the default cell is for click-atypical-straight\n\n# coeffiencents with 95% CIs that do not include zero are:\n#   grouptouch, conditionTypical, prototype_labelCoM\n\n# none of these give us direct information about our research hypotheses\n\n\n\n\n\n\n\n\nExercise 1c\nUse the faintr package to get information relevant for the current research hypotheses. Interpret each result with respect to what we may conclude from it.\n\n\n\n\n\nShow solution\n# 1. Typical trials are faster than atypical ones.\n# -> There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = condition == 'Typical',\n  higher = condition == 'Atypical'\n)\n\n# 2. CoM trials are slower than the other kinds of trials\n#    (straight and curved) together, and respectively.\n# -> There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label != 'CoM',\n  higher = prototype_label == 'CoM'\n)\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label == 'straight',\n  higher = prototype_label == 'CoM'\n)\n\n# 3. 'straight' trials are faster than 'curved' trials.\n# -> There is no evidence for this hypothesis\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label == 'straight',\n  higher = prototype_label == 'curved'\n)\n\n# 4. Click trials are slower than touch trials.\n# -> There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = group == 'touch',\n  higher = group == 'click'\n)"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html",
    "href": "practice-sheets/01a-wrangling-plotting.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models)."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#loading-and-inspecting-the-data",
    "href": "practice-sheets/01a-wrangling-plotting.html#loading-and-inspecting-the-data",
    "title": "Bayesian regression: theory & practice",
    "section": "Loading and inspecting the data",
    "text": "Loading and inspecting the data\nThe data is part of the aida package, but we can give it a fancy new name:\n\ndolphin <- aida::data_MT\n\nTo get some information about the data set, we can use the help function:\n\nhelp(\"data_MT\")\n\nHere is some more information we can get about the data:\n\n# number of rows in the data set\nnrow(dolphin)\n\n[1] 2052\n\n# number of columns in the data set\nncol(dolphin)\n\n[1] 16\n\n# names of the columns\nnames(dolphin)\n\n [1] \"X1\"               \"trial_id\"         \"MAD\"              \"AUC\"             \n [5] \"xpos_flips\"       \"RT\"               \"prototype_label\"  \"subject_id\"      \n [9] \"group\"            \"condition\"        \"exemplar\"         \"category_left\"   \n[13] \"category_right\"   \"category_correct\" \"response\"         \"correct\"         \n\n# number of unique `subject_id`s\ndolphin$subject_id |> unique() |> length()\n\n[1] 108\n\n# number of types each subject saw different `conditions`\ndolphin |> with(table(subject_id, condition)) |> head()\n\n          condition\nsubject_id Atypical Typical\n      1001        6      13\n      1002        6      13\n      1003        6      13\n      1004        6      13\n      1005        6      13\n      1006        6      13"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#a-closer-look-at-the-columns",
    "href": "practice-sheets/01a-wrangling-plotting.html#a-closer-look-at-the-columns",
    "title": "Bayesian regression: theory & practice",
    "section": "A closer look at the columns",
    "text": "A closer look at the columns\nLet’s take a closer look at the columns and the information inside them.\nWe can get a glimpse of all columns like so:\n\nglimpse(dolphin)\n\nRows: 2,052\nColumns: 16\n$ X1               <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ trial_id         <chr> \"id0001\", \"id0002\", \"id0003\", \"id0004\", \"id0005\", \"id…\n$ MAD              <dbl> 82.53319, 44.73484, 283.48207, 138.94863, 401.93988, …\n$ AUC              <dbl> 40169.5, 13947.0, 84491.5, 74084.0, 223083.0, 308376.…\n$ xpos_flips       <dbl> 3, 1, 2, 0, 2, 2, 1, 0, 2, 0, 2, 2, 0, 0, 3, 1, 0, 1,…\n$ RT               <dbl> 950, 1251, 930, 690, 951, 1079, 1050, 830, 700, 810, …\n$ prototype_label  <chr> \"straight\", \"straight\", \"curved\", \"curved\", \"cCoM\", \"…\n$ subject_id       <dbl> 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001,…\n$ group            <chr> \"touch\", \"touch\", \"touch\", \"touch\", \"touch\", \"touch\",…\n$ condition        <chr> \"Atypical\", \"Typical\", \"Atypical\", \"Atypical\", \"Typic…\n$ exemplar         <chr> \"eel\", \"rattlesnake\", \"bat\", \"butterfly\", \"hawk\", \"pe…\n$ category_left    <chr> \"fish\", \"amphibian\", \"bird\", \"Insekt\", \"bird\", \"fish\"…\n$ category_right   <chr> \"reptile\", \"reptile\", \"mammal\", \"bird\", \"reptile\", \"b…\n$ category_correct <chr> \"fish\", \"reptile\", \"mammal\", \"Insekt\", \"bird\", \"bird\"…\n$ response         <chr> \"fish\", \"reptile\", \"bird\", \"Insekt\", \"bird\", \"bird\", …\n$ correct          <dbl> 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,…\n\n\nHere is a quick explanation of all the different columns:\n\ntrial_id = unique id for individual trials\nMAD = maximal deviation into competitor space\nAUC = area under the curve\nxpos_flips = the amount of horizontal direction changes\nRT = reaction time in ms\nprototype_label = different categories of prototypical movement strategies\nsubject_id = unique id for individual participants\ngroup = groups differ in the response design (click vs. touch)\ncondition = category membership (Typical vs. Atypical)\nexemplar = the concrete animal\ncategory_left = the category displayed on the left\ncategory_right = the category displayed on the right\ncategory_correct= the category that is correct\nresponse = the selected category\ncorrect = whether or not the response matches category_correct"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#selecting-columns",
    "href": "practice-sheets/01a-wrangling-plotting.html#selecting-columns",
    "title": "Bayesian regression: theory & practice",
    "section": "Selecting columns",
    "text": "Selecting columns\nFor now, we are only interested in columns RT, group, condition, category_correct, and correct. We can use the select() function of dplyr to get rid of columns we don’t need.\n\n# selecting specific columns\ndolphin_selected <-\n  dolphin |>\n  dplyr::select(RT, group, condition, category_correct, correct)\n \n# let's have a look\ndolphin_selected\n\n# A tibble: 2,052 × 5\n      RT group condition category_correct correct\n   <dbl> <chr> <chr>     <chr>              <dbl>\n 1   950 touch Atypical  fish                   1\n 2  1251 touch Typical   reptile                1\n 3   930 touch Atypical  mammal                 0\n 4   690 touch Atypical  Insekt                 1\n 5   951 touch Typical   bird                   1\n 6  1079 touch Atypical  bird                   1\n 7  1050 touch Typical   fish                   1\n 8   830 touch Typical   fish                   1\n 9   700 touch Typical   mammal                 1\n10   810 touch Typical   fish                   1\n# … with 2,042 more rows"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#filtering-rows",
    "href": "practice-sheets/01a-wrangling-plotting.html#filtering-rows",
    "title": "Bayesian regression: theory & practice",
    "section": "Filtering rows",
    "text": "Filtering rows\nIf we care only about a subset of rows, we can use the filter() function. For example, let’s filter all trials in which the correct category was either a fish or a mammal\n\ndolphin_filter1 <-\n  dolphin_selected |> \n  filter(category_correct == \"fish\" | category_correct == \"mammal\")\n  # the | is a logical operator that indicates that either the first expression OR \n  # the second one has to be true\n\ndolphin_filter1\n\n# A tibble: 1,296 × 5\n      RT group condition category_correct correct\n   <dbl> <chr> <chr>     <chr>              <dbl>\n 1   950 touch Atypical  fish                   1\n 2   930 touch Atypical  mammal                 0\n 3  1050 touch Typical   fish                   1\n 4   830 touch Typical   fish                   1\n 5   700 touch Typical   mammal                 1\n 6   810 touch Typical   fish                   1\n 7  1264 touch Typical   mammal                 1\n 8   890 touch Atypical  mammal                 0\n 9  1040 touch Typical   mammal                 1\n10   730 touch Typical   mammal                 1\n# … with 1,286 more rows\n\n\nYou can also filter() against particular conditions. For example, let’s filter all rows that do not have bird as their correct category:\n\ndolphin_filter2 <-\n  dolphin_selected |> \n  filter(category_correct != \"bird\")\n\ndolphin_filter2\n\n# A tibble: 1,728 × 5\n      RT group condition category_correct correct\n   <dbl> <chr> <chr>     <chr>              <dbl>\n 1   950 touch Atypical  fish                   1\n 2  1251 touch Typical   reptile                1\n 3   930 touch Atypical  mammal                 0\n 4   690 touch Atypical  Insekt                 1\n 5  1050 touch Typical   fish                   1\n 6   830 touch Typical   fish                   1\n 7   700 touch Typical   mammal                 1\n 8   810 touch Typical   fish                   1\n 9  1264 touch Typical   mammal                 1\n10   890 touch Atypical  mammal                 0\n# … with 1,718 more rows\n\n\nWe can also filter according to multiple conditions at once, including numeric conditions. Here, we also filter for trials that have correct responses.\n\ndolphin_filter3 <-\n  dolphin_selected |> \n  filter(category_correct != \"bird\",\n         correct == 1)\n\ndolphin_filter3\n\n# A tibble: 1,602 × 5\n      RT group condition category_correct correct\n   <dbl> <chr> <chr>     <chr>              <dbl>\n 1   950 touch Atypical  fish                   1\n 2  1251 touch Typical   reptile                1\n 3   690 touch Atypical  Insekt                 1\n 4  1050 touch Typical   fish                   1\n 5   830 touch Typical   fish                   1\n 6   700 touch Typical   mammal                 1\n 7   810 touch Typical   fish                   1\n 8  1264 touch Typical   mammal                 1\n 9  1040 touch Typical   mammal                 1\n10   730 touch Typical   mammal                 1\n# … with 1,592 more rows"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#grouping-and-summarizing",
    "href": "practice-sheets/01a-wrangling-plotting.html#grouping-and-summarizing",
    "title": "Bayesian regression: theory & practice",
    "section": "Grouping and summarizing",
    "text": "Grouping and summarizing\nWe can also generate summary statistics of certain variables with a combination of group_by() & summarise(). Let’s get the means and standard deviations of the reactions times for each level in the variable condition. We also include the minimum and maximum values for each condition.\n\ndolphin_aggregate <-\n  dolphin_filter3 |>\n  group_by(condition) |>\n  summarise(\n    min_RT  = min(RT),\n    mean_RT = mean(RT, na.rm = T),\n    sd_RT   = sd(RT, na.rm = T),\n    max_RT  = max(RT)\n    )\n  # the na.rm = T is an argument that is used to tell R that NAs should be ignored \n  # when calculating the summary statistics\n\n# show the aggregated df\ndolphin_aggregate\n\n# A tibble: 2 × 5\n  condition min_RT mean_RT sd_RT max_RT\n  <chr>      <dbl>   <dbl> <dbl>  <dbl>\n1 Atypical     630   2149. 1840.  19903\n2 Typical      510   1665. 1283.  20685\n\n\nSo we find that atypical categories are responded to slower than typical categories. Makes sense. Identifying a dolphin as a mammal might be difficult because it shares a lot of features with fish.\nWe can group according to many different factors simultaneously, and we can create multiple summary statistics at the same time. Here, we get summary statistics for each combination of all levels in variables condition and group. We use the tidyboot package to get bootstrapped 95% confidence intervalls. (Notice that these are more informative than standard deviations in the sense that they give an upper and lower deviation, not just one number for both directions, which can be misleading when the data is skewed (like reaction times typically are)):\n\ndolphin_aggregate2 <-\n  dolphin_filter3 |>\n  group_by(group, condition) |>\n  summarize(\n    lower_CI = tidyboot::ci_lower(RT),\n    mean_RT  = mean(RT, na.rm = T),\n    upper_CI = tidyboot::ci_upper(RT)\n    )\n\n# show the aggregated df\ndolphin_aggregate2\n\n# A tibble: 4 × 5\n# Groups:   group [2]\n  group condition lower_CI mean_RT upper_CI\n  <chr> <chr>        <dbl>   <dbl>    <dbl>\n1 click Atypical     1030.   2417.    7311.\n2 click Typical       950    1847.    4698.\n3 touch Atypical      750    1900.    5368.\n4 touch Typical       686.   1486.    3955.\n\n\nWe can see here that the group that needed to click on a response are overall slower than touch responses, but also much more variable in their behavior."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#changing-and-adding-columns",
    "href": "practice-sheets/01a-wrangling-plotting.html#changing-and-adding-columns",
    "title": "Bayesian regression: theory & practice",
    "section": "Changing and adding columns",
    "text": "Changing and adding columns\nOften, we are interested in standardized measures because we do not know what a value of 1 means on any given scale. Is 1 a large difference or a small difference? For example when we want to explore the impact of several predictors on the same measurement, we want to know the relative size of a number. To achieve this, we standardize measures by dividing their mean by their respective standard deviations. We will use the scale() function for this and create a new variable in our data frame via mutate().\n(Note that the scale() function creates an object that is of the matrix class. That is fine for the most part but might create issues later on. To avoid any issues, we wrap the scale() function in as.numeric() to store the results as a numeric vector.)\n\ndolphin_standardize <-\n  dolphin_selected |>\n  mutate(RT_scale = as.numeric(scale(RT)))\n  \nhead(dolphin_standardize)\n\n# A tibble: 6 × 6\n     RT group condition category_correct correct RT_scale\n  <dbl> <chr> <chr>     <chr>              <dbl>    <dbl>\n1   950 touch Atypical  fish                   1   -0.552\n2  1251 touch Typical   reptile                1   -0.373\n3   930 touch Atypical  mammal                 0   -0.564\n4   690 touch Atypical  Insekt                 1   -0.706\n5   951 touch Typical   bird                   1   -0.551\n6  1079 touch Atypical  bird                   1   -0.475\n\n\nIf we now compare, say atypical and typical categories according to reaction times, we can use the standardized RT ratings. Let’s do all of this in one “pipeline”.\n\ndolphin_agg_standardize <- dolphin_selected |>\n  mutate(RT_scale = scale(RT)) |> \n  group_by(condition) |>\n  summarise(mean_RT_scale = mean(RT_scale, na.rm = T))\n  \nhead(dolphin_agg_standardize)\n\n# A tibble: 2 × 2\n  condition mean_RT_scale\n  <chr>             <dbl>\n1 Atypical          0.219\n2 Typical          -0.101\n\n\nNow we can see that atypical categories exhibit relatively higher RTs, i.e., more than 0.3 standard deviations higher than for typical categories."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#exercises-for-data-wrangling",
    "href": "practice-sheets/01a-wrangling-plotting.html#exercises-for-data-wrangling",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercises for data wrangling",
    "text": "Exercises for data wrangling\n\nExercise 1\n\n\n\n\n\n\nExercise 1a\nTake the dolphin data set and store a reduced variant of it as dolphin_reduced. The new data frame should contain only the following columns: RT, AUC, group, and exemplar.\n\n\n\n\n\nShow solution\ndolphin_reduced <- dolphin |>\n  select(RT, AUC, group, exemplar)\n\nhead(dolphin_reduced)\n\n\n\n\n\n\n\n\nExercise 1b\nWe are for now only interested in those data that have whales as the exemplar. filter() only those rows and store them in a new dataframe called whales_only.\n\n\n\n\n\nShow solution\nwhales_only <- dolphin_reduced |> \n  filter(exemplar == \"whale\")\n\nhead(whales_only)\n\n\n\n\n\n\n\n\nExercise 1c\nNow filter for only those data that have RTs below 1500ms.\n\n\n\n\n\nShow solution\nwhales_only2 <- whales_only |> \n  filter(RT < 1500)\n\nhead(whales_only2)\n\n\n\n\n\n\n\n\nExercise 1d\nWe don’t like that AUC is unstandardized. Use mutate() to create a new vector that represents scaled AUC values (scaling is achieved by the function scale()).\n\n\n\n\n\nShow solution\nwhales_only_scaled <- whales_only2 |> \n  mutate(AUC_scaled = scale(AUC))\n\nhead(whales_only_scaled)\n\n\n\n\n\n\n\n\nExercise 1e\nCalculate the mean scaled AUC ratings for both both groups.\n\n\n\n\n\nShow solution\nwhales_aggregate <- whales_only_scaled |> \n  group_by(group) |> \n  summarise(mean_AUC_scaled = mean(AUC_scaled, na.rm =TRUE))\n\nhead(whales_aggregate)\n\n\n\n\n\n\n\n\nExercise 1f\nDo all of the above (a-e) in one pipeline.\n\n\n\n\n\nShow solution\nwhales_aggregate <- dolphin |>\n  select(RT, AUC, group, exemplar) |> \n  filter(exemplar == \"whale\",\n         RT < 1500) |> \n  mutate(AUC_scaled = scale(AUC)) |> \n  group_by(group) |> \n  summarise(mean_AUC_scaled = mean(AUC_scaled, na.rm =TRUE))\n  \nhead(whales_aggregate)\n\n\n\n\nExercise 2\n\n\n\n\n\n\nExercise 1a\nTake the dolphin data set and store a reduced variant of it. The new data frame should contain only the columns condition, group, and xpos_flips, correct. And within the correct vector, we are only interested in the correct trials (= 1). Filter accordingly.\n\n\n\n\n\nShow solution\ndolphin_sub <- dolphin |> \n  select(condition, group, xpos_flips, correct) |> \n  filter(correct == 1)\n\nhead(dolphin_sub)\n\n\n\n\n\n\n\n\nExercise 2b\nCreate an aggregated data frame that contains the mean xpos_flips value and the standard deviation for group and condition.\n\n\n\n\n\nShow solution\ndolphin_agg <- dolphin_sub |>\n  group_by(group, condition) |> \n  summarise(mean_xpos_flips = mean(xpos_flips, na.rm = TRUE),\n            sd_xpos_flips = sd(xpos_flips, na.rm = TRUE))\n\nhead(dolphin_agg)\n\n\n\n\n\n\n\n\nExercise 2c\nUse the rename() function to rename the new vectors for the mean xflips and their standard deviation to xflips_mean and xflips_sd.\n\n\n\n\n\nShow solution\ndolphin_agg2 <- dolphin_agg |>\n  rename(xflips_mean = mean_xpos_flips,\n         xflips_sd = sd_xpos_flips)\n\n\n\n\n\n\n\n\nExercise 2d\nDo all of the above (a-c) in one pipeline.\n\n\n\n\n\nShow solution\ndolphin |> \n  select(condition, group, xpos_flips, correct) |> \n  filter(correct == 1) |> \n  group_by(group, condition) |> \n  summarise(mean_xpos_flips = mean(xpos_flips, na.rm = TRUE),\n            sd_xpos_flips = sd(xpos_flips, na.rm = TRUE)) |> \n  rename(xflips_mean = mean_xpos_flips,\n         xflips_sd = sd_xpos_flips)"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#basic-plots",
    "href": "practice-sheets/01a-wrangling-plotting.html#basic-plots",
    "title": "Bayesian regression: theory & practice",
    "section": "Basic plots",
    "text": "Basic plots\nNow that we have pre-processed our data set, we are ready to visually explore it. Let’s start very simple. Let’s plot a bar plot. Let’s also add a title to our plot.\n\nggplot(dolphin_agg, aes(x = condition, y = mean_RT)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"a bare bar plot\")\n\n\n\n\n\n\n\n  # stat = \"identity\" takes the number in the dataset as the bar height (as opposed to a 'count')\n\nUgh! What an ugly plot, right? But it’s already telling a story: Atypical categories are responded to slower than typical categories. Let’s add a measure of uncertainty, in our case the bootstrapped 95% confidence intervals, as error bars to the plot:\n\nggplot(dolphin_agg, aes(x = condition, y = mean_RT)) +\n  geom_bar(stat = \"identity\") + \n  \n  # this is the added layer\n  geom_errorbar(aes(ymin = lower_CI, \n                    ymax = upper_CI), \n                colour = \"black\",\n                linewidth = 0.5) +\n  \n  ggtitle(\"a bare bar plot with error bars\")\n\n\n\n\nWe can observe a couple of things here. First, ggplot automatically adjust the axes based on the elements to be plotted unless we tell it not to. Second, the error bars are plotted in front of the bars, i.e. closer to the viewer. This visual ordering reflects the order of layers. We first plotted the bars and THEN the error bars.\nBeyond bar plots, we can create other useful plots types. For example a point plot. Instead of a bar, we plot the mean RT as points.\n\nggplot(dolphin_agg, aes(x = condition, y = mean_RT)) +\n  geom_errorbar(aes(ymin = lower_CI, \n                    ymax = upper_CI), \n                colour = \"black\") +  \n  # this is the new geom \n  geom_point() +\n  ggtitle(\"a point plot\")\n\n\n\n\n\n\n\n\nOr a line plot that connects the means with a line. For the line plot to work, we need to indicate a group aesthetic, i.e. the group that we want to connect with a line. If you have for example several interacting categories, you need to indicate which groups are supposed to be connected with lines (see below). Because we have only one group here, condition, we set group to 1.\n\nggplot(dolphin_agg, aes(x = condition, y = mean_RT, group = 1)) +\n  geom_line() +\n  ggtitle(\"a line plot\")\n\n\n\n\n\n\n\n\nYay, we are on a roll. Let’s plot a box plot. Remember the box shows the median (middle vertical line) and the interquartile range (the middle 50% of the data within the box). Note that for the box plot, we do not plot aggregated values, so we need to refer to the entire data set. We also add the aesthetic fill here and set it to the variable condition to color code our boxes.\n\n# we changed the dataset referred to\nggplot(dolphin, aes(x = condition, y = RT, fill = condition)) +\n  # this is the new geom \n  geom_boxplot() +\n  ggtitle(\"a box plot\")\n\n\n\n\n\n\n\n\nWhile the above plots illustrate one continuous variable (RT) plotted against a categorical variable (condition), we can also plot two continuous variables against each other. For example, we could plot RT against AUC in a scatter plot.\n\n# we changed the y aesthetic to `Hardness`\nggplot(dolphin_subset, aes(x = RT, y = AUC)) +\n  geom_point() +\n  ggtitle(\"a scatter plot\")\n\n\n\n\n\n\n\n\nFinally, one central plot type for our class. The density plot. It plots on the x-axis a continous value and on the y-axis the “density”” of these values. So high values on the y-axis means a lot of data at the corresponding x-values. The density curve can be outlined with color and filled with fill. To keep the two categories visually distinct, we add an argument to the geom_density() function: alpha. Alpha controls the transparency of the color! We will see a lot of these density plots in our class.\n\nggplot(dolphin, aes(x = RT, color = condition, fill = condition)) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"a density plot\")"
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#adjusting-plot-elements",
    "href": "practice-sheets/01a-wrangling-plotting.html#adjusting-plot-elements",
    "title": "Bayesian regression: theory & practice",
    "section": "Adjusting plot elements",
    "text": "Adjusting plot elements\nOkay, so we are now already capable of exploring our data visually with a bunch of plots. These plots are exceptionally ugly and of limited communicative value so far. Note that this is perfectly fine during an exploratory phase of data analysis. If we just eye-ball data and we have no trouble interpreting the plots, thats just fine. However, as soon as we want to communicate patterns to others with these graphs, we need to take a little bit more care of its communicative value. Let’s look at ways we can tailor our plots to the needs of an audience.\nLet’s go back to our bar plot and explore whether condition and group has an impact on RT?\n\n# First we aggregate RT for group and condition\ndolphin_agg2 <- dolphin_subset |>\n  group_by(group, condition) |>\n  summarise(mean_RT = mean(RT),\n            sd_RT = sd(RT))\n\n# then we plot and color code for condition (note that, for bar plots, the aesthetic of `color` refers to the border of the bar, and `fill` refers to the actual colour of the bar)\n\nggplot(dolphin_agg2, aes(x = condition, y = mean_RT, fill = group)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nHm… that doesn’t work out, the bars are on top of each other, we need to tell ggplot to position the bars next to each other instead. We do that with position_dogde(). Note that ggplot assigns a default color coding scheme to your plots if you don’t specify it by hand.\n\nggplot(dolphin_agg2, aes(x = condition, y = mean_RT, fill = group)) +\n  geom_bar(stat = \"identity\", position = position_dodge())\n\n\n\n\n\n\n\n\nAwww much better! Alternatively, we can plot the two categories into separate panels. We achieve this by facetting using the facet_grid() function.\n\nggplot(dolphin_agg2, aes(x = group, y = mean_RT, fill = condition)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  # this is the facetting function\n  facet_grid(~ condition)\n\n\n\n\n\n\n\n\nOkay. We are getting somewhere. We already learned something again. Apparently, the effect of typiciality on RT is pretty similar across tasks. It does not look like we have an interaction here (more on interactions later).\nNow let’s make these plots ready to communicate information. We add appropriate axes titles. Note that we are using a little hack here: The “” inserts an empty line, creating visual distance from axis title to axis, thus making it easier to read it. Our audience will thank us.\n\nggplot(dolphin_agg2, aes(x = group, y = mean_RT, fill = condition)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  facet_grid(~ condition) +\n  # add axis titles\n  xlab(\"\\n task\") +\n    ylab(\"mean response latency in ms\\n\") \n\n\n\n\n\n\n\n\nThe same graph as a point / line plot indicated to the audience whether there is an interaction pattern or not. Note that here we do not facet because we actually want the points to be plotted within the same horizontal space. We also have to specify the group aesthetic to tell ggplot which points to connect with lines.\n\nggplot(dolphin_agg2, aes(x = group, y = mean_RT, color = condition, group = condition)) +\n  # instead of geom_bar we use geom_point and geom_line\n  geom_point(size = 12) +\n  geom_line(size = 2) +\n  xlab(\"\\n task\") +\n  ylab(\"mean response latency in ms\\n\") \n\n\n\n\n\n\n\n  # # need to change to the color aesthetic instead of fill\n  # scale_y_continuous(expand = c(0, 0), breaks = (c(0, 500, 1000, 1500, 2000, 2500, 3000)), limits = c(0,3000))\n\nThese lines look pretty parallel and don’t indicate a strong interaction pattern. But how do different exemplars differ? Let’s aggregate for individual exemplars first and then create the same plot for the means of all exemplars.\n\ndolphin_agg3 <- dolphin_subset |> \n  group_by(exemplar, group, condition) |> \n  summarise(mean_RT = mean(RT, na.rm = TRUE))\n\nggplot(dolphin_agg3, aes(x = group, y = mean_RT, color = condition, group = exemplar)) +\n  # instead of geom_bar we use geom_point and geom_line\n  geom_point(size = 6, alpha = 0.3) +\n  geom_line() +\n  geom_label(aes(label = exemplar)) +\n  xlab(\"\\n task\") +\n  ylab(\"mean response latency in ms\\n\")\n\n\n\n\n\n\n\n\nIt looks like “shark” and “rattlesnake” behave very different from their buddies in the typical condition. Interesting! We wouldn’t have noticed if we had only looked at the overall means."
  },
  {
    "objectID": "practice-sheets/01a-wrangling-plotting.html#exercises-for-plotting",
    "href": "practice-sheets/01a-wrangling-plotting.html#exercises-for-plotting",
    "title": "Bayesian regression: theory & practice",
    "section": "Exercises for plotting",
    "text": "Exercises for plotting\nTake the scatter plot below as a departure point. It plots AUC (area-under-the-curve) against MAD (maximal absolute deviation).\n\nggplot(dolphin, aes(x = MAD, y = AUC)) +\n  geom_point() +\n  ggtitle(\"a scatter plot\")\n\n\n\n\n\n\n\n\n\n\nExercise 3a\n\nChange both the x-axis and the y-axis title to sensible and informative titles.\nChange the plot title to something informative.\nChange the scaling of the x-axis to display only MAD values between -500 and 500\n\n\n\n\n\n\nShow solution\nggplot(dolphin, aes(x = MAD, y = AUC, \n                           \n                           # (d) add color aesthetic\n                           color = group)) +\n  geom_point() +\n  \n  # (1) axes titles\n  xlab(\"\\n maximal absolute deviation\") +\n  ylab(\"area-under-the-curve \\n\") +\n  \n  # (2) change title\n  ggtitle(\"MAD is correlated with AUC\") +\n\n  # (3) change x-axis (note that certain values are not displayed then. R will spit out a warning)\n  scale_x_continuous(limits = c(-500,500))\n\n\n\n\n\n\n\n\nExercise 4\n\nPlot AUC values as a function of group in a density plot (geom_density).\n\n(2)) Make the density curves semi-transparent with the alpha argument\n(3)) Add the aida_theme to the plot.\n\nAdd the mean values for both groups into the density plot as a line.\n\n\n\n\n\n\nShow solutions for a-c\n# (1 - 3)\nggplot(dolphin, aes(x = AUC, color = group, fill = group)) +\n  geom_density(alpha = 0.3) +\n  xlab(\"\\n AUC\")\n\n\n\n\nShow solution for d\n# (4) aggregate means and add to plot\n\ndolphin_agg <- dolphin |>\n  group_by(group) |>\n  summarise(mean_AUC = mean(AUC, na.rm = TRUE),\n            sd_AUC = sd(AUC, na.rm = TRUE))\n\n# add them to the plot as vertical lines\nggplot(dolphin, aes(x = AUC, color = group, fill = group)) +\n  geom_density(alpha = 0.3) +\n  xlab(\"\\n AUC\") +\n  # since the vertical line refers to dolphin_agg, we need to specify the dataset explicitly \n  geom_vline(data = dolphin_agg, \n             aes(xintercept = mean_AUC, color = group),\n             lty = \"dashed\")"
  },
  {
    "objectID": "practice-sheets/06a-model-comparison.html",
    "href": "practice-sheets/06a-model-comparison.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Load relevant packages and “set the scene.”\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models)."
  },
  {
    "objectID": "practice-sheets/06a-model-comparison.html#normal-and-robust-regression-models",
    "href": "practice-sheets/06a-model-comparison.html#normal-and-robust-regression-models",
    "title": "Bayesian regression: theory & practice",
    "section": "Normal and robust regression models",
    "text": "Normal and robust regression models\nA normal regression model uses a normal error function.\n\nfit_n <- brm(\n  formula = y ~ x,\n  data = data_robust,\n  # student prior for slope coefficient\n  prior = prior(\"student_t(1,0,30)\", class = \"b\"),\n)\n\nWe will want to compare this normal regression model with a robust regression model, which uses a Student’s t distribution instead as the error function around the linear predictor:\n\nfit_r <- brm(\n  formula = y ~ x,\n  data = data_robust,\n  # student prior for slope coefficient\n  prior = prior(\"student_t(1,0,30)\", class = \"b\"),\n  family = student()\n)\n\nLet’s look at the posterior inferences of both models about the true (known) parameters of the regression line:\n\nprep_summary <- function(fit, model) {\n  tidybayes::summarise_draws(fit) |> \n    mutate(model = model) |> \n    select(model, variable, q5, mean, q95) |> \n    filter(grepl(variable, pattern = '^b'))  \n}\n\nrbind(prep_summary(fit_n, \"normal\"), prep_summary(fit_r, \"robust\"))\n\n# A tibble: 4 × 5\n  model  variable       q5  mean   q95\n  <chr>  <chr>       <dbl> <dbl> <dbl>\n1 normal b_Intercept  2.45  7.75 13.2 \n2 normal b_x          6.98 13.4  19.5 \n3 robust b_Intercept  1.81  2.48  3.23\n4 robust b_x          4.95  6.07  7.24\n\n\nRemember that the true intercept is 2 and the true slope is 4. Clearly the robust regression model has recovered the ground-truth parameters much better."
  },
  {
    "objectID": "practice-sheets/06a-model-comparison.html#leave-one-out-cross-validation",
    "href": "practice-sheets/06a-model-comparison.html#leave-one-out-cross-validation",
    "title": "Bayesian regression: theory & practice",
    "section": "Leave-one-out cross validation",
    "text": "Leave-one-out cross validation\nWe can use the loo package to compare these two models based on their posterior predictive fit. Here’s how:\n\nloo_comp <- loo_compare(list(normal = loo(fit_n), robust = loo(fit_r)))\nloo_comp\n\n       elpd_diff se_diff\nrobust    0.0       0.0 \nnormal -132.5      26.5 \n\n\nWe see that the robust regression model is better by ca. -132 points of expected log predictive density. The table shown above is ordered with the “best” model on top. The column elpd_diff lists the difference in ELPD of every model to the “best” one. In our case, th estimated ELPD difference has a standard error of about 26. Computing a \\(p\\)-value for this using Lambert’s \\(z\\)-score method, we find that this difference is “significant” (for which we will use other terms like “noteworthy” or “substantial” in the following):\n\n1 - pnorm(-loo_comp[2,1], loo_comp[2,2])\n\n[1] 0\n\n\nWe conclude from this that the robust regression model is much better at predicting the data (from a posterior point of view)."
  },
  {
    "objectID": "practice-sheets/06a-model-comparison.html#bayes-factor-model-comparison-with-bridge-sampling",
    "href": "practice-sheets/06a-model-comparison.html#bayes-factor-model-comparison-with-bridge-sampling",
    "title": "Bayesian regression: theory & practice",
    "section": "Bayes factor model comparison (with bridge sampling)",
    "text": "Bayes factor model comparison (with bridge sampling)\nWe use bridge sampling, as implemented in the formidable bridgesampling package, to estimate the (log) marginal likelihood of each model. To do this, we need also samples from the prior. To do this reliably, we need many more samples than we would normally need for posterior inference. We can update() existing fitted models, so that we do not have to copy-paste all specifications (formula, data, prior, …) each time. It’s important for bridge_sampler() to work that we save all parameters (including prior samples).\n\nif (rerun_models) {\n  # refit normal model\n  fit_n_4Bridge <- update(\n    fit_n,\n    iter = 5e5,\n    save_pars = save_pars(all = TRUE)\n  )\n  # refit robust model\n  fit_r_4Bridge <- update(\n    fit_r,\n    iter = 5e5,\n    save_pars = save_pars(all = TRUE)\n  )\n  normal_bridge <- bridge_sampler(fit_n_4Bridge, silent = T)\n  write_rds(normal_bridge, \"06-normal_bridge.rds\")\n  robust_bridge <- bridge_sampler(fit_r_4Bridge, silent = T)  \n  write_rds(robust_bridge, \"06-robust_bridge.rds\")\n} else {\n  normal_bridge <- read_rds(\"06-normal_bridge.rds\")  \n  robust_bridge <- read_rds(\"06-robust_bridge.rds\")\n}\n\nbf_bridge <- bridgesampling::bf(robust_bridge, normal_bridge)\n\nWe can then use the bf (Bayes factor) method from the bridgesampling package to get the Bayes factor (here: in favor of the robust regression model):\n\nbf_bridge\n\nEstimated Bayes factor in favor of robust_bridge over normal_bridge: 41136407426471809154394622543225576928422395904.00000\n\n\nAs you can see, this is a very clear result. If we had equal levels of credence in both models, after seeing the data, our degree of belief in the robust regression model should … well, virtually infinitely higer than our degree of belief in the normal model."
  },
  {
    "objectID": "practice-sheets/05a-hierarchical-models-tutorial.html",
    "href": "practice-sheets/05a-hierarchical-models-tutorial.html",
    "title": "Bayesian regression: theory & practice",
    "section": "",
    "text": "Load relevant packages and “set the scene.”\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\nThis tutorial takes you through one practical example, showing the use of multilevel models. The main learning goals are:"
  },
  {
    "objectID": "practice-sheets/05a-hierarchical-models-tutorial.html#the-independence-assumption",
    "href": "practice-sheets/05a-hierarchical-models-tutorial.html#the-independence-assumption",
    "title": "Bayesian regression: theory & practice",
    "section": "The independence assumption",
    "text": "The independence assumption\nLast week, we ended on a conundrum. We looked at the probability of observing a straight trajectory predicted by response latency. Here is the plot for all data in the click group plus a logistic smooth term:\n\n# set up data frame\ndolphin_agg <- dolphin %>% \n  filter(correct == 1,\n         group == \"click\") %>% \n  mutate(straight = as.factor(ifelse(prototype_label == \"straight\", 1, 0)),\n         log_RT_s = my_scale(log(RT)),\n         AUC_s = my_scale(AUC))\n\ndolphin_agg$straight_numeric <- as.numeric(as.character(dolphin_agg$straight))\n\n# plot predicted values against data\nggplot(data = dolphin_agg,\n       aes(x = log_RT_s, y = straight_numeric)) +\n  geom_point(position = position_jitter(height = 0.02), alpha = 0.2) +\n  geom_smooth(method = \"glm\", color = \"red\",\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ggtitle(\"overall relationship\") +\n  theme(legend.position = \"right\")\n\n\n\n\nThis picture suggest a negative relationship between the probability of observing straight trajectories (y) and peoples’ response times (x) (i.e. line goes down).\nBut this analysis looked at all responses at once and disregarded that responses came from groups of sources. For example, responses that come from one and the same participant are dependent on each other because participants (subject_id) might differ in characteristics relevant to the task, like how fast they move and how many times they move to the target in a straight trajectory. Another group of data points is related to different stimuli (exemplars). Different stimuli might have some inherent properties that lead to different response times and different proportions of straight trajectories. So analyzing the data without telling the model about these groups violates an important assumption of linear models. The independence assumption.\nLet’s look at these groups individually, starting by aggregating over over subject_ids and exemplars and plot the results.\n\n# aggregate over subjects\ndolphin_agg2 <- dolphin_agg %>% \n  group_by(subject_id) %>% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight_numeric),\n            total = n()) \n\n# plot predicted values for subjects\nggplot(data = dolphin_agg2,\n       aes(x = log_RT_s, y = straights/total)) +\n  geom_point(size = 2, alpha = 0.5) +\n  # we use the geom_smooth function here as a rough proxy of the relationship \n  geom_smooth(method = \"glm\", \n              formula = y ~ x, color = \"red\",\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5) +\n  ggtitle(\"subject aggregates\") +\n  theme(legend.position = \"right\")\n\n\n\n\nHuh. That is interesting. So if we aggregate over subjects, i.e. each data point is one subject reacting to all exemplars, we get a positive relationship between response latency and the proportion of straight trajectories. The slower the reaction the more likely a straight trajectory. That could mean that those participants that are generally slower are also the ones that tend to move more often in a straight fashion. It also makes sense to some extent. Maybe those participants seem to wait until they have made their decision and then move to the target immediately, while other participants move upwards right away and make their decision on the fly during the decision.\nNow, let’s aggregate over exemplars:\n\n# aggregate over exemplars\ndolphin_agg3 <- dolphin_agg %>% \n  group_by(exemplar) %>% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight_numeric),\n            total = n()) \n\n# plot predicted values for exemplars\nggplot(data = dolphin_agg3,\n       aes(x = log_RT_s, y = straights/total)) +\n  geom_point(size = 2, alpha = 0.5) +\n  geom_smooth(method = \"glm\", \n              formula = y ~ x, color = \"red\",\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5) +\n  ggtitle(\"stimuli aggregates\") +\n  theme(legend.position = \"right\")\n\n\n\n\nIf we look at the stimuli aggregates, i.e. each data point is one exemplar that all subjects have reacted to, we get a negative relationship between response latency and the proportion of straight trajectories. The quicker the reaction the more likely a straight trajectory. This could potentially reflect the difficulty of the categorization task. Maybe those exemplars that are inherently less ambiguous, for example the typical exemplars, don’t exhibit any response competition and are thus faster and more often straight.\nUltimately, we use our models to make a generalizing statement about a population. If our theory predicts a relationship between straight trajectories and response latency (without further nuance), we should find this relationship across the population of people AND the population of stimuli. But if we say, “there are more straight trajectories in faster responses”, this claim seems to be only true for within-participant behavior. So we need to inform our models about such groupings in our data, or we might overconfidently make predictions."
  },
  {
    "objectID": "06-model-comparison.html",
    "href": "06-model-comparison.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "The three main things we can do with models are:\n\ninferring (credible) parameter values (on the assumption that the model we use is good (enough))\nmaking predictions (from an ex ante (a priori) or ex post (a posteriori)) point of view\n\npredictions can also be used for model checking (also known as model criticism)\n\ncomparing which of several models is better (in some sense of “better”)\n\nWe have so far looked only at the first two “pillars of Bayesian data analysis”. In this unit we look at the third, model comparison.\nModel comparison is deeply related to the second point: making predictions. We compare models based on their ability to predict data well enough. But not exclusively! We might also rely on other aspects, such as whether a model makes fewer spurious assumptions, i.e., is simpler, more economical or more parsimonious.\nAs usual, there is not one criterion for model comparison that everybody unanimously agrees to as the best. As usual, this is likely because “goodness of a model” is a multi-dimensional concept. What counts as a good model for science (knowledge gain; theoretical understanding) need not be the same as for engineering or application (getting high-quality predictions in the most efficient manner).\nThis unit therefore centers on two important tools for Bayesian model comparison that lie at opposite ends of a continuous spectrum, namely Bayes factors and leave-one-out (LOO) cross validation. Bayes factors take the most extreme point of view of ex ante predictions: we compare models without any updating on the relevant data. LOO-CV, on the other hand, take the (almost) most extreme point of view of ex post predictions, comparing models that are trained on all of the relevant data, except one single data observations.\nIn the practical session, we explore how these different perspectives can give rise to opposite results. This is not a puzzle or paradox, and maybe not even something to quarrel about. It is a natural reflex of comparing the same objects based on a different task, namely making predictions before training, or after.\nHere are slides for session 6."
  },
  {
    "objectID": "01-basics.html",
    "href": "01-basics.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "The first session introduces the basics of BDA and linear regression modeling.\nHere are slides for session 1.\nThere are two hands-on exploration and exercise sheets. The first is a crash-course (or recap) of basic wrangling and plotting in the tidyverse. Here, we will also have a first rendez-vous with the data set (mouse-tracking data) which most of our hands-on monkeying will be based on. The second sheet covers Bayesian simple linear regression (using brms)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This site provides material for an introductory-level course on Bayesian linear regression modeling. The course presupposes some prior exposure to statistics and some acquaintance with R."
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Intended audience",
    "text": "Intended audience\nThis course is offered to students (both under-graduate and graduate) of (computational) linguistics and cognitive science."
  },
  {
    "objectID": "index.html#scope",
    "href": "index.html#scope",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Scope",
    "text": "Scope\nThe course introduces the basics of Bayesian data analysis, and then immedidately zooms in on regression modeling. Starting with simple linear regression, we want to reach a basic understanding of hierarchical generalized linear models."
  },
  {
    "objectID": "index.html#additional-material",
    "href": "index.html#additional-material",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Additional material",
    "text": "Additional material\nAn even more basic introduction to data analysis (introducing R, tidyverse, Bayesian and, eventually, also frequentist statistics) is the webbook “An introduction to Data Analysis”."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nPart of this material was used in a previous course, co-taught with the great Timo Roettger. I’m recycling some of his material here, but have (in full awareness of the catastrohpe) made it more dry, less insightful and more nerdy to conceal the breach of authorship."
  },
  {
    "objectID": "04-MCMC.html",
    "href": "04-MCMC.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "The success of Bayesian statistics is in large part the fruit of very clever algorithms and efficient implementations for drawing samples from complex, high-dimensional posterior distributions. This unit covers:\n\nMarkov Chain Monte Carlo methods, in particular:\n\nsimple Metropolis-Hastings and\nHamiltonian Monte Carlo\n\ncommon notions and diagnostics for assessing the quality of MCMC samples, such as:\n\n\\(\\hat{R}\\)\nautocorrelation\neffective sample size\ntraceplots\ndivergent transitions\n\n\nHere are slides for session 4."
  },
  {
    "objectID": "05-hierarchical-models.html",
    "href": "05-hierarchical-models.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This unit introduces hierarchical regression models (also known as mixed models, or random-effect models).\nHere are slides for session 5."
  },
  {
    "objectID": "02-priorPredictives-categPredictors.html",
    "href": "02-priorPredictives-categPredictors.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "Part 1: Priors and predictives\nPriors are an essential part of Bayesian data analysis. There are different approaches to specify priors:\n\nwe can try to be feign maximal uncertainty;\nwe can try to commit strongly to prior knowledge we think is relevant;\nwe can use priors to make the model “well-behaved” (e.g., help training / fitting)\nwe can try to use priors that are as “objective” and “non-committal”.\n\nWhatever approach to specifying priors we adopt, all (proper) priors generate predictions. So here we look at prior and posterior predictives of a Bayesian model. This is helpful because you might not have intuitions about how to select priors, but you will likely have intuitions about what counts are reasonable /a priori/ predictions.\nHere are slides for session 2.\n\n\nPart 2: Categorical predictors\nIn the second part of this session, we look at categorical predictors in simple linear regression models.\nMore on this topic can be found in this chapter of the webbook."
  }
]