[
  {
    "objectID": "practice-sheets/09a-stochastic-dependence.html",
    "href": "practice-sheets/09a-stochastic-dependence.html",
    "title": "Stochastic independence",
    "section": "",
    "text": "Preamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nUsing regression modeling to test for stochastic independence\nHere are three data sets, each with triples of associated observations for variables \\(X\\), \\(Y\\), and \\(Z\\). Ideally, you do not look at the code, so you will have more fun with the exercise below.\n\n\nToggle code\n##################################################\n# creating three data sets with various dependencies\n##################################################\n\ndata_1 &lt;- tibble(\n  X = rnorm(500),\n  Z = map_dbl(X, function(i) rnorm(1, mean = i)),\n  Y = map_dbl(Z, function(i) rnorm(1, mean = i))\n)\n\ndata_2 &lt;- tibble(\n  Z = rnorm(500),\n  X = map_dbl(Z, function(i) rnorm(1, mean = i)),\n  Y = map_dbl(Z, function(i) rnorm(1, mean = i))\n)\n\ndata_3 &lt;- tibble(\n  X = rnorm(500),\n  Y = rnorm(500),\n  Z = map2_dbl(X, Y, function(i,j) rnorm(1, mean = i+j)),\n)\n\n\n\n\n\n\n\n\nExercise 1: Use regression to assess stochastic independence\n\n\n\n\n\nUse linear regression to investigate whether:\n\n\\(X\\) and \\(Y\\) are stochastically independent\n\\(X\\) and \\(Y\\) are stochastically independent given Z\n\nin each of these data sets.\nIdeally, write a single function that takes a data set as input and output information that tells you about whether \\(X\\) and \\(Y\\) are independent.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere is a reusable function:\n\n\nToggle code\ntest_stochDep &lt;- function(input_data) {\n  \n  fit_simple   &lt;- brms::brm(Y ~ X, data = input_data)\n  fit_multiple &lt;- brms::brm(Y ~ X + Z, data = input_data)\n  \n  result_simple &lt;- brms::hypothesis(fit_simple, \"X&gt;0\")\n  result_multiple &lt;-brms::hypothesis(fit_multiple, \"X&gt;0\")\n  \n  return(list(simple = result_simple, multiple = result_multiple))\n}\n\nresult_1 = test_stochDep(data_1)\nresult_2 = test_stochDep(data_2)\nresult_3 = test_stochDep(data_3)\n\n\nIn data set 1, \\(X\\) and \\(Y\\) are not stochastically independent, but turn independent given \\(Z\\).\n\n\nToggle code\nresult_1\n\n\n$simple\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1    (X) &gt; 0     1.06      0.06     0.96     1.17        Inf         1    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n$multiple\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1    (X) &gt; 0    -0.03      0.06    -0.13     0.08       0.53      0.35     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nIn data set 2, \\(X\\) and \\(Y\\) are not stochastically independent, and turn independent given \\(Z\\).\n\n\nToggle code\nresult_2\n\n\n$simple\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1    (X) &gt; 0     0.53      0.04     0.46     0.59        Inf         1    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n$multiple\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1    (X) &gt; 0        0      0.05    -0.07     0.08       0.99       0.5     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nIn data set 3, \\(X\\) and \\(Y\\) are stochastically independent, but cease to be given \\(Z\\).\n\n\nToggle code\nresult_3\n\n\n$simple\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1    (X) &gt; 0    -0.05      0.05    -0.13     0.03       0.15      0.13     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n$multiple\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1    (X) &gt; 0     -0.5      0.04    -0.57    -0.44          0         0     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Interpret your results\n\n\n\n\n\nInfer which (if any) of the three data sets was created with one of the following causal structures:\n\ncollider: \\(X, Y \\rightarrow Z\\)\nchain: \\(X \\rightarrow Z \\rightarrow Y\\)\nfork: \\(Z \\rightarrow X,Y\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nData sets 1 and 2 could be from a chain or a fork, because \\(X\\) and \\(Y\\) are not stochastically independent, but turn independent given \\(Z\\). We cannot say which based on these tests.\nData set 3 could be from a collider, because \\(X\\) and \\(Y\\) are stochastically independent, but cease to be given \\(Z\\)."
  },
  {
    "objectID": "practice-sheets/00-preamble.html",
    "href": "practice-sheets/00-preamble.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}"
  },
  {
    "objectID": "practice-sheets/03b-hierarchical-models-exercises.html",
    "href": "practice-sheets/03b-hierarchical-models-exercises.html",
    "title": "05b: Hierarchical regression models (exercises)",
    "section": "",
    "text": "Preamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nToggle code\ndolphin &lt;- aida::data_MT\nmy_scale &lt;- function(x) c(scale(x))\n\n\n\n\nExercise 1: Logistic regression\nConsider the following model formula for the dolphin data set:\n\n\nToggle code\nbrms::bf(MAD ~ condition + \n     (condition || subject_id) +\n     (condition || exemplar))\n\n\nMAD ~ condition + (condition || subject_id) + (condition || exemplar) \n\n\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nWhy is the random effect structure of this model questionable? Can we meaningfully estimate all parameters? (Hint: Think about what group levels vary across predictor levels)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFactor condition is not crossed with exemplar. An exemplar is either typical or atypical, thus a random slope does not make sense.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b\n\n\n\n\n\nUse the following data frame:\n\n\nToggle code\n# set up data frame\ndolphin_correct &lt;- dolphin %&gt;% \n  filter(correct == 1) %&gt;% \n  mutate(log_RT_s = my_scale(log(RT)),\n         AUC_s = my_scale(AUC))\n\n\nRun a multilevel model that predicts AUC_s based on condition. Specify maximal random effect structures for exemplars and subject_ids (ignore correlations between intercepts and slopes for now). Specify a seed = 98.\nIf you encounter “divergent transition” warning, make them go away by refitting the model appropriately (Hint: brms gives very useful, actionable advice)\n(This might take a couple of minutes, get used to it ;)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# refit with upped adapt_delta and max_treedepth\nxmdl_AUC2 &lt;- brm(AUC_s ~ condition +\n                  (condition || subject_id) +\n                  (1 | exemplar),\n                data = dolphin_correct,\n                control=list(adapt_delta=0.99, max_treedepth=15), \n                seed = 98\n                )\nxmdl_AUC2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1c\n\n\n\n\n\nYou want to run a multilevel model that predicts log_RT_s based on group. You want to account for group-level variation of both subject_id and exemplar. What kind of groupings can be meaningfully estimated, given the dataset and the experimental design. You can check the crossing of different vectors with xtabs() for example.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# check crossing\nxtabs(~ group + subject_id, dolphin_correct)\n\n\n       subject_id\ngroup   1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014\n  click    0   19   17    0    0    0   19    0    0   19    0    0   17   16\n  touch   16    0    0   18   17   19    0   18   19    0   19   16    0    0\n       subject_id\ngroup   1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028\n  click   18    0    0    0   19   17    0   19   14    0    0   19    0   19\n  touch    0   18   19   17    0    0   18    0    0   19   12    0   17    0\n       subject_id\ngroup   1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042\n  click    0   19   15    0   17   19    0    0   18    0   19    0   17    0\n  touch   19    0    0   19    0    0   19   16    0   17    0   18    0   18\n       subject_id\ngroup   1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056\n  click    0   17   17   18    0   18   17   18    0    0    0    0   18    0\n  touch   19    0    0    0   19    0    0    0   18   17   18   19    0   19\n       subject_id\ngroup   1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070\n  click    0    0   18   18   17    0   16   17    0   18    0   17    0   18\n  touch   19   18    0    0    0   19    0    0   18    0   18    0   17    0\n       subject_id\ngroup   1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084\n  click   19   19    0   19    0   19    0   19   19    0    0    0   18   19\n  touch    0    0   15    0   19    0   19    0    0   19   17   15    0    0\n       subject_id\ngroup   1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098\n  click   19    0    0   18    0    0   18    0    0    0   19   15    0    0\n  touch    0   11   17    0   19   18    0   19   18   15    0    0   19   17\n       subject_id\ngroup   1099 1100 1101 1102 1103 1104 1105 1106 1107 1108\n  click   16    0   18    0    0   14   19    0   17    0\n  touch    0   19    0   18   17    0    0   19    0   17\n\n\nToggle code\n# individual subject_ids contributed data only to one group because it is a between-subject design\n# --&gt; we need varying intercepts only, i.e. a different base-rate for subjects\n\nxtabs(~ group + exemplar, dolphin_correct)\n\n\n       exemplar\ngroup   alligator bat butterfly cat chameleon dog eel goldfish hawk horse lion\n  click        51  38        52  53        52  53  50       53   53    53   53\n  touch        53  46        54  54        54  53  53       55   52    53   54\n       exemplar\ngroup   penguin rabbit rattlesnake salmon sealion shark sparrow whale\n  click      48     53          38     53      48    46      53    42\n  touch      52     51          43     52      50    44      55    45\n\n\nToggle code\n# each exemplar contributes data to both groups\n# --&gt; we can integrate varying intercepts and slopes for exemplars\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1d\n\n\n\n\n\nRun a multilevel model that predicts log_RT_s based on group and add maximal random effect structures licensed by the experimental design (ignore possible random intercept-slope interactions for now).\nSpecify weakly informative priors as you see fit.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\npriors &lt;- c(\n  #priors for all fixed effects (group)\n  set_prior(\"student_t(3, 0, 3)\", class = \"b\"),\n  #prior for the Intercept\n  set_prior(\"student_t(3, 0, 3)\", class = \"Intercept\"),\n  #prior for all SDs including the varying intercepts and slopes\n  set_prior(\"student_t(3, 0, 3)\", class = \"sd\")\n)\n\nxmdl &lt;- brm(log_RT_s ~ group + \n              (1 | subject_id) +\n              (group || exemplar),\n            prior = priors,\n            data = dolphin_correct)\nxmdl\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log_RT_s ~ group + (1 | subject_id) + (group || exemplar) \n   Data: dolphin_correct (Number of observations: 1915) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~exemplar (Number of levels: 19) \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)      0.45      0.09     0.32     0.65 1.00      778     1525\nsd(grouptouch)     0.11      0.06     0.01     0.23 1.00      890      890\n\n~subject_id (Number of levels: 108) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.57      0.04     0.49     0.65 1.00      790     1533\n\nPopulation-Level Effects: \n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      0.29      0.13     0.04     0.54 1.01      465     1009\ngrouptouch    -0.52      0.12    -0.74    -0.28 1.01      447      937\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.68      0.01     0.66     0.71 1.00     5868     2566\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1e\n\n\n\n\n\nExtract the posterior means and 95% CrIs of touch vs. click log_RT_s and plot them.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# Extract the posteriors\nposteriors &lt;- xmdl %&gt;%\n  spread_draws(b_Intercept, \n               b_grouptouch) %&gt;%\n  # calculate posteriors for each individual level\n  mutate(click = b_Intercept,\n         touch = b_Intercept + b_grouptouch) %&gt;% \n  select(click, touch) %&gt;% \n  gather(key = \"parameter\", value = \"posterior\") %&gt;% \n  group_by(parameter) %&gt;% \n  summarise(mean_posterior = mean(posterior),\n            `95lowerCrI` = HDInterval::hdi(posterior, credMass = 0.95)[1],\n            `95higherCrI` = HDInterval::hdi(posterior, credMass = 0.95)[2])\n\n# plot\nggplot(data = posteriors, \n       aes(x = parameter, y = mean_posterior,\n           color = parameter, fill = parameter)) + \n  geom_errorbar(aes(ymin = `95lowerCrI`, ymax = `95higherCrI`),\n                width = 0.2, color = \"grey\") +\n  geom_line(aes(group = 1), color = \"black\") +\n  geom_point(size = 4) +\n  labs(x = \"group\",\n       y = \"posterior log(RT) (scaled)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1f\n\n\n\n\n\nAdd the posterior estimates for different exemplars to the plot. (Hint: Check code from the previous “tutorial” to extract the random effect estimates.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# extract the random intercepts for exemplars\nrandom_intc_matrix &lt;- ranef(xmdl)$exemplar[, , \"Intercept\"] %&gt;% \n  round(digits = 2) \n\n# extract the by-exemplar random slopes for group\nrandom_slope_matrix &lt;- ranef(xmdl)$exemplar[, , \"grouptouch\"] %&gt;% \n  round(digits = 2)\n\n# random intercepts to dataframe\nrandom_intc_df &lt;- data.frame(exemplar = row.names(random_intc_matrix), random_intc_matrix) %&gt;% \n  select(exemplar, Estimate) %&gt;% \n  rename(rintercept = Estimate)\n\n# combine with random slope matrix\nrandom_slope_df &lt;- data.frame(exemplar = row.names(random_slope_matrix), random_slope_matrix) %&gt;% \n  select(exemplar, Estimate) %&gt;% \n  rename(rslope = Estimate) %&gt;% \n  full_join(random_intc_df) %&gt;% \n  # add population parameters and group-specific parameters\n  mutate(click_population = fixef(xmdl)[1],\n         touch_population = fixef(xmdl)[1] + fixef(xmdl)[2],\n         click = rintercept + click_population,\n         touch = rintercept + rslope + touch_population) %&gt;% \n  select(exemplar, touch, click) %&gt;% \n  gather(parameter, mean_posterior, -exemplar)\n  \n\n# combine with plot\nggplot(data = posteriors, \n       aes(x = parameter, y = mean_posterior,\n           color = parameter, fill = parameter)) + \n   # add random estimates\n  geom_point(data = random_slope_df, \n             alpha = 0.4,\n             size = 2,\n             position = position_jitter(width = 0.01)\n             ) +\n  # add lines between random estimates\n  geom_line(data = random_slope_df, \n            aes(group = exemplar),\n            color = \"grey\", alpha = 0.3) +\n  # add population-level estimates\n  geom_errorbar(aes(ymin = `95lowerCrI`, ymax = `95higherCrI`),\n                width = 0.2, color = \"grey\") +\n  geom_line(aes(group = 1), size = 2, color = \"black\") +\n  geom_point(size = 4, pch = 21, color = \"black\") +\n  labs(x = \"group\",\n       y = \"posterior log(RT) (scaled)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Poisson regression\n\n\n\n\n\n\nExercise 2a\n\n\n\n\n\nRun a multilevel poisson regression predicting xpos_flips based on group, log_RT_s, and their two-way interaction. Specify maximal random effect structures for exemplars and subject_ids licensed by the design (ignore correlations between intercepts and slopes for now). (Hint: allow groupings to differ regarding the interaction effect if licensed by the design.) Specify weakly informative priors.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\npriors &lt;- c(\n  #priors for all fixed effects\n  set_prior(\"student_t(3, 0, 3)\", class = \"b\"),\n  #prior for all SDs including the varying intercepts and slopes for both groupings\n  set_prior(\"student_t(3, 0, 3)\", class = \"sd\")\n)\n\npoisson_mdl &lt;- brm(xpos_flips ~ group * log_RT_s +\n                     (log_RT_s || subject_id) +\n                     (group * log_RT_s || exemplar),\n                   data = dolphin_correct,\n                   prior = priors,\n                   family = \"poisson\")\n\npoisson_mdl\n\n\n Family: poisson \n  Links: mu = log \nFormula: xpos_flips ~ group * log_RT_s + (log_RT_s || subject_id) + (group * log_RT_s || exemplar) \n   Data: dolphin_correct (Number of observations: 1915) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~exemplar (Number of levels: 19) \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)               0.06      0.04     0.00     0.15 1.00     1473\nsd(grouptouch)              0.11      0.07     0.00     0.26 1.00     1060\nsd(log_RT_s)                0.04      0.04     0.00     0.13 1.00     1515\nsd(grouptouch:log_RT_s)     0.08      0.06     0.00     0.22 1.00     1252\n                        Tail_ESS\nsd(Intercept)               1936\nsd(grouptouch)              1842\nsd(log_RT_s)                2086\nsd(grouptouch:log_RT_s)     1861\n\n~subject_id (Number of levels: 108) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.59      0.06     0.49     0.71 1.00     1278     2285\nsd(log_RT_s)      0.13      0.05     0.02     0.23 1.00      645      924\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              -0.15      0.09    -0.33     0.03 1.00     1109     1665\ngrouptouch             -0.46      0.13    -0.72    -0.20 1.00     1379     1985\nlog_RT_s                0.39      0.05     0.30     0.49 1.00     2933     2994\ngrouptouch:log_RT_s     0.17      0.07     0.03     0.32 1.00     2769     2815\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2b\n\n\n\n\n\nExtract and plot the population level estimates for both click and touch group as a regression line into a scatter plot (x = b_log_RT_s, y = xpos_flips).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# extract posterior means for model coefficients\npredicted_Poisson_values &lt;- poisson_mdl %&gt;%\n  spread_draws(b_Intercept, b_log_RT_s, \n               b_grouptouch, `b_grouptouch:log_RT_s`\n               ) %&gt;%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-5, 10, 0.5))) %&gt;% \n  unnest(log_RT) %&gt;%\n  mutate(click = exp(b_Intercept + b_log_RT_s*log_RT),\n         touch = exp(b_Intercept + b_log_RT_s*log_RT +\n                            b_grouptouch + `b_grouptouch:log_RT_s`*log_RT)) %&gt;%\n  select(log_RT, click, touch) %&gt;% \n  gather(group, posterior, -log_RT) %&gt;% \n  group_by(log_RT, group) %&gt;%\n  summarise(pred_m = mean(posterior, na.rm = TRUE),\n            pred_low = quantile(posterior, prob = 0.025),\n            pred_high = quantile(posterior, prob = 0.975)\n            ) \n\n# plot population level\nggplot(data = predicted_Poisson_values, aes(x = log_RT, y = pred_m)) +\n  geom_point(data = dolphin_correct, aes(x = log_RT_s, y = xpos_flips, color = group), \n             position = position_jitter(height = 0.2), alpha = 0.2) +\n  geom_line(aes(y = pred_m, color = group), size = 2) +\n  facet_grid(~group) +\n  ylab(\"Predicted prob of xflips\") +\n  ylim(-1,10) +\n  xlim(-5,10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2c\n\n\n\n\n\nExtract the respective subject-specific estimates from the model and plot them into the same plot (use thinner lines).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# extract the random effects for subject_id\n\n# intercepts\nrandom_intc_matrix &lt;- ranef(poisson_mdl)$subject_id[, , \"Intercept\"] %&gt;% \n  round(digits = 3)\n\n# slopes\nrandom_slope_matrix &lt;- ranef(poisson_mdl)$subject_id[, , \"log_RT_s\"] %&gt;% \n  round(digits = 3)\n\n# to df\nrandom_intc_df &lt;- data.frame(subject_id = row.names(random_intc_matrix), random_intc_matrix) %&gt;% \n  select(subject_id, Estimate) %&gt;% \n  rename(rintercept = Estimate)\n\n# wrangle into one df \nrandom_slope_df &lt;- data.frame(subject_id = row.names(random_slope_matrix), random_slope_matrix) %&gt;% \n  select(subject_id, Estimate) %&gt;% \n  rename(rslope = Estimate) %&gt;% \n  full_join(random_intc_df) %&gt;% \n  expand_grid(group = c(\"click\", \"touch\")) %&gt;% \n  # add population parameters and group-specific parameters\n  mutate(adjusted_int = ifelse(group == \"click\",\n           rintercept + fixef(poisson_mdl)[1],\n           rintercept + fixef(poisson_mdl)[1] + fixef(poisson_mdl)[2]),\n         adjusted_slope = ifelse(group == \"click\",\n           rslope + fixef(poisson_mdl)[3],\n           rslope + fixef(poisson_mdl)[3] + fixef(poisson_mdl)[4])) %&gt;% \n  mutate(log_RT = list(seq(-5, 10, 0.5))) %&gt;% \n  unnest(log_RT) %&gt;%\n  select(subject_id, log_RT, group, \n         adjusted_int, adjusted_slope) %&gt;% \n  group_by(subject_id, log_RT, group) %&gt;%\n  mutate(pred_m = exp(adjusted_int + adjusted_slope*log_RT))\n\n# plot the individual regression lines on top of the population estimate\nggplot(data = predicted_Poisson_values, aes(x = log_RT, y = pred_m)) +\n  geom_point(data = dolphin_correct, aes(x = log_RT_s, y = xpos_flips), \n             position = position_jitter(height = 0.2), alpha = 0.01) +\n  geom_line(aes(y = pred_m, color = group), size = 2) +\n  geom_line(data = random_slope_df, \n            aes(x = log_RT, y = pred_m, group = subject_id, color = group),\n            size = 0.5, alpha = 0.2) +\n  facet_grid(~group) +\n  ylab(\"Predicted prob of xflips\") +\n  ylim(-1,10) +\n  xlim(-5,10)"
  },
  {
    "objectID": "practice-sheets/08-hypothesis-testing.html",
    "href": "practice-sheets/08-hypothesis-testing.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "There are different ways of addressing research hypotheses with Bayesian models. For a more in-depth treatment see here. This unit gives code examples that showcase approaches to hypotheses testing based on the three pillars of BDA:\nThe running example is the 24/7 coin flip case (flipping a coin \\(N=24\\) times and observing \\(k=7\\) heads), addressing the research question that the coin is fair (\\(\\theta = 0.5\\))."
  },
  {
    "objectID": "practice-sheets/08-hypothesis-testing.html#credible-intervals",
    "href": "practice-sheets/08-hypothesis-testing.html#credible-intervals",
    "title": "Hypothesis testing",
    "section": "Credible Intervals",
    "text": "Credible Intervals\nThe first approach compares the ROPE to an \\(n\\)% credible interval of the posterior. The plot below shows the posterior, starting from a flat prior, for our case. it also shows the 95% credible interval.\n\n\nToggle code\nhdi = HDInterval::hdi(qbeta , shape1 = 8 , shape2 = 18 )\nhdiData &lt;- tibble(\n  theta = rep(hdi, each = 2),\n  post = c(0,dbeta(hdi, 8, 18), 0)\n)\nexpData &lt;- tibble(\n  theta = c(8/26,8/26),\n  post = c(0,dbeta(8/26, 8, 18 ))\n)\n\ntibble(\n  theta = seq(0.01,1, by = 0.01),\n  posterior = dbeta(seq(0.01,1, by = 0.01), 8, 18 )\n) %&gt;% \n  ggplot(aes(x = theta, y = posterior)) + \n  xlim(0,1) + \n  labs(\n    x = latex2exp::TeX(\"Bias $\\\\theta$\"),\n    y = latex2exp::TeX(\"Posterior probability $P_{M}(\\\\theta \\\\, | \\\\, D)$\"),\n    title = \"Posterior\"\n  ) +\n  geom_line(data = hdiData, aes(x = theta, y = post), color = project_colors[2], size = 1.5) +\n  geom_label(x = 0.7, y = 0.5, label = \"Cred.Int.: 0.14 - 0.48\", color = project_colors[2], size = 5) +\n  geom_line(data = expData, aes(x = theta, y = post), color = project_colors[1], size = 1.5) +\n  geom_label(x = 0.52, y = dbeta(8/26, 8, 18 ), label = \"expectation: 0.308\", color = project_colors[1], size = 5) +\n  geom_line(color = \"black\", size = 2)"
  },
  {
    "objectID": "practice-sheets/08-hypothesis-testing.html#posterior-probability",
    "href": "practice-sheets/08-hypothesis-testing.html#posterior-probability",
    "title": "Hypothesis testing",
    "section": "Posterior probability",
    "text": "Posterior probability\n\n\nToggle code\nk &lt;- 7\nN &lt;- 24\n\nROPE &lt;- c(0.49,0.51)\n\n# posterior probability of the ROPE\npostProb_ROPE &lt;- pbeta(ROPE[2],8,18) - pbeta(ROPE[1],8,18)\n\n# plot\nplotData &lt;- tibble(\n  theta = seq(0,1, length.out = 200),\n  posterior = dbeta(theta, 8, 18)\n)\nplotData |&gt; \n  ggplot(aes(x = theta, y = posterior)) + \n  geom_ribbon(aes(ymin=0, ymax=posterior), \n              fill=project_colors[2],\n              alpha=0.8, \n              data=subset(plotData, theta &gt;= 0.485 & theta &lt;= 0.515)) +\n  geom_line(size = 2) +\n  xlim(0,1) + \n  geom_label(x = 0.75, y = 0.5, \n             label = str_c(\"Post. prob. ROPE: \", round(postProb_ROPE, 3)), \n             color = project_colors[2], size = 3) +\n  labs(\n    x = latex2exp::TeX(\"Bias $\\\\theta$\"),\n    y = latex2exp::TeX(\"Posterior probability $P_{M}(\\\\theta \\\\, | \\\\, D)$\"),\n    title = \"Posterior\"\n  ) \n\n\n\n\n\nToggle code\n## ggsave(\"../pics/24-7-posterior-prob.pdf\", width = 6, height = 4)"
  },
  {
    "objectID": "practice-sheets/08-hypothesis-testing.html#frequentist-p-value",
    "href": "practice-sheets/08-hypothesis-testing.html#frequentist-p-value",
    "title": "Hypothesis testing",
    "section": "Frequentist \\(p\\)-value",
    "text": "Frequentist \\(p\\)-value\nFor a frequentist hypothesis test with the null hypothesis that \\(\\theta = 0.5\\), we can simply use:\n\n\nToggle code\nbinom.test(k, 24)\n\n\n\n    Exact binomial test\n\ndata:  k and 24\nnumber of successes = 7, number of trials = 24, p-value = 0.06391\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.1261521 0.5109478\nsample estimates:\nprobability of success \n             0.2916667 \n\n\nWe can also approximate the \\(p\\)-value through Monte Carlo simulation, like so:\n\nobtain samples of hypothetical observations \\(k_{rep}\\) from the likelihood function given \\(\\theta = 0.5\\) and \\(N = 24\\)\nobtain the likelihood for each \\(k_{rep}\\)\nobtain the likelihood for the observed \\(k_{obs}\\)\ndetermine the proportion of sampled \\(k_rep\\) whose likelihood is at least as low as that of \\(k_{obs}\\)\n\n\n\nToggle code\nn_samples &lt;- 1e+7\nk_reps    &lt;- rbinom(n_samples, 24, prob=0.5)\nLH_k_reps &lt;- dbinom(k_reps, 24, prob=0.5)\nLH_k_obs  &lt;- dbinom(7, 24, prob=0.5)\nmean(LH_k_reps &lt;= LH_k_obs)\n\n\n[1] 0.0640344"
  },
  {
    "objectID": "practice-sheets/08-hypothesis-testing.html#bayesian-p-value",
    "href": "practice-sheets/08-hypothesis-testing.html#bayesian-p-value",
    "title": "Hypothesis testing",
    "section": "Bayesian \\(p\\)-value",
    "text": "Bayesian \\(p\\)-value\nTo get a \\(p\\) value from a Bayesian model (no matter whether prior or posterior), we follow the same recipe. The only complication is that, when using the likelihood as a test statistic as in the “exact test” above, we (usually) need to use Monte Carlo simulation to approximate the likelihood of each \\(k_{rep}\\).\n\nWithout BRMS\nTo more clearly see the logic of our sampling scheme, let’s first look at how to compute a Bayesian \\(p\\)-value with MC sampling without using the BRMS package. Notice that the number of samples to approximate the likelihood need not be as big (it could even be 1), as long as we compute the likelihood based on an independent sample of paramters from the model.\n\n\nToggle code\nlogistic &lt;- function(x) {\n  1 / (1 + exp(x))\n}\n\nlogit &lt;- function(x) {\n  log(x / (1-x))\n}\n\nepsilon &lt;- 0.01\nROPE = c(0.5 - epsilon, 0.5 + epsilon)\n\nget_prior_sample_theta &lt;- function(n_samples=1) {\n  sample_prior_eta   &lt;- runif(n_samples, logit(ROPE[1]), logit(ROPE[2]))\n  # sample_prior_eta   &lt;- rep(0,n_samples)\n  sample_prior_theta &lt;- logistic(sample_prior_eta)\n  # print(sample_prior_theta)\n  return(sample_prior_theta)\n}\n\nget_prior_sample_k &lt;- function(n_samples=1) {\n  sample_prior_theta &lt;- get_prior_sample_theta(n_samples)\n  sample_prior_k     &lt;- map_dbl(sample_prior_theta, function(theta) rbinom(1, 24, theta))\n  return(sample_prior_k)\n}\n\nget_LH_approx &lt;- function(k, n_samples=1) {\n  dbinom(k, 24, get_prior_sample_theta(n_samples), log = F) |&gt; mean()\n}\n\nn_samples_k         &lt;- 10000\nn_samples_LH_approx &lt;- 100\n\nk_reps    &lt;- get_prior_sample_k(n_samples_k)\nLH_k_reps &lt;- map_dbl(k_reps, function(k) get_LH_approx(k, n_samples_LH_approx))\nLH_k_obs  &lt;- get_LH_approx(k, 500000)\nmean(LH_k_reps &lt;= LH_k_obs)\n\n\n[1] 0.0415\n\n\n\n\n\n\n\n\nExercise 1: Exploring effects of the ROPE\n\n\n\n\n\nThe Bayesian \\(p\\)-value for a ROPE-d hypothesis is different from the previous \\(p\\)-value for the point-valued hypothesis. Set a narrower ROPE to sanity-check if we retrieve the same value with this latter code as well. Can you epxlain to yourself why the wider ROPE has the categorical (!?) effect of moving the estimated \\(p\\)-value below the “magical” (!?!) boundary of 0.05?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nValues of \\(\\theta\\) higher than 0.5 make the observed data ever more unlikely. They therefore contribute strongly to the result.\n\n\n\n\n\n\n\n\nWith BRMS\nWe can also implement the same sampling procedure with BRMS. For the example at hand, this is actually overkill (in fact, rather inefficient). Nevertheless, this simple example shows how the logic of \\(p\\)-value testing can be implemented on top of BRMS also for more complicated models\nFirst, we need to set up and obtain the model, from the prior point of view. Crucially, we need to use a binomial regression model, because we want to use the binomial distribution as the likelihood function. (This is because under \\(\\theta = 0.5\\) every sequence of fails and success of a fixed length is exactly equally likely. It is only when we look at more global properties of the sequence (like the number of heads) that we can get a grip on whether this aspect of the sequence is compatible with \\(\\theta = 0.5\\).)\n\n\nToggle code\ndata_24_7_binomial &lt;- \n  tibble(k = 7, N = 24)\n\nfit_logistic_prior &lt;- brms::brm(\n  formula = k | trials(N) ~ 1,\n  data = data_24_7_binomial,\n  family = binomial(link = \"logit\"),\n  # this is an extremely stupid \"unBayesian prior\" !!!\n  #   but it's the hypothesis we are interested in\n  prior = brms::prior(\"uniform(-0.04000533, 0.04000533)\", \n                      class = \"Intercept\", \n                      lb = -0.04000533, ub = 0.04000533),\n  sample_prior = \"only\",\n  iter = 5000,\n  warmup = 1000\n)\n\n\nWe then collect samples from the prior predictive, obtain the likelihood for all of these and compare these to the likelihood of the data, just like we did before, but now using BRMS for sampling \\(k_{reps}\\) and approximating their likelihood.\n\n\nToggle code\nn_samples_k         &lt;- 1e+04\nn_samples_LH_approx &lt;- 100\n\npriorPred_samples &lt;- tidybayes::add_predicted_draws(\n  fit_logistic_prior,\n  newdata = data_24_7_binomial |&gt; select(-k),\n  ndraws = n_samples_k,\n  value = \"k\"\n) |&gt; ungroup() |&gt; \n  select(.draw, k, N)\n\n# extract likelihood for observations (approximated w/ MC sampling)\nget_LH &lt;- function(k, N, ndraws = n_samples_LH_approx) {\n  brms::log_lik(\n    object  = fit_logistic_prior,\n    newdata = tibble(k = k, N = N),\n    ndraws  = ndraws) |&gt; \n    exp() |&gt; \n    mean()\n}\n\n# get likelihood of predictive samples\n# (this may take a while!!)\nLH_predictions &lt;- priorPred_samples |&gt; \n  group_by(.draw) |&gt; \n  summarize(LH_post_pred = get_LH(k, N)) |&gt; \n  pull(LH_post_pred)\n\n# get likelihood of data\nLH_data &lt;- get_LH(k = 7, N = 24, ndraws = 16000)\n\n# Bayesian $p$-values with LH as test statistic\nprint(mean(LH_predictions &lt;= LH_data))\n\n\n[1] 0.0401"
  },
  {
    "objectID": "practice-sheets/08-hypothesis-testing.html#bayes-factors",
    "href": "practice-sheets/08-hypothesis-testing.html#bayes-factors",
    "title": "Hypothesis testing",
    "section": "Bayes factors",
    "text": "Bayes factors\nFirst, we will explore comparison with Bayes Factors, in particular using the (generalized) Savage-Dickey method. Following this approach, we consider an encompassing model \\(M_e\\) which contains the (interval-based) null hypothesis \\(I_0\\) and its alternative \\(I_1\\) (in the sense that it puts positive probability on both). We can then compute the Bayes factor in favor of hypothesis \\(I_0\\) as:\n\\[\nBF_{01} = \\frac{\\text{posterior odds of $I_0$ in $M_e$}}{\\text{prior odds of $I_0$ in $M_e$}}\n\\]\nHere is how we can calculate this for the 24/7 data using the Beta-binomial model:\n\n\nToggle code\n# set the scene\nk &lt;- 7\nN &lt;- 24\ntheta_null &lt;- 0.5\nepsilon &lt;- 0.01                 # epsilon margin for ROPE\nupper &lt;- theta_null + epsilon   # upper bound of ROPE\nlower &lt;- theta_null - epsilon   # lower bound of ROPE\nalpha &lt;- 1                      # prior beta parameter\nbeta  &lt;- 1                      # prior beta parameter\n# calculate prior odds of the ROPE-d hypothesis\nprior_of_hypothesis &lt;- pbeta(upper, alpha, beta) - pbeta(lower, alpha, beta)\nprior_odds &lt;- prior_of_hypothesis / (1 - prior_of_hypothesis)\n# calculate posterior odds of the ROPE-d hypothesis\nposterior_of_hypothesis &lt;- pbeta(upper, alpha + k, beta + (N-k)) - pbeta(lower, alpha + k, beta + (N-k))\nposterior_odds &lt;- posterior_of_hypothesis / (1 - posterior_of_hypothesis)\n# calculate Bayes factor\nbf_ROPEd_hypothesis &lt;- posterior_odds / prior_odds\nbf_ROPEd_hypothesis\n\n\n[1] 0.5133012\n\n\nAnd here is how this can be calculated using a logistic regression instead. We first set up the model and fit it to the data. We use a normal prior for the Intercept, with values that very roughly induces a uniform distribution on the a priori predicted central tendency (= coin bias). (NB: the prior predictive of central tendency in this model is a logit-normal distribution.) We use a lot of samples to achieve better accuracy of our estimates.\n\n\nToggle code\nfit_logistic_posterior &lt;- brms::brm(\n  formula = outcome ~ 1, \n  data = data_24_7,\n  family = brms::bernoulli(link = \"logit\"),\n  prior = brms::prior(\"normal(0,1.8)\", \n                      class = \"Intercept\"),\n  iter = 50000,\n  warmup = 1000\n)\n\n\nWe then collect samples from the prior over parameters as well:\n\n\nToggle code\nfit_logistic_prior &lt;- stats::update(\n  fit_logistic_posterior, \n  sample_prior = \"only\",\n  iter = 50000,\n  warmup = 1000)\n\n\nLet’s do a quick sanity check and visualize the prior distribution of the predictor of central tendency (the coin’s bias) implied by our choice of prior on the intercept:\n\n\nToggle code\n# sanity check: prior distribution of central tendency\nfit_logistic_prior |&gt; \n  tidybayes::add_epred_draws(newdata = data_24_7) |&gt; \n  ggplot(aes(x = .epred)) + \n  geom_density()\n\n\n\n\n\nWe then approximate the probability of \\(I_0\\) and \\(I_1\\) from both the prior and posterior samples:\n\n\nToggle code\nprior_samples_Intercept &lt;- fit_logistic_prior |&gt; \n  tidybayes::tidy_draws()\n\nposterior_samples_Intercept &lt;- fit_logistic_posterior |&gt; \n  tidybayes::tidy_draws()\n\nP_I0_prior &lt;- mean(prior_samples_Intercept &gt;= lower & prior_samples_Intercept &lt;= upper )\nP_I1_prior &lt;- 1 - P_I0_prior\n\nP_I0_posterior &lt;- mean(posterior_samples_Intercept &gt;= lower & posterior_samples_Intercept &lt;= upper )\nP_I1_posterior &lt;- 1 - P_I0_posterior\n\n# Bayes factor\n\n(P_I0_posterior / P_I1_posterior) / (P_I0_prior / P_I1_prior)\n\n\n[1] 0.3651219"
  },
  {
    "objectID": "practice-sheets/08-hypothesis-testing.html#loo",
    "href": "practice-sheets/08-hypothesis-testing.html#loo",
    "title": "Hypothesis testing",
    "section": "LOO",
    "text": "LOO\n\n\nToggle code\nfit_logistic_alternative &lt;- brms::brm(\n  formula = outcome ~ 1, \n  data = data_24_7,\n  family = brms::bernoulli(link = \"logit\"),\n  prior = brms::prior(\"normal(0,1.8)\", \n                      class = \"Intercept\"),\n  iter = 50000,\n  warmup = 1000\n)\n\nfit_logistic_null &lt;- brms::brm(\n  formula = outcome ~ 1, \n  data = data_24_7,\n  family = brms::bernoulli(link = \"logit\"),\n  prior = brms::prior(\"uniform(0.49,0.51)\", \n                      class = \"Intercept\",\n                      lb=0.49, ub=0.51),\n  iter = 50000,\n  warmup = 1000\n)\n\n\n\n\nToggle code\nloo_comp &lt;- loo_compare(list(\n  alternative = loo(fit_logistic_alternative), \n  null = loo(fit_logistic_null)))\nloo_comp\n\n\n            elpd_diff se_diff\nalternative  0.0       0.0   \nnull        -4.4       3.2   \n\n\nIt seems that the alternative model is better.\nChecking whether this difference is substantial:\n\n\nToggle code\n1 - pnorm(-loo_comp[2,1], loo_comp[2,2])\n\n\n[1] 0.1192925"
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "",
    "text": "Generalized (non-)linear mixed effect models are a powerful statistical tool that gains increasing popularity for data analysis in cognitive science and many other disciplines. This tutorial will provide an overview of different categorical variable coding schemes commonly used in regression modeling. In particular, it covers:\nWe will look at two example data sets from factorial-design experiments with categorical predictors and a continuous dependent variable which we will analyze using a Bayesian approach. We also show how the faintr package allows extraction of (samples of estimates for) cell means irrespective of encoding scheme.\nEstimated reading time: 1.5h"
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#dataset",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#dataset",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Dataset",
    "text": "Dataset\nThe first part of this tutorial is based on a data set from an experiment by Winter and Grawunder (2012) You can get the dataset by running:\n\n\nToggle code\npoliteness_df &lt;- faintr::politeness\n\n# get a look at the data set\nhead(politeness_df)\n\n\n# A tibble: 6 × 5\n  subject gender sentence context pitch\n  &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt;\n1 F1      F      S1       pol      213.\n2 F1      F      S1       inf      204.\n3 F1      F      S2       pol      285.\n4 F1      F      S2       inf      260.\n5 F1      F      S3       pol      204.\n6 F1      F      S3       inf      287.\n\n\nThe data contains records of the voice pitch of speakers in different social contexts (polite and informal). They investigated whether the mean voice pitch differs across the factor gender of the speakers (F and M) and across the factor contexts - resulting in four different condition combinations (gender X context). Such a design is called factorial design and the single combinations are called design cells."
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#explore-data-visually",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#explore-data-visually",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Explore Data visually",
    "text": "Explore Data visually\nBefore we dive into any statistical analyses of our dataset it is helpful to get a rough idea of what the data looks like. For example, we can start by exploring the dataset visually.\n\n\n\n\n\nFurthermore, we can compute some basic statistics - e.g. the mean of the different design cells, before we turn to more complex linear models. We can also compute the overall mean pitch across all the conditions - the grand mean. These values will be helpful for a sanity check when interpreting the linear models later on.\n\n\nToggle code\ntibble_means &lt;- politeness_df %&gt;%\n  group_by(context, gender) %&gt;%\n  summarize(mean = mean(pitch))\nhead(tibble_means)\n\n\n# A tibble: 4 × 3\n# Groups:   context [2]\n  context gender  mean\n  &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;\n1 inf     F       261.\n2 inf     M       144.\n3 pol     F       233.\n4 pol     M       133.\n\n\nToggle code\nmean(tibble_means$mean)\n\n\n[1] 192.8605"
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#dummy-treatment-coding",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#dummy-treatment-coding",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Dummy (Treatment) Coding",
    "text": "Dummy (Treatment) Coding\nDummy coding, or treatment coding, is the default coding scheme used by R. Understanding the name ‘treatment coding’ helps understanding what this coding scheme does: imagine a medical experiment with a single control group (who obtain a placebo) and different experimental groups each of which gets a different treatment (e.g., different drugs), and where we want to compare each treatment group to the single, pivotal control group. Consequently, dummy coded variables are estimated by comparing all levels of the variable to a reference level. The intercept of a linear model containing dummy-coded variables is the mean of the reference level.\nOur variables only have two levels, so the effect of gender could be estimated by treating female as the reference level and estimating the effect of being male compared to the reference level – so basically estimating the difference in pitch it takes to “get from female to male”. Similarly, we can estimate the effect of context: the informal context can be treated as the reference level and the effect of politeness can be estimated against it. By default, the first level of a factor is treated as the reference level (for unordered factors that is the first string in alphanumeric order) - but principally, there is no difference as to which level should be used as the reference level. It makes sense to choose the level which is in some sense the ‘control’ in your experimental design.\nBecause R uses dummy-coding by default, we can look at the default numerical coding right away. The function contrasts() displays the contrast matrix for the respective variable:\n\n\nToggle code\ncontrasts(politeness_df$gender)\n\n\n  M\nF 0\nM 1\n\n\nToggle code\ncontrasts(politeness_df$context)\n\n\n    pol\ninf   0\npol   1\n\n\nBut if we wish to explicitly assign a dummy (treatment) coding to a variable, we may do so by a built-in R function.\n\n\nToggle code\ncontrasts(politeness_df$gender) &lt;- contr.treatment(n=2) # insert the number of levels here\ncolnames(contrasts(politeness_df$gender)) &lt;- \"M\" # manually declare the contrast level names\n\n\nSo both variables \\(x_1\\) and \\(x_2\\) can take either the value 0 or 1 (because we dummy-code both categorical variables; see below for more). We already defined the referenc levels of the single variables, now we can define the overall reference level of our model (by combining the two individual reference levels) – it is the mean pitch of female speakers in informal contexts.\nHaving set all the basics, we can now turn to computing a linear model of the mean pitch as predicted by the factors gender and context:\n\n\nToggle code\n# here, we only use fixed effects\nfit_dummy_FE &lt;- brm(\n  pitch ~ gender * context,\n  data = politeness_df,\n  cores = 4,\n  iter = 1000\n)\n\n\n\n\nToggle code\nfit_dummy_FE.coefs &lt;- fixef(fit_dummy_FE)[,1] %&gt;% as.numeric() # get the estimated coefficients\nsummary(fit_dummy_FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ gender * context \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            260.81      7.83   244.99   275.32 1.00     1022     1339\ngenderM             -116.33     11.01  -136.43   -94.23 1.00      848     1299\ncontextpol           -27.70     11.32   -49.81    -5.14 1.00     1152     1305\ngenderM:contextpol    16.37     15.97   -15.76    47.01 1.00      973     1158\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.18      2.88    30.96    42.25 1.00     1925     1391\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow how do we interpret the estimated coefficients?\nLet us recall the regression equation that is hidden behind this output: \\[y = \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 + \\beta_3*x_1x_2\\]\nIn order to help us interpret the output, R assigns string names to the estimated coefficients using the names we used in the generic formula. The (Intercept) corresponds to \\(\\beta_0\\), genderM corresponds to \\(\\beta_1\\), contextpol corresponds to \\(\\beta_2\\) and genderM:contextpol (the interaction term) to \\(\\beta_3\\).\nFurther, let us recall the numerical coding of our variables: for \\(x_1\\) (gender) a 0 means female, a 1 means male; for \\(x_2\\) (context) a 0 means informal, a 1 means polite. So the computed values are the estimates for conditions differing from the respective reference conditions - i.e. when the respective \\(x\\) is a 1.\nTo get an estimate of a certain design cell (\\(y_i\\)) - let’s start with the mean pitch of female speakers (0 for \\(x_1\\)) in informal contexts (0 for \\(x_2\\)) - we just insert the corresponding numeric values for the corresponding \\(x\\) and the estimated value for the corresponding \\(\\beta\\). Thus we get:\n\n\nToggle code\ny1 = fit_dummy_FE.coefs[1] + fit_dummy_FE.coefs[2]*0 +\n  fit_dummy_FE.coefs[3]*0 + fit_dummy_FE.coefs[4]*(0)\ny1\n\n\n[1] 260.813\n\n\nHence, the mean pitch of female speakers in informal context corresponds to the intercept. As a sanity check, we can recall that for dummy coded variables the model intercept is just the mean of the reference cell (in our case, female speakers in informal contexts!).\nLet’s now calculate the mean pitch of male speakers (1 for \\(x_1\\)) in informal contexts (0 for \\(x_2\\)):\n\n\nToggle code\ny2 = fit_dummy_FE.coefs[1] + fit_dummy_FE.coefs[2]*1 +\n  fit_dummy_FE.coefs[3]*0 + fit_dummy_FE.coefs[4]*(1*0)\ny2\n\n\n[1] 144.4836\n\n\n\nQuantifying uncertainty\nThe previous estimate of the cell mean used the mean estimate for model paramters. This way, we get no information about the uncertainty in this estimate. But we can do the same kind of calculations we did before for the means of the estimates, but for each of the samples from the posterior, thus obtaining samples from a derived variable, for which we can quantify the uncertainty as usual.\n\n\nToggle code\ncell_means_BayesStats &lt;- fit_dummy_FE |&gt; \n  tidybayes::tidy_draws() |&gt; \n  select(starts_with(\"b_\")) |&gt; \n  mutate(\n    female_informal = b_Intercept,\n    female_polite   = b_Intercept + b_contextpol,\n    male_informal   = b_Intercept + b_genderM,\n    male_polite     = b_Intercept + b_contextpol + \n                        b_genderM + `b_genderM:contextpol`\n  ) |&gt; \n  select(5:8) |&gt; \n  pivot_longer(cols = everything()) |&gt;\n  group_by(name) |&gt; \n  reframe(aida::summarize_sample_vector(value)[-1])\ncell_means_BayesStats\n\n\n# A tibble: 4 × 4\n  name            `|95%`  mean `95%|`\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 female_informal   246.  261.   276.\n2 female_polite     217.  233.   249.\n3 male_informal     131.  144.   162.\n4 male_polite       118.  133.   150.\n\n\nThat’s a lot of manual labor. Thankfully, there is a package to make this simpler.\n\n\nExtracting cell samples with faintr\nThe faintr package allows to extract samples from different levels of categorical predictors, like so:\n\n\nToggle code\n# sample for male speakers in polite contexts\nfaintr::filter_cell_draws(\n  fit = fit_dummy_FE,\n  group = gender == \"M\" & context == \"pol\"\n) |&gt; pull(draws) |&gt; \n  aida::summarize_sample_vector(name = \"male_polite\")\n\n\n# A tibble: 1 × 4\n  Parameter   `|95%`  mean `95%|`\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 male_polite   118.  133.   150.\n\n\nThe faintr package also allows to compare cells (e.g., “diagonally”):\n\n\nToggle code\nfaintr::compare_groups(\n  fit = fit_dummy_FE,\n  higher  = gender == \"F\" & context == \"inf\",\n  lower   = gender == \"M\" & context == \"pol\"\n)\n\n\nOutcome of comparing groups: \n * higher:  gender == \"F\" & context == \"inf\" \n * lower:   gender == \"M\" & context == \"pol\" \nMean 'higher - lower':  127.7 \n95% HDI:  [ 107.8 ; 150.6 ]\nP('higher - lower' &gt; 0):  1 \nPosterior odds:  Inf \n\n\nThis compares the estimated mean pitch for male speakers in polite contexts against those of female speakers in informal contexts. Clearly, the hypothesis that polite male speakers have lower pitch than informal female speakers is supported by this data and analysis."
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#simple-contrast-coding",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#simple-contrast-coding",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Simple (Contrast) Coding",
    "text": "Simple (Contrast) Coding\nAnother common coding scheme is the simple coding (also called contrast coding). Simple coded variables are also compared to a reference level (just like dummy-coded ones). However, the intercept of a simple coded model is the grand mean – the mean of all cells (i.e. the mean of female-informal & female-polite & male-informal & male-polite cells).\nGenerally, this kind of coding can be created by subtracting \\(1/k\\) from the dummy coding contrast matrix, where \\(k\\) is the number of levels a variable has (in our case, both have two). Hence, the reference level will always only have negative values in the contrast matrix. The general rule is that the contrasts within a column have to add up to 0. R does not provide a built-in function for simple coding, but we can easily create the respective matrix ourselves by subtracting \\(1/k\\) (i.e. 1/2) from the dummy coding matrix:\n\n\nToggle code\n# manual creation of contrasts\ncontr.matrix &lt;- matrix( rep(0.5, 2))\ndummy.matrix &lt;- contr.treatment(2)\ncontr.coding &lt;- dummy.matrix - contr.matrix\n\n# we should duplicate the values to not overwrite previous contrasts\npoliteness_df &lt;- politeness_df %&gt;%\n  mutate(context_contr = context,\n         gender_contr = gender)\ncontrasts(politeness_df$context_contr) &lt;- contr.coding\ncontrasts(politeness_df$gender_contr)  &lt;- contr.coding\n\n\nHence now the gender is coded as -0.5 for female and 0.5 for male; context is coded as -0.5 for informal and 0.5 for polite.\nLet’s again look at our regression model:\n\n\nToggle code\nlm.contr.FE &lt;- brm(\n  pitch ~ gender_contr * context_contr,\n  data = politeness_df,\n  cores = 4,\n  iter =  1000\n)\n\n\n\n\nToggle code\nlm.contr.FE.coefs &lt;- fixef(lm.contr.FE)[,1] %&gt;% as.numeric() # get vector of estimated coefficients\nsummary(lm.contr.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ gender_contr * context_contr \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                      193.09      4.11   184.91   201.04 1.00     2572\ngender_contr2                 -107.93      7.92  -124.24   -92.83 1.00     2320\ncontext_contr2                 -19.54      7.83   -35.45    -5.30 1.00     2488\ngender_contr2:context_contr2    16.08     16.42   -14.88    48.77 1.00     2374\n                             Tail_ESS\nIntercept                        1459\ngender_contr2                    1551\ncontext_contr2                   1377\ngender_contr2:context_contr2     1525\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.23      2.95    31.13    42.53 1.00     2039     1547\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn order to compute the mean pitch of a specific cell, we proceed just as with dummy-coded variables and insert the respective estimates and values for \\(x\\). Let us start with female speakers (\\(x_1\\) is -0.5) in informal contexts (\\(x_2\\) is -0.5):\n\n\nToggle code\ny1 = lm.contr.FE.coefs[1] + lm.contr.FE.coefs[2]*(-0.5) + lm.contr.FE.coefs[3]*(-0.5) + lm.contr.FE.coefs[4]*(-0.5)*(-0.5)\ny1\n\n\n[1] 260.8414\n\n\nWe get the same result as before (as we should - the estimates should not depend on a coding scheme but only on the data). As a sanity check, we can again look at the intercept – it matches the grand mean we computed in the beginning of this tutorial – as it should."
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#deviation-sum-coding",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#deviation-sum-coding",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Deviation (Sum) Coding",
    "text": "Deviation (Sum) Coding\nDeviation coding (also called sum coding) is the most popular coding scheme and is often considered the best choice to get a clear picture of presence (or absence) of an effect and a clear random effects interpretation.\nIt is slightly different from the previous schemes. It compares the mean of the predicted variable for a specific condition to the grand mean. So the estimates do not tell you the difference between the reference level and another level anymore. The intercept of linear models with sum coded variables is the grand mean.\nR has a built-in function for creating sum coded variables:\n\n\nToggle code\n# again create a new variable\npoliteness_df %&gt;%\n  mutate(context_dev = context,\n         gender_dev = gender) -&gt; politeness_df\ncontrasts(politeness_df$context_dev) &lt;- contr.sum(2) # insert number of levels\ncontrasts(politeness_df$gender_dev)  &lt;- contr.sum(2)\n\n\nNow the gender is coded as s 1 for female and -1 for male; context is coded as 1 for informal and -1 for polite.\nBelow we fit a model with the sum-coded variables:\n\n\nToggle code\nlm.dev.FE &lt;- brm(pitch ~ context_dev * gender_dev,\n                data = politeness_df,\n                cores = 4,\n                iter = 1000)\n\n\n\n\nToggle code\nlm.dev.FE.coefs &lt;- fixef(lm.dev.FE)[,1] %&gt;% as.numeric()\nsummary(lm.dev.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ context_dev * gender_dev \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                  192.74      3.88   185.26   200.28 1.00     1943\ncontext_dev1                 9.81      4.06     1.84    17.79 1.00     2261\ngender_dev1                 54.15      4.03    46.54    61.79 1.00     2365\ncontext_dev1:gender_dev1     3.91      3.93    -3.88    11.39 1.00     1998\n                         Tail_ESS\nIntercept                    1389\ncontext_dev1                 1592\ngender_dev1                  1465\ncontext_dev1:gender_dev1     1406\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.23      3.04    31.11    43.13 1.00     2065     1452\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe coefficients denote now the difference between the grand mean (i.e. intercept) and the mean of the respective condition.\nWe apply the same idea to estimate the pitch means for specific cases: E.g. for female speakers in informal contexts we do:\n\n\nToggle code\ny1 = lm.dev.FE.coefs[1] + lm.dev.FE.coefs[2]*1 + lm.dev.FE.coefs[3]*1 + lm.dev.FE.coefs[4]*1*1\ny1\n\n\n[1] 260.6034\n\n\nSince the intercept is now the grand mean and not a specific reference level, let us think about the interpretation of the single estimates. The estimate of e.g. the context effect now denotes the value by which the mean pitch in informal (estimate * 1, remember our coding!) or polite contexts (estimate * -1) differs from the grand mean. So if we wish to calculate the mean pitch in polite contexts (across genders), we would do:\n\n\nToggle code\nyPol = lm.dev.FE.coefs[1] + lm.dev.FE.coefs[2] * (-1)\nyPol\n\n\n[1] 182.9254\n\n\nThis means that the single estimates are in some sense ‘independent’ of each other (in contrast to e.g. dummy-coded variables where the estimates are bound to the reference levels of two variables) and give us insight if a specific factor is credibly different from 0. Similarly, if we wish to calculate the mean pitch of male speakers, we would calculate:\n\n\nToggle code\nyM = lm.dev.FE.coefs[1] + lm.dev.FE.coefs[3] * (-1)\nyM\n\n\n[1] 138.5881"
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#helmert-coding",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#helmert-coding",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Helmert Coding",
    "text": "Helmert Coding\nIn this coding scheme, a level of a variable is compared to its subsequent levels. In our dataset, e.g. for gender the level female is compared to the subsequent level male.\nGenerally, to create such a coding, in order to compare the first level to the subsequent levels you would assign \\((k-1)/k\\) to the first level and \\(-1/k\\) to all subsequent levels where \\(k\\) is the total number of levels. To compare the second level to the subsequent levels you would assign 0 to the first level, \\((i-1)/i\\) to the second level and \\(-i/1\\) to all subsequent levels where \\(i = k-1\\) and so on. The difference of this coding scheme to previous ones is more clear for variales with &gt;2 levels (see below). The intercept of a linear model corresponds to the grand mean.\nR does not have a built-in function for standard Helmert coding, so we do it manually:\n\n\nToggle code\n# with politeness data\nhelm.matrix &lt;- matrix(c(0.5, -0.5))\npoliteness_df &lt;-\n politeness_df %&gt;%\n mutate(gender_helm = gender,\n         context_helm = context)\ncontrasts(politeness_df$gender_helm)  &lt;- helm.matrix\ncontrasts(politeness_df$context_helm) &lt;- helm.matrix\n\n\nThe linear model looks like this:\n\n\nToggle code\nlm.helmert.FE &lt;- brm(pitch ~ context_helm * gender_helm,\n                data = politeness_df,\n                cores = 4,\n                iter = 1000)\n\n\n\n\nToggle code\nlm.helmert.FE.coefs &lt;- fixef(lm.helmert.FE)[,1] %&gt;% as.numeric()\nsummary(lm.helmert.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: pitch ~ context_helm * gender_helm \n   Data: politeness_df (Number of observations: 83) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                    192.86      3.81   185.62   200.43 1.00     2265\ncontext_helm1                 19.70      8.13     3.94    36.21 1.00     2660\ngender_helm1                 108.34      8.41    92.06   124.66 1.00     2456\ncontext_helm1:gender_helm1    15.51     16.31   -16.16    47.14 1.01     2903\n                           Tail_ESS\nIntercept                      1524\ncontext_helm1                  1209\ngender_helm1                   1454\ncontext_helm1:gender_helm1     1677\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    36.18      2.90    30.98    42.26 1.00     2041     1382\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe contrast estimate for the first level and the remaining levels is calculated by taking the mean of the dependent variable for the first level and subtracting the mean of the dependent variable for the remaining levels (in our case, just the mean of the second level). In other words, if we look at the context coefficient it denotes the difference between the mean of the polite and informal context means."
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#mixing-coding-schemes",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#mixing-coding-schemes",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Mixing Coding Schemes",
    "text": "Mixing Coding Schemes\nIf you have several categorical predictor variables, it is also possible (and often useful!) to use different coding schemes for the different variables. It might, for example, make sense to use dummy coding for a variable which has a control and a treatment condition, and to use e.g. simple coding for a variable which has two ‘equal’ levels.\nFor example, we could use dummy-coding for context and simple-coding for gender.\nWhen you mix coding schemes or define your own schemes there might be no pre-defined answers to questions as to what the sigle coefficients or the intercept mean. But knowing how the different schemes work, you can easily find this out!\nLet us explore how the interpretation of the model changes if we mix coding schemes:\n\n\nToggle code\nlm.mixedCode.FE &lt;- brm(pitch ~ context * gender_contr,\n                data = politeness_df,\n                cores = 4,\n                iter = 1000)\n\n\n{lm.mixedCode.FE.coefs &lt;- fixef(lm.mixedCode.FE)[,1] %&gt;% as.numeric()} summary(lm.mixedCode.FE)\nGenerally, the interpretation is just the combination of what we have learned about the individual coding schemes. Recall that the intercept of a dummy-coded model is the mean of the reference level – since we dummy-coded context, the refernce level would be informal context. But it is not the intercept yet! We have the second predictor in our model – the simple coded gender. In simple coded models the intercept is the mean across the levels of the variable. Now the intercept of our model with the two different predictors is the mean pitch in informal contexts - across genders.\nFollowing this logic, the context estimate denotes the difference between the informal and polite contexts - still across genders. The gender estimate denoted the difference between the mean pitch and female speakers if multiplied by the value -0.5 (recall our coding above); and the mean pitch and male speakers if multiplied by the value 0.5."
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#dummy-treatment-coding-1",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#dummy-treatment-coding-1",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Dummy (Treatment) Coding",
    "text": "Dummy (Treatment) Coding\nGenerally, the coding schemes work in the very same way independently of the number of levels. The only difference to our old data set is that now we need two numeric variables coding the contrasts between the levels of one variable. If we look at the default (dummy) coding of e.g. the variable List we see a contrast matrix with two columns, each denoting the comparisons between two levels:\n\n\nToggle code\ncontrasts(latinsquare$SOA)\n\n\n       medium short\nlong        0     0\nmedium      1     0\nshort       0     1\n\n\nToggle code\ncontrasts(latinsquare_df$List)\n\n\n   L2 L3\nL1  0  0\nL2  1  0\nL3  0  1\n\n\nSo now the recoding of the categorical variable takes two numeric variables: e.g. \\(x_{1_2}\\) and \\(x_{1_3}\\), where both can take the values 0 or 1; the single levels are denoted by the combination of the two numeric variables. Again there is a reference level - List 1 - described by \\(x_{1_2}\\) and \\(x_{1_3}\\) being 0. \\(x_{1_2}\\) being 1 describes the difference between the reference level and List 2; \\(x_{1_3}\\) being 1 describes the difference between the reference level and List 3. Correspondingly, there is and individual \\(\\beta\\) for each numeric variable estimated in the regression model. The coding of the SOA factor works just the same way. The interactions between specific levels are described by combining the respective numeric variables \\(x\\). So the model we are fitting is described by:\n\\[y = \\beta_0 + \\beta_1 * x_{1_2} + \\beta_2 * x_{1_3} + \\beta_3 * x_{2_2} + \\beta_4 * x{2_3} + \\beta_5 * x_{1_2}x_{2_2} + \\beta_6 * x_{1_3}x_{2_2} + \\beta_7 * x_{1_2}x_{2_3}  + \\beta8 * x_{1_3}x_{2_3}\\]\n\n\nToggle code\nlm3.dummy.FE &lt;- brm(RT ~ List * SOA,\n                   data = latinsquare_df,\n                   cores = 4,\n                   iter = 1000)\n\n\n\n\nToggle code\nlm3.dummy.FE.coefs &lt;- fixef(lm3.dummy.FE)[,1] %&gt;% as.numeric()\nsummary(lm3.dummy.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List * SOA \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          529.34     11.81   505.64   552.48 1.00     1008     1382\nListL2              13.79     16.45   -17.58    45.33 1.00      955     1365\nListL3               0.30     16.03   -31.48    33.13 1.00      994     1259\nSOAmedium            7.98     16.85   -25.72    40.07 1.00     1003     1303\nSOAshort            14.53     16.68   -17.73    47.60 1.00     1014     1494\nListL2:SOAmedium     5.31     23.65   -40.26    53.19 1.00      989     1346\nListL3:SOAmedium   -22.84     23.97   -68.66    23.67 1.00      966     1295\nListL2:SOAshort    -22.92     23.78   -68.38    26.16 1.00     1051     1299\nListL3:SOAshort    -21.76     22.64   -66.19    21.54 1.00     1147     1467\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.55      2.87    41.13    52.68 1.00     1786     1333\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSince both predictors are dummy-coded the intercept represents the reference level - the mean RT for List 1 and a long SOA. Following the same procedure as for the two-level variables you could calculate the estimated mean RTs for specific conditions."
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#simple-contrast-coding-1",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#simple-contrast-coding-1",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Simple (Contrast) Coding",
    "text": "Simple (Contrast) Coding\nSimple coding only slightly differs from dummy-coding – the intercept of the model is the grand mean, not the mean RT of the reference level. Otherwise, the coefficients still denote the difference between the reference level and other specific levels.\n\n\nToggle code\nlatinsquare_df %&gt;%\n  mutate(List_contr = factor(List),\n         SOA_contr = factor(SOA)) -&gt; latinsquare_df\ndummy.matrix3 &lt;- contr.treatment(3)\ncontr.matrix3 &lt;- matrix(c(1/3, 1/3, 1/3, 1/3, 1/3, 1/3), ncol=2)\ncontrasts(latinsquare_df$List_contr) &lt;- dummy.matrix3 - contr.matrix3\ncontrasts(latinsquare_df$SOA_contr) &lt;-  dummy.matrix3 - contr.matrix3\n\n\n\n\nToggle code\nlm3.simple.FE &lt;- brm(RT ~ List_contr * SOA_contr,\n                   data = latinsquare_df,\n                   cores = 4,\n                   iter = 1000)\n\n\n\n\nToggle code\nlm3.simple.FE.coefs &lt;- fixef(lm3.simple.FE)[,1] %&gt;% as.numeric()\nsummary(lm3.simple.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_contr * SOA_contr \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                534.56      3.85   527.01   542.21 1.01     2362\nList_contr2                7.77      9.51   -10.54    26.84 1.00     1928\nList_contr3              -14.94      9.51   -33.16     3.16 1.00     1810\nSOA_contr2                 2.25      9.57   -16.42    21.16 1.00     1857\nSOA_contr3                -0.56      9.46   -19.05    18.01 1.00     1560\nList_contr2:SOA_contr2     5.71     23.34   -39.48    51.40 1.00     1782\nList_contr3:SOA_contr2   -21.49     22.78   -65.71    24.11 1.00     1527\nList_contr2:SOA_contr3   -22.86     22.53   -66.72    21.22 1.00     1503\nList_contr3:SOA_contr3   -20.99     23.30   -66.28    24.42 1.00     1543\n                       Tail_ESS\nIntercept                  1726\nList_contr2                1605\nList_contr3                1324\nSOA_contr2                 1479\nSOA_contr3                 1364\nList_contr2:SOA_contr2     1440\nList_contr3:SOA_contr2     1531\nList_contr2:SOA_contr3     1492\nList_contr3:SOA_contr3     1512\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.52      2.86    41.59    52.47 1.00     2349     1612\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#deviation-sum-coding-1",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#deviation-sum-coding-1",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Deviation (Sum) Coding",
    "text": "Deviation (Sum) Coding\nWith increasing number of levels within the factors the complexity and messiness of interpreting the differences between levels against each other increases considerably. Hence it might make a lot of sense to use the deviation coding scheme which provides estimates of effects comapred to the grand mean. We again can use the R built-in function to assign deviation coding to our three-level factors:\n\n\nToggle code\nlatinsquare_df %&gt;%\n  mutate(List_dev = List,\n         SOA_dev = SOA) -&gt; latinsquare_df\ncontrasts(latinsquare_df$List_dev) &lt;- contr.sum(3) # insert number of levels\ncontrasts(latinsquare_df$SOA_dev) &lt;- contr.sum(3)\n\n\nFor e.g. SOA our numeric variables now denote the effect of long SOA compared to the grand mean when \\(x_{2_2}\\) is a 1 and \\(x_{2_3}\\) is a 0; they denote the effect of medium SOA compared to the grand mean when \\(x_{2_2}\\) is a 0 and \\(x_{2_3}\\) is a 1; the effect of short SOA is never compared to the grand mean since it is always assigned a -1.\n\n\nToggle code\nlm3.dev.FE &lt;- brm(RT ~ List_dev * SOA_dev,\n                   data = latinsquare_df,\n                 cores = 4,\n                 iter = 1000)\n\n\n\n\nToggle code\nlm3.dev.FE.coefs &lt;- fixef(lm3.dev.FE)[,1] %&gt;% as.numeric()\nsummary(lm3.dev.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_dev * SOA_dev \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            534.61      4.04   526.71   542.65 1.00     2620     1388\nList_dev1              2.42      5.35    -7.87    12.15 1.00     2102     1545\nList_dev2              9.96      5.48    -0.20    20.49 1.00     1992     1675\nSOA_dev1              -0.48      5.56   -11.13    10.37 1.01     2220     1591\nSOA_dev2               1.66      5.36    -8.95    12.07 1.00     2263     1512\nList_dev1:SOA_dev1    -6.75      7.92   -22.09     8.98 1.00     1621     1807\nList_dev2:SOA_dev1    -0.92      7.77   -15.70    14.74 1.00     1683     1756\nList_dev1:SOA_dev2    -1.34      7.84   -17.22    13.60 1.00     1743     1560\nList_dev2:SOA_dev2     9.97      7.99    -5.72    25.72 1.00     1750     1436\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.54      2.92    41.31    52.46 1.00     2635     1458\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThink about what the single estimates mean!"
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#helmert-coding-1",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#helmert-coding-1",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Helmert Coding",
    "text": "Helmert Coding\nFor recap: In this coding scheme, a level of a variable is compared to its subsequent levels. What does this mean for the three-level factors?\n\n\nToggle code\nlatinsquare_df %&gt;%\n  mutate(List_helm = List,\n         SOA_helm = SOA) -&gt; latinsquare_df\nhelm.matrix3 &lt;- matrix(c(2/3, -1/3, -1/3, 0, 1/2, -1/2 ), ncol = 2)\ncontrasts(latinsquare_df$List_helm) &lt;- helm.matrix3\ncontrasts(latinsquare_df$SOA_helm) &lt;- helm.matrix3\n\n\n\n\nToggle code\nlm3.helm.FE &lt;- brm(RT ~ List_helm * SOA_helm,\n                   data = latinsquare_df,\n                  cores = 4,\n                  iter = 1000)\n\n\n\n\nToggle code\nlm3.helm.FE.coefs &lt;- fixef(lm3.helm.FE)[,1] %&gt;% as.numeric()\nsummary(lm3.helm.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_helm * SOA_helm \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept              534.48      3.90   526.51   542.22 1.00     3801\nList_helm1               3.51      8.17   -12.50    20.55 1.00     4107\nList_helm2              22.65      9.76     4.12    41.27 1.01     4547\nSOA_helm1               -0.98      8.15   -17.48    14.47 1.00     4097\nSOA_helm2                2.59      9.60   -16.36    21.25 1.00     4142\nList_helm1:SOA_helm1   -15.29     17.87   -49.67    20.80 1.01     4262\nList_helm2:SOA_helm1   -13.06     20.69   -52.29    28.56 1.00     4177\nList_helm1:SOA_helm2   -14.19     19.95   -51.78    25.77 1.01     3204\nList_helm2:SOA_helm2    29.13     22.87   -17.47    72.36 1.01     3982\n                     Tail_ESS\nIntercept                1595\nList_helm1               1454\nList_helm2               1313\nSOA_helm1                1459\nSOA_helm2                1469\nList_helm1:SOA_helm1     1479\nList_helm2:SOA_helm1     1332\nList_helm1:SOA_helm2     1320\nList_helm2:SOA_helm2     1459\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.57      2.88    41.30    52.93 1.00     3245      808\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe main effect estimates denote the differences between the mean of List1 and the mean of (List2 + List3); and between the mean of List2 and the mean of List3. Respectively, they denote the differences between the mean of SOA long and the mean of (medium + short); and between the mean of SOA medium and the mean of short. The intercept is the grand mean."
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#reverse-helmert-coding",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#reverse-helmert-coding",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Reverse Helmert Coding",
    "text": "Reverse Helmert Coding\nThe reverse Helmert coding scheme (also called difference coding) is quite similar to the Helmert coding, but compares the mean of a level to its previous levels. Since we basically reverse the coding we used in the previous scheme, we also ‘reverse’ the contrast matrix to create such a coding.\n\n\nToggle code\nlatinsquare_df %&gt;%\n  mutate(List_rhelm = List,\n         SOA_rhelm = SOA) -&gt; latinsquare_df\nrhelm.matrix3 &lt;- matrix(c(-1/2, 1/2, 0, -1/3, -1/3, 2/3 ), ncol = 2)\ncontrasts(latinsquare_df$List_rhelm) &lt;- rhelm.matrix3\ncontrasts(latinsquare_df$SOA_rhelm) &lt;- rhelm.matrix3\n\n\n\n\nToggle code\nlm3.rhelm.FE &lt;- brm(RT ~ List_rhelm * SOA_rhelm,\n                   data = latinsquare_df,\n                   iter = 1000,\n                   cores = 4)\n\n\n\n\nToggle code\nlm3.rhelm.FE.coefs &lt;- fixef(lm3.rhelm.FE)[,1] %&gt;% as.numeric()\nsummary(lm3.rhelm.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List_rhelm * SOA_rhelm \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                534.50      4.03   526.54   542.08 1.00     4563\nList_rhelm1                7.63      9.60   -10.33    26.07 1.00     3499\nList_rhelm2              -18.66      8.38   -35.67    -2.23 1.00     3841\nSOA_rhelm1                 1.99      9.44   -16.06    20.96 1.01     3923\nSOA_rhelm2                -1.23      8.07   -17.52    14.92 1.00     3847\nList_rhelm1:SOA_rhelm1     5.96     23.19   -38.39    51.28 1.00     3610\nList_rhelm2:SOA_rhelm1   -24.02     20.04   -61.55    15.31 1.00     3497\nList_rhelm1:SOA_rhelm2   -26.34     19.53   -63.91    11.30 1.00     3498\nList_rhelm2:SOA_rhelm2     2.13     17.90   -32.87    36.77 1.00     3967\n                       Tail_ESS\nIntercept                  1530\nList_rhelm1                1549\nList_rhelm2                1374\nSOA_rhelm1                 1271\nSOA_rhelm2                 1423\nList_rhelm1:SOA_rhelm1     1652\nList_rhelm2:SOA_rhelm1     1617\nList_rhelm1:SOA_rhelm2     1311\nList_rhelm2:SOA_rhelm2     1417\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.61      2.97    41.24    52.76 1.00     2664     1499\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn our example, the first estimate denotes the difference between the mean of List2 and the mean of List1; the second - the difference between the mean of List3 and the mean of (List1 + List2). The main effects of SOA can be interpreted similarly."
  },
  {
    "objectID": "practice-sheets/02a-contrast-coding-tutorial.html#mixed-schemes-dummy-and-deviation-coding",
    "href": "practice-sheets/02a-contrast-coding-tutorial.html#mixed-schemes-dummy-and-deviation-coding",
    "title": "A tutorial on contrast coding for (Bayesian) regression",
    "section": "Mixed Schemes: Dummy and Deviation Coding",
    "text": "Mixed Schemes: Dummy and Deviation Coding\nJust like with two-level factors, we might wish to use different coding schemes for different predictors. It might make sense to use dummy coding for a variable which has a control and two different treatment conditions, and to use deviation coding for a variable which has ‘equal’ levels.\nFor example, we could use dummy-coding for List and deviation-coding for SOA.\n\n\nToggle code\nlm3.mixedCode.FE &lt;- brm(RT ~ List * SOA_dev,\n                   data = latinsquare_df,\n                   cores = 4,\n                   iter = 1000)\nlm3.mixedCode.FE.coefs &lt;- fixef(lm3.mixedCode.FE)[,1] %&gt;% as.numeric()\nsummary(lm3.mixedCode.FE)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ List * SOA_dev \n   Data: latinsquare_df (Number of observations: 144) \n  Draws: 4 chains, each with iter = 1000; warmup = 500; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         536.89      6.59   524.34   550.00 1.00     1688     1306\nListL2              7.89      9.41   -10.23    25.73 1.00     1796     1467\nListL3            -14.61      9.67   -34.32     4.37 1.00     1988     1684\nSOA_dev1           -6.86      9.47   -26.30    10.98 1.00     1061     1059\nSOA_dev2           -0.48      9.49   -19.64    18.25 1.00     1045     1235\nListL2:SOA_dev1     5.34     13.31   -20.67    31.89 1.00     1196     1448\nListL3:SOA_dev1    14.01     13.45   -13.15    40.43 1.00     1256     1556\nListL2:SOA_dev2    12.66     13.67   -15.22    39.27 1.00     1238     1351\nListL3:SOA_dev2    -6.36     13.51   -32.69    19.72 1.00     1138     1624\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    46.61      2.83    41.35    53.00 1.00     2045     1473\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nTowards the end of this tutorial, the main take-away is this: when you look at the estimates of (any) model, you could ask yourself a couple of questions like these to make sure you understand what was calculated:\n\nWhat does the intercept represent?\nWhat do the single estimates mean?\nWhat do they tell me about my hypotheses?\n\nOf course, you will also encounter experimental designs which use a two-level and a three-level categorical predictors – but the conceptual basics regarding how to choose the contrasts and how to interpret linear models are the same."
  },
  {
    "objectID": "practice-sheets/01a-regression-WebPPL.html",
    "href": "practice-sheets/01a-regression-WebPPL.html",
    "title": "Bayesian regression models in WebPPL",
    "section": "",
    "text": "Preamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nSimple regression\nLet’s look at R’s built-in data set cars for simple example of two paired metric measurements. Here, we have the speed a car was traveling and the distance before coming to a halt (from the 1920s):\n\n\nToggle code\ncars |&gt; \n  ggplot(aes(x = speed, y = dist)) +\n  geom_point(alpha = 0.8) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\nHere is an example of a simple Bayesian regression model for this data set in WebPPL.\n// R's 'cars' data set: 50 paired measurements of speed and distance (after breaking)\nvar speed = [4, 4, 7, 7, 8, 9, 10, 10, \n             10, 11, 11, 12, 12, 12, \n             12, 13, 13, 13, 13, 14, \n             14, 14, 14, 15, 15, 15, \n             16, 16, 17, 17, 17, 18, \n             18, 18, 18, 19, 19, 19, \n             20, 20, 20, 20, 20, 22, \n             23, 24, 24, 24, 24, 25]\nvar dist = [2, 10, 4, 22, 16, 10, 18, \n            26, 34, 17, 28, 14, 20, 24, \n            28, 26, 34, 34, 46, 26, 36, \n            60, 80, 20, 26, 54, 32, 40, \n            32, 40, 50, 42, 56, 76, 84, \n            36, 46, 68, 32, 48, 52, 56, \n            64, 66, 54, 70, 92, 93, 120, 85]\n\nvar model = function() {\n  \n  // parameters & priors\n  var Intercept = gaussian(-18, 5); // data-informed choice (!)\n  var slope = gaussian(0, 10);\n  var sigma = gamma(5, 5);\n\n  // linear predictor value for a given 'x' value\n  var linPred = function(x) {\n    return slope * x + Intercept;\n  };\n\n  map2(\n    function(x, y) {\n      factor(Gaussian({mu: linPred(x), sigma: sigma}).score(y));\n    },\n    speed,\n    dist);\n\n  return {Intercept,slope, sigma};\n}\n\nviz.marginals(Infer({method: 'MCMC', samples: 20000, burn: 10000, lag: 2}, model));\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\nTry to see the underlying model of the data-generating process behind the code.\nVisualize just the priors and compare them with the posteriors.\nWhy is it (usually!) okay to glimpse at the data to set priors for the Intercept?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUsually, it is the slope coefficients that capture the research question of interest. We should therefore (usually) be more mindful about setting the priors for the slope coefficients.\n\n\n\n\n\n\n\n\nLinear regression for A/B testing\nHere is an example in WebPPL of an application of a linear regression model to a case of A/B-testing, i.e., investigating whether there is a difference between two sets of measurements. If we encode the difference between groups as a slope coefficient, we can use the linear regression format to, essentially, mimick a kind of t-test (with unpaired measurements and equal variance).\nvar y_A = [127, 130, 106, 129, 114, 125, 119, 128]\nvar y_B = [131, 119, 115, 109, 110, 105, 118, 112]\n\n// define likelihood function\nvar LH_fun = function(mu, sigma, y_obs) {\n  map(\n    function(y) {var LH = Gaussian({mu: mu, sigma: sigma}).score(y);\n                 factor(LH);},\n    y_obs );\n}\n\nvar model = function() {\n  // model parameters\n  var beta_0 = gaussian(120,50);\n  var beta_1 = gaussian(0,30);\n  var sigma  = gamma(5,5);\n\n  // linear predictor (= predictor of central tendency)\n  var mu_A = beta_0\n  var mu_B = beta_0 + beta_1\n\n  // apply likelihood function\n  LH_fun(mu_A, sigma, y_A);\n  LH_fun(mu_B, sigma, y_B);\n\n  // samples from the posterior predictive distribution\n//   var y_A_pred = gaussian(mu_A, sigma)\n//   var y_B_pred = gaussian(mu_B, sigma)\n\n  return {beta_0, beta_1, sigma};\n//   return {y_A_pred , y_B_pred}\n  \n}\n\nviz.marginals(Infer({method: 'MCMC', samples: 15000, lag: 2, burn: 5000}, model));\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\nTry to see the underlying model of the data-generating process behind the code.\nVisualize just the priors and compare them with the posteriors.\nUse the commented-out code to draw samples from the posterior predictive distribution.\nNow draw samples from the prior predictive distribution.\nIf you wanted to mimick a t-test with unequal variance instead, could you change the code to do that? (NB: we only have very few data points, so your posteriors will likely be quite “ragged”.)"
  },
  {
    "objectID": "practice-sheets/01b-regression-BRMS.html",
    "href": "practice-sheets/01b-regression-BRMS.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "This is a very basic tutorial for running a simple Bayesian regression with brms. You will learn how to specify and run the model, and to extract, plot, and visualize posterior samples."
  },
  {
    "objectID": "practice-sheets/01b-regression-BRMS.html#data-wrangling",
    "href": "practice-sheets/01b-regression-BRMS.html#data-wrangling",
    "title": "Simple linear regression",
    "section": "Data wrangling",
    "text": "Data wrangling\nFirst, we will massage the data a little bit. We only want to look at those rows in which the participant selected the correct response. And instead of looking at all the data, we will only deal with the median values for each participant. In this way, we get a more robust signal (but really we do this here for more practical purposes (faster fitting, less clutter)). We will also scale both variables of interest, making the interpretation of slope coefficients easier (as otherwise the numbers are very large and possibly more confusing).\n\n\nToggle code\n# aggregate\ndolphin_agg &lt;- dolphin |&gt; \n  filter(correct == 1) |&gt; \n  group_by(subject_id) |&gt; \n  dplyr::summarize(\n    AUC = median(AUC, na.rm = TRUE),\n    MAD = median(MAD, na.rm = TRUE)) |&gt; \n  ungroup() |&gt; \n  mutate(\n    AUC = scale(AUC),\n    MAD = scale(MAD)\n  )\n  \n# let's take a look\nhead(dolphin_agg)\n\n\n# A tibble: 6 × 3\n  subject_id AUC[,1] MAD[,1]\n       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1       1001   0.643   0.593\n2       1002   0.732   0.367\n3       1003  -0.839  -0.833\n4       1004  -0.551  -0.535\n5       1005   0.619   0.436\n6       1006   0.748   1.02"
  },
  {
    "objectID": "practice-sheets/01b-regression-BRMS.html#visual-assessment",
    "href": "practice-sheets/01b-regression-BRMS.html#visual-assessment",
    "title": "Simple linear regression",
    "section": "Visual assessment",
    "text": "Visual assessment\nBefore we start thinking about statistical inference, we always want to get a feel for the data visually. You basically always want to plot the data.\n\n\nToggle code\n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_point(size = 3, alpha = 0.3) \n\n\n\n\n\nThis graph displays the distribution of AUC and MAD values. We can see that there is a strong relationship between AUC and MAD. And that makes a lot of sense. The more the cursor strives toward the competitor, the larger is the overall area under the curve. Heureka! Our hypothesis is confirmed.\nBut wait! As Bayesians, we would like to translate the data into an expression of evidence: do the data provide evidence for our research hypotheses? Also, notice that there is some variability. We want precise estimates of potential effects. We also want a measure of how certain we can be about these estimates."
  },
  {
    "objectID": "practice-sheets/01b-regression-BRMS.html#bayesian-linear-regression-with-brms",
    "href": "practice-sheets/01b-regression-BRMS.html#bayesian-linear-regression-with-brms",
    "title": "Simple linear regression",
    "section": "Bayesian linear regression with brms",
    "text": "Bayesian linear regression with brms\nThe brms package allows us to run Bayesian regression models, both simple and rather complex. It uses a sampling method, so its output will be vectors of (correlated) samples from the posterior distribution of the model’s parameters.\nSo, to quantify evidence and uncertainty with posterior samples, let’s run a simple linear regression model using brms. We use R’s standard notation to specify a formula in which AUC is predicted by MAD. (NB: brms extends this syntax substantially for more complex models beyond R’s standard formula syntax).\nAUC ~ MAD\nWhen you run this code, the brms package generates Stan code and runs the Stan program in the background. Stan code is executed in C++, and the model will be ‘compiled’ (you get information about this in the console output). You we will want to learn later what this compilation does (spoiler: it computes gradients for all stochastic nodes in the model). The only thing that is relevant for you at the moment is this: This compilation can take quite a while (especially for complex models) before anything happens.\n\n\nToggle code\n# specify model \nmodel1 = brm(\n  # model formula\n  AUC ~ MAD, \n  # data\n  data = dolphin_agg\n  )\n\n\n\n\nToggle code\nsummary(model1)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: AUC ~ MAD \n   Data: dolphin_agg (Number of observations: 108) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.00      0.03    -0.07     0.07 1.00     3663     2793\nMAD           0.94      0.03     0.87     1.00 1.00     3863     2994\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.35      0.02     0.31     0.40 1.00     3495     2428\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe output of such a model looks very familiar if you have worked with lm() before. We want to look at what is here called “Population-Level Effects”, which is a small table in this case. The first column contains the names of our coefficients; the Estimate column gives us the posterior mean of these coefficients; the Est.Error give us the standard error; the l-95%and u-95% give us the lower and upper limit of the 95% Credible Interval (henceforth CrI). The column Rhat (R^) which is a diagnostic of chain convergence and should not diverge much from 1 (rule of thumb: should by &lt;1.1). Again, more about that later. The Bulk_ESS and Tail_ESS columns give us numbers of “useful” samples. This number should be sufficiently high. If its not, brms will give you a convenient warning (more about that later, so don’t worry for now). If that happens, you need to increase the chains and / or the number of iterations in order to increase the overall number of samples (again, don’t worry for now).\nIf we need the main summary output in a tidy tibble format, we can use this function from the tidybayes package:\n\n\nToggle code\n# show summary in tidy format\ntidybayes::summarise_draws(model1)\n\n\n# A tibble: 5 × 10\n  variable        mean   median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;          &lt;num&gt;    &lt;num&gt;   &lt;num&gt;   &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 b_Intercept  7.94e-4 -5.50e-5 0.0344  0.0339   -0.0554   0.0572  1.00    3663.\n2 b_MAD        9.40e-1  9.40e-1 0.0337  0.0340    0.885    0.995   1.00    3863.\n3 sigma        3.50e-1  3.48e-1 0.0245  0.0243    0.312    0.391   1.00    3495.\n4 lprior      -3.16e+0 -3.16e+0 0.00241 0.00232  -3.16    -3.16    1.00    3545.\n5 lp__        -4.32e+1 -4.29e+1 1.28    1.01    -45.7    -41.8     1.00    1890.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\n\nThe model output suggests that the posterior mean of the Intercept is around 0.8e-4. The coefficient for MAD is estimated to be about 0.94.\nTo see how good a fit this is, we could manually draw this line into the graph from above.\n\n\nToggle code\n# extract model parameters:\nmodel_intercept &lt;- summary(model1)$fixed[1,1]\nmodel_slope &lt;- summary(model1)$fixed[2,1]\n\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(intercept = model_intercept, slope = model_slope, color = project_colors[2], size  = 1) +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1])\n\n\n\n\n\nLooking at the graph, it does make sense, right? The red line seems to capture the main trend pretty well.\nNow is there a relationship between AUC and MAD? What would it mean if there was no relationship between these two measures? Well no relationship would mean a slope of 0. How would that look like?\n\n\nToggle code\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(intercept = model_intercept, slope = model_slope, color = project_colors[2], size = 1) +\n  geom_abline(intercept = model_intercept, slope = 0, color = project_colors[3], size = 1, lty = \"dashed\") +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1])\n\n\n\n\n\nThese lines look quite different indeed. But Bayesian data analysis does not give us only one single line. It gives us infinitely many lines, weighted by plausibility. Let’s explore this universe of weighted predictions."
  },
  {
    "objectID": "practice-sheets/01b-regression-BRMS.html#extracting-posterior-distributions-and-plotting-them",
    "href": "practice-sheets/01b-regression-BRMS.html#extracting-posterior-distributions-and-plotting-them",
    "title": "Simple linear regression",
    "section": "Extracting posterior distributions and plotting them",
    "text": "Extracting posterior distributions and plotting them\nWe can interpret and visualize our coefficients immediately. We can create a data frame with all posterior samples for each parameter and plot those distributions for all coefficients. Let’s first see what coefficients there are with the get_variables() function from the tidybayes package.\n\n\nToggle code\n# inspect parameters\ntidybayes::get_variables(model1)\n\n\n [1] \"b_Intercept\"   \"b_MAD\"         \"sigma\"         \"lprior\"       \n [5] \"lp__\"          \"accept_stat__\" \"stepsize__\"    \"treedepth__\"  \n [9] \"n_leapfrog__\"  \"divergent__\"   \"energy__\"     \n\n\nEverything that is preceded by a b_ is a population level coefficients, i.e. our predictors. Now let’s wrangle this data frame to get what we need. You don’t have to entirely understand the following code, but make sure you understand it well enough to recycle it later on.\n\n\nToggle code\n# wrangle data frame\nposteriors1 &lt;- model1 |&gt;\n  tidybayes::spread_draws(b_MAD, b_Intercept) |&gt;\n  select(b_MAD, b_Intercept)\n\nposteriors1\n\n\n# A tibble: 4,000 × 2\n   b_MAD b_Intercept\n   &lt;dbl&gt;       &lt;dbl&gt;\n 1 0.955    -0.0618 \n 2 0.927     0.0575 \n 3 0.982    -0.0646 \n 4 0.943    -0.0379 \n 5 0.941     0.0202 \n 6 0.955     0.00394\n 7 0.949     0.00947\n 8 0.948     0.0121 \n 9 0.833    -0.0762 \n10 0.906     0.0769 \n# ℹ 3,990 more rows\n\n\nNow that we know how to extract posterior samples, let’s actually take a bunch of these samples and plot them as lines into our scatter plot from above. In this code chunk we generate a subsample of 100 parameter pairs. (There are methods to directly sample from the posterior values of the linear predictor, which is what you want to use for complex models, but here we go full hands-on.)\n\n\nToggle code\n# wrangle data frame\nposteriors2 &lt;- model1 |&gt;\n  # parameter 'ndraws' requests 100 random subsamples\n  tidybayes::spread_draws(b_MAD, b_Intercept, ndraws = 100) |&gt;\n  select(b_MAD, b_Intercept)\n  \n# plot\nggplot(data = dolphin_agg, \n       aes(x = MAD, \n           y = AUC)) + \n  geom_abline(data = posteriors2,\n              aes(intercept = b_Intercept, slope = b_MAD), \n              color = project_colors[2], size  = 0.1, alpha = 0.4) +\n  geom_point(size = 3, alpha = 0.3, color = project_colors[1])\n\n\n\n\n\n\n\n\n\nGiven our model, assumptions and data, these are 100 plausible regression lines. As you can see they are very similar.\nUsing this pipeline we can also calculate the mean of the posteriors and any kind of Credible Interval (CrI). We first extract the posterior and bring them into a tidy form. Let’s only look at the coefficient for MAD here.\n\n\nToggle code\nposteriors3 &lt;- model1 |&gt;\n   # use the gather_draws() function for \"long data\"\n   tidybayes::gather_draws(b_MAD) |&gt; \n   # change names of columns\n   rename(parameter = .variable,\n          posterior = .value) |&gt; \n   # select only those columns that are relevant\n   select(parameter, posterior)\n\nhead(posteriors3)\n\n\n# A tibble: 6 × 2\n# Groups:   parameter [1]\n  parameter posterior\n  &lt;chr&gt;         &lt;dbl&gt;\n1 b_MAD         0.955\n2 b_MAD         0.927\n3 b_MAD         0.982\n4 b_MAD         0.943\n5 b_MAD         0.941\n6 b_MAD         0.955\n\n\nAnd then calculate the mean, the lower and the upper bound of a 90% CrI, using the function tidybayes::hdi().\n\n\nToggle code\n# get posteriors for the relevant coefficients\nposteriors3_agg &lt;- posteriors3 |&gt; \n  group_by(parameter) |&gt; \n  summarise(\n    `90lowerCrI`   = tidybayes::hdi(posterior, credMass = 0.90)[1],\n    mean_posterior = mean(posterior),\n    `90higherCrI`  = tidybayes::hdi(posterior, credMass = 0.90)[2])\n\nposteriors3_agg \n\n\n# A tibble: 1 × 4\n  parameter `90lowerCrI` mean_posterior `90higherCrI`\n  &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1 b_MAD            0.875          0.940          1.00\n\n\nNow we use this newly created data frame to plot the posterior distributions of all population-level coefficients. Again, we use our new best friend, the tidybayes package which offers some sweet extensions to ggplot’s geom_ family of functions. We also add a reference point to compare the posteriors against. A common and reasonable reference point is 0. Remember a slope coefficient of zero would correspond to a flat regression line.\n\n\nToggle code\n# plot the regression coefficients\nposteriors1 |&gt; \n  pivot_longer(cols = everything(), names_to = \"parameter\", values_to = \"posterior\") |&gt; \n  ggplot(aes(x = posterior, y = parameter, fill = parameter)) + \n    # plot density w/ 90% credible interval\n    tidybayes::stat_halfeye(.width = 0.9) +\n    # add axes titles\n    xlab(\"\") +\n    ylab(\"\") +\n    # adjust the x-axis \n    scale_x_continuous(limits = c(-0.25,1.5)) +\n    # add line for the value zero\n    geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n                 lty = \"dashed\") +\n    theme(legend.position=\"none\")\n\n\n\n\n\nToggle code\nposteriors3_agg[1,2]\n\n\n# A tibble: 1 × 1\n  `90lowerCrI`\n         &lt;dbl&gt;\n1        0.875\n\n\nHere you see density plots for our critical coefficients of the model. We care mostly about the slope coefficient (b_MAD) (the posterior of which is shown in red). Values between about 0.87 and about 1 are plausible (at the 90% level) and they are indicated by the thick black line in the density plot for this coefficient. The mean of the distribution is indicated by the thick black dot.\nThat’s helpful because we can relate this distribution to relevant values, for example the value 0 (dashed line). If you look at the coefficient, you can see that the posterior distribution does not include the value zero or any small-ish “Region of Practical Equivalence” around it. In fact, the posterior is really far away from zero. Thus, if we believe in the data and the model, we can be very certain that this coefficient is not zero. In other words, we would be very certain that there is a positive relationship between AUC and MAD (and in turn that ‘no relationship’ is not a very plausible scenario).\nThe brms package allows us to quickly evaluate how many posterior samples fall into a certain value range. Just for fun, let’s focus on the slope coefficient for MAD and ask whether this parameter is credibly larger than 1 in the posterior. We can estimate the posterior probability of \\(P(\\text{MAD}&gt;1 \\mid D)\\) given the data by the proportion of posterior samples for this parameter that are larger than one. Notice that, alternatively, we can take all posterior samples, subtract one, and then consider the proportion of modified values which is bigger than 0. We could do this by hand, but we can also use the hypothesis() function from the brms package, which gives us even more information. (See the later chapter on hypothesis testing for pros and cons of using this function.)\n\n\nToggle code\nhypothesis(model1, 'MAD &gt; 1')\n\n\nHypothesis Tests for class b:\n     Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (MAD)-(1) &gt; 0    -0.06      0.03    -0.12    -0.01       0.04      0.04     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nThe results tell us that fewer than 5% of all posterior samples are larger than 1 (in the column “Post.Prob”. It also tells us the evidence ratio (more on this later), which is the odds of the hypothesis in question (here ‘MAD &gt; 1’)."
  },
  {
    "objectID": "practice-sheets/04e-mixture-models.html",
    "href": "practice-sheets/04e-mixture-models.html",
    "title": "XX: Mixture models",
    "section": "",
    "text": "This tutorial discusses a minimal example of a mixture model. After introducing the main idea behind mixture models, a fictitious (minimal) data set is analyzed first with a hand-written Stan program, and then with a mixture regression model using brms."
  },
  {
    "objectID": "practice-sheets/04e-mixture-models.html#the-stan-model",
    "href": "practice-sheets/04e-mixture-models.html#the-stan-model",
    "title": "XX: Mixture models",
    "section": "The Stan model",
    "text": "The Stan model\nWe are going to pack the data together for fitting the Stan model:\n\n\nToggle code\ndata_GMM &lt;- list(\n  y = flower_heights,\n  N = length(flower_heights)\n)\n\n\nBelow is the Stan code for this model. It is also given in file Gaussian-mixture-01-basic.stan. A few comments on this code:\n\nThere is no occurrence of variable \\(z_i\\), as this is marginalized out. We do this by incrementing the log-score manually, using target += log_sum_exp(alpha).\nWe declare vector mu to be of a particular type which we have not seen before. We want the vector to be ordered. We will come back to this later. Don’t worry about it now.\n\ndata {\n  int&lt;lower=1&gt; N; \n  real y[N];      \n}\nparameters {\n  real&lt;lower=0,upper=1&gt; p;         \n  ordered[2] mu;             \n  vector&lt;lower=0&gt;[2] sigma; \n}\nmodel {\n  p ~ beta(1,1);\n  mu ~ normal(12, 10);\n  sigma ~ lognormal(0, 1);\n  for (i in 1:N) {\n    vector[2] alpha;\n    alpha[1] = log(p)   + normal_lpdf(y[i] | mu[1], sigma[1]);\n    alpha[2] = log(1-p) + normal_lpdf(y[i] | mu[2], sigma[2]);\n    target += log_sum_exp(alpha);\n  }\n}\n\n\nToggle code\nstan_fit_2b_GMM &lt;- stan(\n  file = 'stan-files/Gaussian-mixture-01-basic.stan',\n  data = data_GMM\n)\n\n\n\n\nToggle code\nstan_fit_2b_GMM\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n            mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff\np           0.54    0.01 0.14    0.25    0.46    0.54    0.63    0.82   692\nmu[1]      10.42    0.03 0.85    8.96    9.84   10.34   10.92   12.38   712\nmu[2]      15.70    0.02 0.63   14.05   15.39   15.79   16.12   16.64   812\nsigma[1]    2.08    0.02 0.58    1.12    1.67    2.01    2.44    3.35   753\nsigma[2]    1.41    0.02 0.50    0.68    1.08    1.34    1.65    2.77   727\nlp__     -127.32    0.07 2.02 -132.54 -128.37 -126.86 -125.81 -124.77   739\n         Rhat\np           1\nmu[1]       1\nmu[2]       1\nsigma[1]    1\nsigma[2]    1\nlp__        1\n\nSamples were drawn using NUTS(diag_e) at Sat Feb 10 10:26:55 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\n\n\n\n\n\nExercise 1b: Interpret this outcome\n\n\n\n\n\nInterpret these results! Focus on parameters \\(p\\), \\(\\mu_1\\) and \\(\\mu_2\\). What does \\(p\\) capture in this implementation? Do the (mean) estimated values make sense?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes, they do make sense. \\(p\\) is the prevalence of data from the group with the higher mean, which is group \\(B\\) in our case. The model infers that there are roughly equally many data points from each group, which is indeed the case. The model also recovers the descriptive means of each group!\n\n\nToggle code\nffm_data |&gt; \n  group_by(species) |&gt; \n  summarise(\n    mean     = mean(height),\n    std_dev  = sd(height)\n  )\n\n\n# A tibble: 2 × 3\n  species  mean std_dev\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 A        10.0    1.76\n2 B        15.7    1.38"
  },
  {
    "objectID": "practice-sheets/04e-mixture-models.html#an-unidentifiable-model",
    "href": "practice-sheets/04e-mixture-models.html#an-unidentifiable-model",
    "title": "XX: Mixture models",
    "section": "An unidentifiable model",
    "text": "An unidentifiable model\nLet’s run the model in file Gaussian-mixture-02-unindentifiable.stan, which is exactly the same as before but with vector mu being an unordered vector of reals.\n\n\nToggle code\nstan_fit_2c_GMM &lt;- stan(\n  file = 'stan-files/Gaussian-mixture-02-unindentifiable.stan',\n  data = data_GMM,\n  # set a seed for reproducible results\n  seed = 1734\n)\n\n\nHere’s a summary of the outcome:\n\n\nToggle code\nstan_fit_2c_GMM\n\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n            mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff\np           0.50    0.02 0.14    0.21    0.40    0.50    0.59    0.77    81\nmu[1]      12.85    1.80 2.77    9.13   10.20   11.71   15.72   16.50     2\nmu[2]      13.26    1.75 2.77    9.19   10.44   14.76   15.82   16.50     3\nsigma[1]    1.75    0.21 0.61    0.80    1.30    1.66    2.12    3.11     8\nsigma[2]    1.74    0.21 0.64    0.80    1.27    1.64    2.10    3.18     9\nlp__     -128.86    0.05 1.86 -133.37 -129.79 -128.46 -127.51 -126.51  1241\n         Rhat\np        1.04\nmu[1]    2.72\nmu[2]    2.41\nsigma[1] 1.18\nsigma[2] 1.16\nlp__     1.00\n\nSamples were drawn using NUTS(diag_e) at Sat Feb 10 10:27:17 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\n\n\n\n\n\nExercise 1c: Interpret model output\n\n\n\n\n\nWhat is remarkable here? Explain what happened. Explain in what sense this model is “unidentifiable”.\nHint: Explore the parameters with high \\(\\hat{R}\\) values. When a model fit seems problematic, a nice tool to explore what might be amiss is the package shinystan. You could do this:\n\n\nToggle code\nshinystan::launch_shinystan(stan_fit_2c_GMM)\n\n\nThen head over to the tab “Explore” and have a look at some of the parameters.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe \\(\\hat{R}\\) values of the mean parameters are substantially above 1, suggesting that the model did not converge. But if we look at trace plots for these parameters, for example, we see that \\(\\mu_1\\) has “locked into” group A for some chains, and into group B for some other chains.\nThe model is therefore unidentifiable in the sense that, without requiring that mu is ordered, \\(\\mu_1\\) could be for group A or group B, and which one it will take on depends on random initialization. Requiring that mu be ordered, breaks this symmetry."
  },
  {
    "objectID": "practice-sheets/04e-mixture-models.html#posterior-predictive-check",
    "href": "practice-sheets/04e-mixture-models.html#posterior-predictive-check",
    "title": "XX: Mixture models",
    "section": "Posterior predictive check",
    "text": "Posterior predictive check\nWe can extend the (identifiable) model from above to also output samples from the posterior predictive distribution. This is given in file Gaussian-mixture-03-withPostPred.stan. Let’s run this model, collect the posterior predictive samples in a variable called yrep and draw a density plot.\n\n\nToggle code\nstan_fit_2d_GMM &lt;- stan(\n  file = 'stan-files/Gaussian-mixture-03-withPostPred.stan',\n  data = data_GMM,\n  # only return the posterior predictive samples\n  pars = c('yrep')\n)\n\n\n\n\nToggle code\ntibble(\n  source  = c(rep(\"data\", length(flower_heights)), rep(\"PostPred\", length(extract(stan_fit_2d_GMM)$yrep))),\n  height = c(flower_heights, extract(stan_fit_2d_GMM)$yrep)\n) |&gt;  \n  ggplot(aes(x = height, fill=source, color = source)) +\n  geom_density(size = 2, alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\n\nExercise 1d: Scrutinize posterior predictive check\n\n\n\n\n\nDoes this look like a distribution that could have generated the data?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe distribution looks plausible enough. The visual fit in these density plots is not perfect also because we use quite a different number of samples to estimate the density."
  },
  {
    "objectID": "practice-sheets/05b-divergences.html",
    "href": "practice-sheets/05b-divergences.html",
    "title": "Divergent transitions",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}"
  },
  {
    "objectID": "practice-sheets/05b-divergences.html#why-is-non-central-paramterization-better",
    "href": "practice-sheets/05b-divergences.html#why-is-non-central-paramterization-better",
    "title": "Divergent transitions",
    "section": "Why is non-central paramterization better?",
    "text": "Why is non-central paramterization better?\nTo understand (intuitively) why is non-central paramterization more resilient against divergent, first recall the reason for the divergencies in the first model. Inside the funnel, so to speak, the variance of “good” \\(\\theta_i\\) values were much smaller than outside of it. So, the sampler was not able to to flexibly accommodate differences in changes to \\(\\theta_i\\) parameters as \\(\\sigma'\\) changed. The non-central parameterization overcomes exactly this problem, because it decouples \\(\\sigma'\\) and \\(\\theta_i\\) via an additional parameter \\(\\eta_i\\).\nA heuristic way of seeing the crucial difference between models is that in the first (central parameterization) model, the distribution from which latent parameters are sampled depends itself on the value of sampled latent parameters, whereas in the second (non-central parameterization) model the distributions from which each latent parameter is sampled are fixed, i.e., independent from values of other parameters. Sampling from the latter is therefore easier to optimize."
  },
  {
    "objectID": "practice-sheets/05b-divergences.html#why-care-about-divergent-transitions",
    "href": "practice-sheets/05b-divergences.html#why-care-about-divergent-transitions",
    "title": "Divergent transitions",
    "section": "Why care about divergent transitions?",
    "text": "Why care about divergent transitions?\nWe see why we should care about divergent transitions based on this simple example if we ask concretely:\n\nWhat is the main striking difference (apart from the presence/absence of divergent transitions)?\nHow is this difference a reason for why divergent transitions can be problematic?\nIs any estimated posterior mean for any parameter noticeably affected by this?\n\nThe “funnel” in the non-central model fit is much “deeper”. The samples for \\(\\log \\sigma'\\) stopped at around 1 for the first model with central-parameterization. But for the non-central one they go to values of -4. While this is in log-scale, it nevertheless shows how the divergencies can cause failure to explore a reasonable chunk of the posterior space. Expectations based on samples with such restrictions can consequently be biased. Indeed, the estimated mean for \\(\\sigma'\\) is discernibly lower for the non-centralized parameterization."
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html",
    "href": "practice-sheets/11a-cheat-sheet.html",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "",
    "text": "This document provides a cursory run-down of common operations and manipulations for working with the brms package."
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#updating-a-model",
    "href": "practice-sheets/11a-cheat-sheet.html#updating-a-model",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Updating a model",
    "text": "Updating a model\nUsing stats::update(), refit a model based on an existing model fit, keeping everything as is, except for what is explicitly set:\n\n\nToggle code\n# take existing fit, refit on smaller data set, just take 100 samples (all else equal)\nfit_first_five &lt;- \n  stats::update(\n    object = fit_MC,\n    iter = 100,\n    # use first five participants only\n    newdata = data_MC |&gt; filter(submission_id &gt;= 8550)\n  )"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#formula-syntax",
    "href": "practice-sheets/11a-cheat-sheet.html#formula-syntax",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Formula syntax",
    "text": "Formula syntax\nThe basic form of a brms formula is: response ~ pterms + (gterms | group)\n\nMulti-level modeling\n\n(gterms || group) : suppress correlation between gterms\n(gterms | g1 + g2) : syntactic sugar for (gterms | g1) + (gterms | g2)\n(gterms | g1 : g2) : all combinations of g1 and g2 (Cartesian product)\n(gterms | g1 / g2) : nesting g2 within g1; equals (gterms | g1) + (gterms | g1 : g2)\n(gterms | IDx | group) : correlation for all group-level categories with IDx\n\nuseful for multi-formula models (e.g., non-linear models)"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#summaries",
    "href": "practice-sheets/11a-cheat-sheet.html#summaries",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Summaries",
    "text": "Summaries\nStandard summary of the model fit:\n\n\nToggle code\nsummary(fit_MC)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ condition + (1 + condition + shape | submission_id) \n   Data: data_MC (Number of observations: 2519) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nGroup-Level Effects: \n~submission_id (Number of levels: 50) \n                                               Estimate Est.Error l-95% CI\nsd(Intercept)                                     46.98      8.13    32.61\nsd(conditionreaction)                             26.72     11.83     3.76\nsd(conditiondiscrimination)                       89.69     12.00    68.57\nsd(shapesquare)                                   18.74     10.10     1.29\ncor(Intercept,conditionreaction)                  -0.37      0.29    -0.81\ncor(Intercept,conditiondiscrimination)             0.66      0.17     0.29\ncor(conditionreaction,conditiondiscrimination)    -0.29      0.30    -0.83\ncor(Intercept,shapesquare)                        -0.05      0.35    -0.65\ncor(conditionreaction,shapesquare)                -0.10      0.40    -0.81\ncor(conditiondiscrimination,shapesquare)           0.25      0.32    -0.47\n                                               u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)                                     64.43 1.00     4482     5891\nsd(conditionreaction)                             50.06 1.00     1313     1602\nsd(conditiondiscrimination)                      115.34 1.00     3609     5512\nsd(shapesquare)                                   38.59 1.00     1709     2647\ncor(Intercept,conditionreaction)                   0.34 1.00     4798     3772\ncor(Intercept,conditiondiscrimination)             0.93 1.00     1608     2408\ncor(conditionreaction,conditiondiscrimination)     0.32 1.00     1542     2726\ncor(Intercept,shapesquare)                         0.68 1.00     4435     4791\ncor(conditionreaction,shapesquare)                 0.70 1.00     2712     3689\ncor(conditiondiscrimination,shapesquare)           0.80 1.00     7163     5503\n\nPopulation-Level Effects: \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                 442.00      9.02   424.58   459.89 1.00     5035\nconditionreaction        -130.78      8.53  -147.21  -114.03 1.00    11961\nconditiondiscrimination    74.66     14.87    45.68   104.10 1.00     4481\n                        Tail_ESS\nIntercept                   6029\nconditionreaction           6399\nconditiondiscrimination     5319\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   135.47      1.99   131.62   139.46 1.00     9557     5691\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSame in tidy format:\n\n\nToggle code\ntidybayes::summarise_draws(fit_MC)\n\n\n# A tibble: 216 × 10\n   variable         mean   median     sd    mad       q5      q95  rhat ess_bulk\n   &lt;chr&gt;           &lt;num&gt;    &lt;num&gt;  &lt;num&gt;  &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n 1 b_Intercept   442.     442.     9.02   9.01   427.     457.     1.00    5035.\n 2 b_condition… -131.    -131.     8.53   8.53  -145.    -117.     1.00   11961.\n 3 b_condition…   74.7     74.6   14.9   14.7     50.5     99.2    1.00    4481.\n 4 sd_submissi…   47.0     46.3    8.13   7.99    34.7     61.3    1.00    4482.\n 5 sd_submissi…   26.7     26.8   11.8   11.9      6.77    46.3    1.00    1313.\n 6 sd_submissi…   89.7     89.1   12.0   11.7     71.3    110.     1.00    3609.\n 7 sd_submissi…   18.7     18.6   10.1   11.1      2.56    35.5    1.00    1709.\n 8 cor_submiss…   -0.367   -0.416  0.292  0.272   -0.756    0.188  1.00    4798.\n 9 cor_submiss…    0.655    0.673  0.168  0.172    0.356    0.901  1.00    1608.\n10 cor_submiss…   -0.288   -0.298  0.301  0.317   -0.771    0.219  1.00    1542.\n# ℹ 206 more rows\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\n\nSummary of just the fixed effects:\n\n\nToggle code\nbrms::fixef(fit_MC)\n\n\n                          Estimate Est.Error       Q2.5     Q97.5\nIntercept                441.99889   9.01703  424.58412  459.8895\nconditionreaction       -130.78398   8.52971 -147.21041 -114.0289\nconditiondiscrimination   74.66133  14.86693   45.67624  104.1007\n\n\nSummary of just the random effects (this is huge, so output suppressed):\n\n\nToggle code\nbrms::ranef(fit_MC)"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#retrieve-names-of-model-variables",
    "href": "practice-sheets/11a-cheat-sheet.html#retrieve-names-of-model-variables",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Retrieve names of model variables",
    "text": "Retrieve names of model variables\n\n\nToggle code\ntidybayes::get_variables(fit_MC)[1:10]\n\n\n [1] \"b_Intercept\"                                                  \n [2] \"b_conditionreaction\"                                          \n [3] \"b_conditiondiscrimination\"                                    \n [4] \"sd_submission_id__Intercept\"                                  \n [5] \"sd_submission_id__conditionreaction\"                          \n [6] \"sd_submission_id__conditiondiscrimination\"                    \n [7] \"sd_submission_id__shapesquare\"                                \n [8] \"cor_submission_id__Intercept__conditionreaction\"              \n [9] \"cor_submission_id__Intercept__conditiondiscrimination\"        \n[10] \"cor_submission_id__conditionreaction__conditiondiscrimination\""
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#explore-via-shinystan",
    "href": "practice-sheets/11a-cheat-sheet.html#explore-via-shinystan",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Explore via shinystan",
    "text": "Explore via shinystan\n\n\nToggle code\nshinystan::launch_shinystan(fit_MC)"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#tidy-samples-with-tidybayes",
    "href": "practice-sheets/11a-cheat-sheet.html#tidy-samples-with-tidybayes",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Tidy samples with tidybayes",
    "text": "Tidy samples with tidybayes\nRetrieve all samples with tidybayes::tidy_draws():\n\n\nToggle code\ntidybayes::tidy_draws(fit_MC)\n\n\n# A tibble: 8,000 × 225\n   .chain .iteration .draw b_Intercept b_conditionreaction\n    &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;               &lt;dbl&gt;\n 1      1          1     1        432.               -124.\n 2      1          2     2        435.               -122.\n 3      1          3     3        412.               -118.\n 4      1          4     4        440.               -147.\n 5      1          5     5        415.               -120.\n 6      1          6     6        427.               -131.\n 7      1          7     7        438.               -129.\n 8      1          8     8        445.               -133.\n 9      1          9     9        427.               -121.\n10      1         10    10        449.               -130.\n# ℹ 7,990 more rows\n# ℹ 220 more variables: b_conditiondiscrimination &lt;dbl&gt;,\n#   sd_submission_id__Intercept &lt;dbl&gt;,\n#   sd_submission_id__conditionreaction &lt;dbl&gt;,\n#   sd_submission_id__conditiondiscrimination &lt;dbl&gt;,\n#   sd_submission_id__shapesquare &lt;dbl&gt;,\n#   cor_submission_id__Intercept__conditionreaction &lt;dbl&gt;, …"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#getting-summaries-for-samples",
    "href": "practice-sheets/11a-cheat-sheet.html#getting-summaries-for-samples",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Getting summaries for samples",
    "text": "Getting summaries for samples\nTo get (Bayesian) summary statistics for a vector of samples from a parameter you can do this:\n\n\nToggle code\nposterior_Intercept &lt;- \n  tidybayes::tidy_draws(fit_MC) |&gt; \n  dplyr::pull(\"b_Intercept\")\n\n\nThe tidybayes::hdi function gives the upper and lower bound of a Bayesian credible interval:\n\n\nToggle code\ntidybayes::hdi(posterior_Intercept, credMass = 0.90)\n\n\n         [,1]     [,2]\n[1,] 424.3248 459.5983\n\n\nThe function aida::summarize_sample_vector does so, too.\n\n\nToggle code\naida::summarize_sample_vector(posterior_Intercept, name = \"Intercept\")\n\n\n# A tibble: 1 × 4\n  Parameter `|95%`  mean `95%|`\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Intercept   425.  442.   460.\n\n\nHere is how you can do this for several vectors at once:\n\n\nToggle code\ntidybayes::tidy_draws(fit_MC) |&gt; \n  dplyr::select(starts_with(\"b_\")) |&gt; \n  pivot_longer(cols = everything()) |&gt; \n  group_by(name) |&gt; \n  reframe(aida::summarize_sample_vector(value)[-1])\n\n\n# A tibble: 3 × 4\n  name                      `|95%`   mean `95%|`\n  &lt;chr&gt;                      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 b_Intercept                425.   442.    460.\n2 b_conditiondiscrimination   44.7   74.7   103.\n3 b_conditionreaction       -148.  -131.   -114."
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#population-level-parameters",
    "href": "practice-sheets/11a-cheat-sheet.html#population-level-parameters",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Population-level parameters",
    "text": "Population-level parameters\nTo plot the posteriors over model paramters, you can use various plots from the bayesplot package (here information here):\n\n\nToggle code\nposterior_draws &lt;- brms::as_draws_matrix(fit_MC)[,c(\"b_conditionreaction\", \"b_conditiondiscrimination\")]\nbayesplot::mcmc_areas(posterior_draws)\n\n\n\n\n\nOr, use the tidybayes package:\n\n\nToggle code\nfit_MC |&gt; \n  tidy_draws() |&gt; \n  select(starts_with(\"b_con\")) |&gt; \n  rename(reaction = b_conditionreaction,\n         discrimination = b_conditiondiscrimination) |&gt; \n  pivot_longer(cols = everything()) |&gt; \n  ggplot(aes(x = value, y = name)) +\n  tidybayes::stat_halfeye(fill = project_colors[1]) +\n  ylab(\"\") +\n  geom_vline(aes(xintercept = 0), color = project_colors[2], size = 2)"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#group-level-parameters",
    "href": "practice-sheets/11a-cheat-sheet.html#group-level-parameters",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Group-level parameters",
    "text": "Group-level parameters\nHere is an example of plotting the posterior for random effects (here: the by-subject random intercepts):\n\n\nToggle code\nranef(fit_MC)$submission_id[,,\"Intercept\"] |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"submission_id\") |&gt; \n  ggplot(aes(y = submission_id, x = Estimate)) +\n  geom_errorbar(aes(xmin = `Q2.5`, xmax = `Q97.5`), \n                color = project_colors[6], alpha = 0.7)+\n  geom_vline(aes(xintercept = 0), color = project_colors[1], \n             size = 2, alpha = 0.8) +\n  geom_point(color = project_colors[2], size = 2)"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#visual-ppcs",
    "href": "practice-sheets/11a-cheat-sheet.html#visual-ppcs",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Visual PPCs",
    "text": "Visual PPCs\nUse tools from the bayesplot package. The basic bayesplot::pp_check() plots the distribution of ndraws samples from the posterior (data) predictive against the distribution of the data the model was trained on:\n\n\nToggle code\nbayesplot::pp_check(fit_MC, ndraws = 30)\n\n\n\n\n\nThere are many tweaks to pp_check:\n\n\nToggle code\nbayesplot::pp_check(fit_MC, ndraws = 30, type = \"hist\")\n\n\n\n\n\nYou can also directly use underlying functions like ppc_stat. See help(\"PPC-overview\") and help(\"PPD-overview\") for what is available.\n\n\nToggle code\npredictive_samples &lt;- brms::posterior_predict(fit_MC, ndraws = 1000)\npredictive_samples[1:5, 1:5] \n\n\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 299.9574 224.1997 387.6902 534.5947 330.5859\n[2,] 305.9095 321.3366 169.0921 340.9020 444.8452\n[3,] 347.1188 476.9047 372.1247 181.1809 356.2991\n[4,] 269.2256 255.3181 397.9657 250.2674 513.4674\n[5,] 429.6383 311.3259 620.8028 241.3073 163.6873\n\n\nToggle code\nbayesplot::ppc_stat(\n  y    = data_MC$RT, \n  yrep = predictive_samples,\n  # specify the test statistic of interest\n  stat = function(x){quantile(x, 0.8)})"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#extracting-samples-from-the-posterior-predictive-distribution",
    "href": "practice-sheets/11a-cheat-sheet.html#extracting-samples-from-the-posterior-predictive-distribution",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Extracting samples from the posterior predictive distribution",
    "text": "Extracting samples from the posterior predictive distribution\nThere are three kinds of commonly relevant variables a generalized linear model predicts:\n\nthe central tendency of data \\(y\\) for some predictor \\(x\\),\nthe shape of the (hypothetical) data \\(y'\\) for \\(x\\), and\na linear predictor value given values of \\(x\\).\n\nAll of these measures can be obtained from a fitted model with different functions, e.g., from the tidyverse package. Here, it does not matter whether the model was fitted to data or it is a “prior model”, so to speak, fit with the flag sample_prior = \"only\".\nHere is an example for a logistic regression model (where all the three measures clearly show their conceptual difference).\n\n\nToggle code\nfit_MT_logistic &lt;- \n  brms::brm(\n    formula = correct ~ group * condition,\n    data    = aida::data_MT,\n    family  = brms::bernoulli()\n  )\n\n\n\n\nToggle code\n# 2 samples from the predicted central tendency\naida::data_MT |&gt; \n  dplyr::select(group, condition) |&gt; \n  unique() |&gt; \n  tidybayes::add_epred_draws(\n    fit_MT_logistic,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .epred\n  &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1 touch Atypical      1     NA         NA     1  0.917\n2 touch Atypical      1     NA         NA     2  0.929\n3 touch Typical       2     NA         NA     1  0.932\n4 touch Typical       2     NA         NA     2  0.946\n5 click Atypical      3     NA         NA     1  0.878\n6 click Atypical      3     NA         NA     2  0.851\n7 click Typical       4     NA         NA     1  0.963\n8 click Typical       4     NA         NA     2  0.961\n\n\nToggle code\n# 2 samples from the predictive distribution (data samples)\naida::data_MT |&gt; \n  dplyr::select(group, condition) |&gt; \n  unique() |&gt; \n  tidybayes::add_predicted_draws(\n    fit_MT_logistic,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .prediction\n  &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;int&gt;\n1 touch Atypical      1     NA         NA     1           1\n2 touch Atypical      1     NA         NA     2           1\n3 touch Typical       2     NA         NA     1           1\n4 touch Typical       2     NA         NA     2           1\n5 click Atypical      3     NA         NA     1           1\n6 click Atypical      3     NA         NA     2           0\n7 click Typical       4     NA         NA     1           1\n8 click Typical       4     NA         NA     2           1\n\n\nToggle code\n# 2 samples for the linear predictor\naida::data_MT |&gt; \n  dplyr::select(group, condition) |&gt; \n  unique() |&gt; \n  tidybayes::add_linpred_draws(\n    fit_MT_logistic,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .linpred\n  &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n1 touch Atypical      1     NA         NA     1     2.05\n2 touch Atypical      1     NA         NA     2     2.77\n3 touch Typical       2     NA         NA     1     2.68\n4 touch Typical       2     NA         NA     2     2.81\n5 click Atypical      3     NA         NA     1     1.80\n6 click Atypical      3     NA         NA     2     1.75\n7 click Typical       4     NA         NA     1     3.10\n8 click Typical       4     NA         NA     2     3.37"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#inspecting-default-priors-without-running-the-model",
    "href": "practice-sheets/11a-cheat-sheet.html#inspecting-default-priors-without-running-the-model",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Inspecting default priors without running the model",
    "text": "Inspecting default priors without running the model\n\n\nToggle code\n# define the model as a \"brmsformula\" object\nmyFormula &lt;- brms::bf(RT ~ 1 + condition + (1 + condition | submission_id))\n\n# get prior information\nbrms::get_prior(\n  formula = myFormula,\n  data    = data_MC,\n  family  = exgaussian()\n  )\n\n\n                    prior     class                    coef         group resp\n student_t(3, 385, 133.4) Intercept                                           \n                   (flat)         b                                           \n                   (flat)         b conditiondiscrimination                   \n                   (flat)         b       conditionreaction                   \n            gamma(1, 0.1)      beta                                           \n                   lkj(1)       cor                                           \n                   lkj(1)       cor                         submission_id     \n   student_t(3, 0, 133.4)        sd                                           \n   student_t(3, 0, 133.4)        sd                         submission_id     \n   student_t(3, 0, 133.4)        sd               Intercept submission_id     \n   student_t(3, 0, 133.4)        sd conditiondiscrimination submission_id     \n   student_t(3, 0, 133.4)        sd       conditionreaction submission_id     \n   student_t(3, 0, 133.4)     sigma                                           \n dpar nlpar lb ub       source\n                       default\n                       default\n                  (vectorized)\n                  (vectorized)\n             0         default\n                       default\n                  (vectorized)\n             0         default\n             0    (vectorized)\n             0    (vectorized)\n             0    (vectorized)\n             0    (vectorized)\n             0         default"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#setting-priors",
    "href": "practice-sheets/11a-cheat-sheet.html#setting-priors",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Setting priors",
    "text": "Setting priors\n\n\nToggle code\nmyPrior &lt;- \n  brms::prior(\"normal(30,100)\",  class = \"b\", coef = \"conditiondiscrimination\") +\n  brms::prior(\"normal(-30,100)\", class = \"b\", coef = \"conditionreaction\")\n\nfit_with_specified_prior &lt;- \n  brms::brm(\n    formula = myformula,\n    data    = data_MC,\n    prior   = myPrior,\n    family  = exgaussian()\n  )"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#sampling-from-the-prior",
    "href": "practice-sheets/11a-cheat-sheet.html#sampling-from-the-prior",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Sampling from the prior",
    "text": "Sampling from the prior\n\n\nToggle code\nfit_samples_from_prior_only &lt;- \nbrms::brm(\n  formula = myformula,\n  data    = data_MC,\n  prior   = myPrior,\n  family  = exgaussian(),\n  sample_prior = \"only\"\n)"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#extract-the-stan-code",
    "href": "practice-sheets/11a-cheat-sheet.html#extract-the-stan-code",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Extract the Stan code",
    "text": "Extract the Stan code\n\n\nToggle code\nbrms::stancode(fit_MC)\n\n\n// generated with brms 2.20.1\nfunctions {\n /* compute correlated group-level effects\n  * Args:\n  *   z: matrix of unscaled group-level effects\n  *   SD: vector of standard deviation parameters\n  *   L: cholesky factor correlation matrix\n  * Returns:\n  *   matrix of scaled group-level effects\n  */\n  matrix scale_r_cor(matrix z, vector SD, matrix L) {\n    // r is stored in another dimension order than z\n    return transpose(diag_pre_multiply(SD, L) * z);\n  }\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int&lt;lower=1&gt; Kc;  // number of population-level effects after centering\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  int&lt;lower=1&gt; J_1[N];  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  vector[N] Z_1_2;\n  vector[N] Z_1_3;\n  vector[N] Z_1_4;\n  int&lt;lower=1&gt; NC_1;  // number of group-level correlations\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  matrix[N, Kc] Xc;  // centered version of X without an intercept\n  vector[Kc] means_X;  // column means of X before centering\n  for (i in 2:K) {\n    means_X[i - 1] = mean(X[, i]);\n    Xc[, i - 1] = X[, i] - means_X[i - 1];\n  }\n}\nparameters {\n  vector[Kc] b;  // regression coefficients\n  real Intercept;  // temporary intercept for centered predictors\n  real&lt;lower=0&gt; sigma;  // dispersion parameter\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  matrix[M_1, N_1] z_1;  // standardized group-level effects\n  cholesky_factor_corr[M_1] L_1;  // cholesky factor of correlation matrix\n}\ntransformed parameters {\n  matrix[N_1, M_1] r_1;  // actual group-level effects\n  // using vectors speeds up indexing in loops\n  vector[N_1] r_1_1;\n  vector[N_1] r_1_2;\n  vector[N_1] r_1_3;\n  vector[N_1] r_1_4;\n  real lprior = 0;  // prior contributions to the log posterior\n  // compute actual group-level effects\n  r_1 = scale_r_cor(z_1, sd_1, L_1);\n  r_1_1 = r_1[, 1];\n  r_1_2 = r_1[, 2];\n  r_1_3 = r_1[, 3];\n  r_1_4 = r_1[, 4];\n  lprior += student_t_lpdf(Intercept | 3, 385, 133.4);\n  lprior += student_t_lpdf(sigma | 3, 0, 133.4)\n    - 1 * student_t_lccdf(0 | 3, 0, 133.4);\n  lprior += student_t_lpdf(sd_1 | 3, 0, 133.4)\n    - 4 * student_t_lccdf(0 | 3, 0, 133.4);\n  lprior += lkj_corr_cholesky_lpdf(L_1 | 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n] + r_1_3[J_1[n]] * Z_1_3[n] + r_1_4[J_1[n]] * Z_1_4[n];\n    }\n    target += normal_id_glm_lpdf(Y | Xc, mu, b, sigma);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(to_vector(z_1));\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n  // compute group-level correlations\n  corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1);\n  vector&lt;lower=-1,upper=1&gt;[NC_1] cor_1;\n  // extract upper diagonal of correlation matrix\n  for (k in 1:M_1) {\n    for (j in 1:(k - 1)) {\n      cor_1[choose(k - 1, 2) + j] = Cor_1[j, k];\n    }\n  }\n}"
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#extract-stan-data",
    "href": "practice-sheets/11a-cheat-sheet.html#extract-stan-data",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Extract Stan data",
    "text": "Extract Stan data\nThis is the data passed to Stan. Useful for inspecting dimensions etc.\n\n\nToggle code\nbrms::standata(fit_MC) |&gt; names()\n\n\n [1] \"N\"          \"Y\"          \"K\"          \"Kc\"         \"X\"         \n [6] \"Z_1_1\"      \"Z_1_2\"      \"Z_1_3\"      \"Z_1_4\"      \"J_1\"       \n[11] \"N_1\"        \"M_1\"        \"NC_1\"       \"prior_only\""
  },
  {
    "objectID": "practice-sheets/11a-cheat-sheet.html#inspect-design-matrices",
    "href": "practice-sheets/11a-cheat-sheet.html#inspect-design-matrices",
    "title": "Cheat sheet: common things to do with BRMS",
    "section": "Inspect design matrices",
    "text": "Inspect design matrices\n\nPopulation-level effects\n\n\nToggle code\nX &lt;- brms::standata(fit_MC)$X\nX |&gt; head()\n\n\n  Intercept conditionreaction conditiondiscrimination\n1         1                 1                       0\n2         1                 1                       0\n3         1                 1                       0\n4         1                 1                       0\n5         1                 1                       0\n6         1                 1                       0\n\n\n\n\nGroup-level effects\nThe group-level design matrix is spread out over different variables (all names Z_ followed by some indices), but retrievable like so:\n\n\nToggle code\ndata4Stan &lt;- brms::standata(fit_MC)\nZ &lt;- data4Stan[str_detect(data4Stan |&gt; names(), \"Z_\")] |&gt; as_tibble()\nZ\n\n\n# A tibble: 2,519 × 4\n       Z_1_1     Z_1_2     Z_1_3     Z_1_4\n   &lt;dbl[1d]&gt; &lt;dbl[1d]&gt; &lt;dbl[1d]&gt; &lt;dbl[1d]&gt;\n 1         1         1         0         0\n 2         1         1         0         1\n 3         1         1         0         1\n 4         1         1         0         1\n 5         1         1         0         0\n 6         1         1         0         1\n 7         1         1         0         1\n 8         1         1         0         0\n 9         1         1         0         1\n10         1         1         0         0\n# ℹ 2,509 more rows"
  },
  {
    "objectID": "practice-sheets/04c-GLM-predictives.html",
    "href": "practice-sheets/04c-GLM-predictives.html",
    "title": "Predictions from generalized linear models",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}"
  },
  {
    "objectID": "practice-sheets/04c-GLM-predictives.html#posterior-predictives",
    "href": "practice-sheets/04c-GLM-predictives.html#posterior-predictives",
    "title": "Predictions from generalized linear models",
    "section": "Posterior predictives",
    "text": "Posterior predictives\nHere is an example for a logistic regression model (where all the three measures clearly show their conceptual difference). Fit the model to some data first (here: predicting accuracy for two categorical factors with two levels each):\n\n\nToggle code\nfit_MT_logistic &lt;- \n  brms::brm(\n    formula = correct ~ group * condition,\n    data    = aida::data_MT,\n    family  = brms::bernoulli()\n  )\n\n\nThe posterior predictive (in the most general sense) makes predictions about the to-be-expected data, here a Boolean value of whether a response was correct.\n\n\nToggle code\n# 2 samples from the predictive distribution (data samples)\ndata_MT |&gt; \n  select(group, condition) |&gt; \n  unique() |&gt; \n  tidybayes::add_predicted_draws(\n    fit_MT_logistic,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .prediction\n  &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;int&gt;\n1 touch Atypical      1     NA         NA     1           1\n2 touch Atypical      1     NA         NA     2           1\n3 touch Typical       2     NA         NA     1           1\n4 touch Typical       2     NA         NA     2           0\n5 click Atypical      3     NA         NA     1           1\n6 click Atypical      3     NA         NA     2           1\n7 click Typical       4     NA         NA     1           1\n8 click Typical       4     NA         NA     2           1\n\n\nA predicted central tendency for this logistic model is a probability of giving a correct answer.\n\n\nToggle code\n# 2 samples from the predicted central tendency\ndata_MT |&gt; \n  select(group, condition) |&gt; \n  unique() |&gt; \n  tidybayes::add_epred_draws(\n    fit_MT_logistic,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .epred\n  &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1 touch Atypical      1     NA         NA     1  0.935\n2 touch Atypical      1     NA         NA     2  0.918\n3 touch Typical       2     NA         NA     1  0.938\n4 touch Typical       2     NA         NA     2  0.936\n5 click Atypical      3     NA         NA     1  0.885\n6 click Atypical      3     NA         NA     2  0.849\n7 click Typical       4     NA         NA     1  0.964\n8 click Typical       4     NA         NA     2  0.950\n\n\nPredictions at the linear predictor level are sometimes not so easy to interpret. The interpretation depends on the kind of link function used (more on this under the topic of “generalized linear models”). For a logistic regression, this number is a log-odds ratio (which determines the predicted correctness-probability).\n\n\nToggle code\n# 2 samples for the linear predictor\ndata_MT |&gt; \n  select(group, condition) |&gt; \n  unique() |&gt; \n  tidybayes::add_linpred_draws(\n    fit_MT_logistic,\n    ndraws = 2\n    )\n\n\n# A tibble: 8 × 7\n# Groups:   group, condition, .row [4]\n  group condition  .row .chain .iteration .draw .linpred\n  &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n1 touch Atypical      1     NA         NA     1     2.10\n2 touch Atypical      1     NA         NA     2     2.29\n3 touch Typical       2     NA         NA     1     2.74\n4 touch Typical       2     NA         NA     2     2.55\n5 click Atypical      3     NA         NA     1     1.89\n6 click Atypical      3     NA         NA     2     2.33\n7 click Typical       4     NA         NA     1     3.46\n8 click Typical       4     NA         NA     2     3.52"
  },
  {
    "objectID": "practice-sheets/02b-catPreds-exercises.html",
    "href": "practice-sheets/02b-catPreds-exercises.html",
    "title": "02c: Categorical predictors (exercises)",
    "section": "",
    "text": "Preamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nToggle code\ndolphin &lt;- aida::data_MT\n\n\n\n\nExercise 1: Regression w/ multiple categorical predictors\nWe want to regress log(RT) against the full combination of categorical factors group, condition, and prototype_label.\nlog(RT) ~ group * condition * prototype_label\nThe research hypotheses we would like to investigate are:\n\nTypical trials are faster than atypical ones.\nCoM trials are slower than the other kinds of trials (straight and curved) together, and respectively.\n‘straight’ trials are faster than ‘curved’ trials.\nClick trials are slower than touch trials.\n\nBut for this to work (without at least mildly informative priors), we would need to have a sufficient amount of observations in each cell. So, let’s check:\n\n\nToggle code\ndolphin |&gt;\n  mutate(group = as_factor(group),\n         condition = as_factor(condition),\n         prototype_label = as_factor(prototype_label)) |&gt;\n  count(group, condition, prototype_label, .drop = FALSE) |&gt;\n  arrange(n)\n\n\n# A tibble: 20 × 4\n   group condition prototype_label     n\n   &lt;fct&gt; &lt;fct&gt;     &lt;fct&gt;           &lt;int&gt;\n 1 touch Atypical  dCoM2               0\n 2 touch Typical   dCoM2               0\n 3 touch Atypical  dCoM                9\n 4 click Atypical  dCoM2              11\n 5 click Typical   dCoM2              11\n 6 touch Typical   dCoM               14\n 7 touch Atypical  cCoM               21\n 8 touch Typical   cCoM               31\n 9 click Atypical  cCoM               32\n10 click Atypical  curved             36\n11 touch Atypical  curved             37\n12 click Typical   cCoM               48\n13 click Atypical  dCoM               50\n14 click Typical   dCoM               52\n15 touch Typical   curved             72\n16 click Typical   curved             84\n17 click Atypical  straight          189\n18 touch Atypical  straight          263\n19 click Typical   straight          494\n20 touch Typical   straight          598\n\n\nSo, there are cells for which we have no observations at all. For simplicity, we therefore just lump all “change of mind”-type trajectories into one category:\n\n\nToggle code\ndolphin_prepped &lt;-\n  dolphin |&gt;\n  mutate(\n    prototype_label = case_when(\n     prototype_label %in% c('curved', 'straight') ~ prototype_label,\n     TRUE ~ 'CoM'\n    ),\n    prototype_label = factor(prototype_label,\n                             levels = c('straight', 'curved', 'CoM')))\n\ndolphin_prepped |&gt;\n  select(RT, prototype_label)\n\n\n# A tibble: 2,052 × 2\n      RT prototype_label\n   &lt;dbl&gt; &lt;fct&gt;          \n 1   950 straight       \n 2  1251 straight       \n 3   930 curved         \n 4   690 curved         \n 5   951 CoM            \n 6  1079 CoM            \n 7  1050 CoM            \n 8   830 straight       \n 9   700 straight       \n10   810 straight       \n# ℹ 2,042 more rows\n\n\nHere is a plot of the data to be analyzed:\n\n\nToggle code\ndolphin_prepped |&gt;\n  ggplot(aes(x = log(RT), fill = condition)) +\n  geom_density(alpha = 0.4) +\n  facet_grid(group ~ prototype_label)\n\n\n\n\n\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nUse brm() to run a linear regression model for the data set dolphin_prepped and the formula:\nlog(RT) ~ group * condition * prototype_label\nSet the prior for all population-level slope coefficients to a reasonable, weakly-informative but unbiased prior.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nfit &lt;- brm(\n  formula = log(RT) ~ group * condition * prototype_label,\n  prior   = prior(student_t(1, 0, 3), class = \"b\"),\n  data    = dolphin_prepped\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b\n\n\n\n\n\nPlot the posteriors for population-level slope coefficients using the tidybayes package in order to:\n\ndetermine which combination of factor levels is the default cell\ncheck which coefficients have 95% CIs that do not include zero\ntry to use this latter information to address any of our research hypotheses (stated above)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ntidybayes::summarise_draws(fit)\n\n\n# A tibble: 15 × 10\n   variable       mean   median      sd     mad       q5      q95  rhat ess_bulk\n   &lt;chr&gt;         &lt;num&gt;    &lt;num&gt;   &lt;num&gt;   &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n 1 b_Interce…  7.63e+0  7.63e+0 0.0337  0.0337   7.58e+0  7.69e+0  1.00    1953.\n 2 b_groupto… -2.39e-1 -2.39e-1 0.0442  0.0429  -3.12e-1 -1.67e-1  1.00    1772.\n 3 b_conditi… -2.46e-1 -2.46e-1 0.0390  0.0395  -3.10e-1 -1.81e-1  1.00    1776.\n 4 b_prototy… -5.14e-2 -5.07e-2 0.0823  0.0833  -1.88e-1  8.79e-2  1.00    1829.\n 5 b_prototy…  1.77e-1  1.78e-1 0.0574  0.0584   8.11e-2  2.68e-1  1.00    1981.\n 6 b_groupto…  1.25e-2  1.32e-2 0.0521  0.0513  -7.50e-2  9.85e-2  1.00    1660.\n 7 b_groupto…  1.34e-1  1.33e-1 0.115   0.115   -4.94e-2  3.21e-1  1.00    1847.\n 8 b_groupto…  3.34e-3  1.79e-3 0.106   0.107   -1.69e-1  1.76e-1  1.00    2288.\n 9 b_conditi…  3.16e-2  3.16e-2 0.0980  0.0993  -1.29e-1  1.95e-1  1.00    1852.\n10 b_conditi…  4.50e-2  4.53e-2 0.0743  0.0764  -7.51e-2  1.71e-1  1.00    1970.\n11 b_groupto… -1.37e-1 -1.37e-1 0.137   0.136   -3.59e-1  9.15e-2  1.00    1932.\n12 b_groupto…  5.35e-2  5.12e-2 0.137   0.137   -1.68e-1  2.78e-1  1.00    2117.\n13 sigma       4.60e-1  4.60e-1 0.00716 0.00703  4.49e-1  4.72e-1  1.00    4666.\n14 lprior     -2.79e+1 -2.79e+1 0.0129  0.0101  -2.79e+1 -2.79e+1  1.00    1574.\n15 lp__       -1.35e+3 -1.35e+3 2.55    2.47    -1.35e+3 -1.34e+3  1.00    1564.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\n\nToggle code\ntidybayes::gather_draws(fit, `b_.*`, regex = TRUE) |&gt;\n  filter(.variable != \"b_Intercept\") |&gt;\n  ggplot(aes(y = .variable, x = .value)) +\n  tidybayes::stat_halfeye() +\n  labs(x = \"\", y = \"\") +\n  geom_segment(x = 0, xend = 0, y = Inf, yend = -Inf,\n               lty = \"dashed\")\n\n\n\n\n\nThe default cell is for click-atypical-straight. The coeffiencents with 95% CIs that do not include zero are: grouptouch, conditionTypical, prototype_labelCoM. None of these give us direct information about our research hypotheses.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1c\n\n\n\n\n\nUse the faintr package to get information relevant for the current research hypotheses. Interpret each result with respect to what we may conclude from it.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# 1. Typical trials are faster than atypical ones.\n# -&gt; There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = condition == 'Typical',\n  higher = condition == 'Atypical'\n)\n\n\nOutcome of comparing groups: \n * higher:  condition == \"Atypical\" \n * lower:   condition == \"Typical\" \nMean 'higher - lower':  0.2284 \n95% HDI:  [ 0.17 ; 0.2894 ]\nP('higher - lower' &gt; 0):  1 \nPosterior odds:  Inf \n\n\nToggle code\n# 2. CoM trials are slower than the other kinds of trials\n#    (straight and curved) together, and respectively.\n# -&gt; There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label != 'CoM',\n  higher = prototype_label == 'CoM'\n)\n\n\nOutcome of comparing groups: \n * higher:  prototype_label == \"CoM\" \n * lower:   prototype_label != \"CoM\" \nMean 'higher - lower':  0.2158 \n95% HDI:  [ 0.1424 ; 0.2821 ]\nP('higher - lower' &gt; 0):  1 \nPosterior odds:  Inf \n\n\nToggle code\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label == 'straight',\n  higher = prototype_label == 'CoM'\n)\n\n\nOutcome of comparing groups: \n * higher:  prototype_label == \"CoM\" \n * lower:   prototype_label == \"straight\" \nMean 'higher - lower':  0.2143 \n95% HDI:  [ 0.1521 ; 0.2815 ]\nP('higher - lower' &gt; 0):  1 \nPosterior odds:  Inf \n\n\nToggle code\n# 3. 'straight' trials are faster than 'curved' trials.\n# -&gt; There is no evidence for this hypothesis\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = prototype_label == 'straight',\n  higher = prototype_label == 'curved'\n)\n\n\nOutcome of comparing groups: \n * higher:  prototype_label == \"curved\" \n * lower:   prototype_label == \"straight\" \nMean 'higher - lower':  -0.003049 \n95% HDI:  [ -0.06976 ; 0.06543 ]\nP('higher - lower' &gt; 0):  0.4655 \nPosterior odds:  0.8709 \n\n\nToggle code\n# 4. Click trials are slower than touch trials.\n# -&gt; There is overwhelming evidence that this is true\n#    (given the data and the model).\n\nfaintr::compare_groups(\n  fit,\n  lower  = group == 'touch',\n  higher = group == 'click'\n)\n\n\nOutcome of comparing groups: \n * higher:  group == \"click\" \n * lower:   group == \"touch\" \nMean 'higher - lower':  0.2014 \n95% HDI:  [ 0.1385 ; 0.2603 ]\nP('higher - lower' &gt; 0):  1 \nPosterior odds:  Inf \n\n\n\n\n\n\n\n\n\n\nExercise 2: Regression w/ metric & categorical predictors\n\n\n\n\n\n\nExercise 2a\n\n\n\n\n\nCreate a new data frame that contains only the mean values of the RT, and MAD for each animal (exemplar) and for correct and incorrect responses. Print out the head of the new data frame.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# aggregate\ndolphin_agg &lt;- dolphin |&gt; \n  group_by(exemplar, correct) |&gt; \n  dplyr::summarize(MAD = mean(MAD, na.rm = TRUE),\n                   RT = mean(RT, na.rm = TRUE))\n  \n# let's have a look\nhead(dolphin_agg)\n\n\n# A tibble: 6 × 4\n# Groups:   exemplar [3]\n  exemplar  correct   MAD    RT\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 alligator       0 271.  4246.\n2 alligator       1  87.4 1717.\n3 bat             0 176.  3334.\n4 bat             1 182.  2252.\n5 butterfly       0  15.0 1636.\n6 butterfly       1 145.  1761.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2b\n\n\n\n\n\nRun a linear regression using brms. MAD is the dependent variable (i.e. the measure) and both RT and correct are independent variables (MAD ~ RT + correct). (Hint: the coefficients might be really small, so make sure the output is printed with enough numbers after the comma.)\nTry to understand the coefficient table. There is one coefficient for RT and one coefficient for correct which gives you the change in MAD from incorrect to correct responses.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# specify the model \nmodel2 = brm(\n  # model formula\n  MAD ~ RT + correct, \n  # data\n  data = dolphin_agg\n  )\n\nprint(summary(model2), digits = 5)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MAD ~ RT + correct \n   Data: dolphin_agg (Number of observations: 36) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n           Estimate Est.Error  l-95% CI u-95% CI    Rhat Bulk_ESS Tail_ESS\nIntercept  21.43821  36.00766 -47.86667 92.78947 0.99987     4281     3191\nRT          0.06231   0.01541   0.03179  0.09370 1.00072     4111     3277\ncorrect   -15.08456  26.27864 -68.47390 36.01476 1.00147     4043     2838\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI    Rhat Bulk_ESS Tail_ESS\nsigma 77.51173   9.58571 61.48184 98.87038 1.00061     3640     3036\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2c\n\n\n\n\n\nPlot a scatter plot of MAD ~ RT and color code it for correct responses. (Hint: Make sure that correct is treated as a factor and not a numeric vector). Draw two predicted lines into the scatterplot. One for correct responses (“lightblue”) and one for incorrect responses (“orange”).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_agg$correct &lt;- as.factor(as.character(dolphin_agg$correct))\n\n# extract model parameters:\nmodel_intercept &lt;- summary(model2)$fixed[1,1]\nmodel_RT &lt;- summary(model2)$fixed[2,1]\nmodel_correct &lt;- summary(model2)$fixed[3,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = RT, \n           y = MAD,\n           color = correct)) + \n  geom_abline(intercept = model_intercept, slope = model_RT, color = \"orange\", size  = 2) +\n  geom_abline(intercept = model_intercept + model_correct , slope = model_RT, color = \"lightblue\",size  = 2) +\n  geom_point(size = 3, alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2d\n\n\n\n\n\nExtract the posteriors for the coefficients of both RT and correct from the model output (use the spread_draws() function), calculate their means and a 67% Credible Interval. Print out the head of the aggregated dataframe.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# get posteriors for the relevant coefficients\nposteriors2 &lt;- model2 |&gt;\n  spread_draws(b_RT, b_correct) |&gt;\n  select(b_RT, b_correct) |&gt; \n  gather(key = \"parameter\", value = \"posterior\")\n\n# aggregate\nposteriors2_agg &lt;- posteriors2 |&gt; \n  group_by(parameter) |&gt; \n  summarise(mean_posterior = mean(posterior),\n            `67lowerCrI` = HDInterval::hdi(posterior, credMass = 0.67)[1],\n            `67higherCrI` = HDInterval::hdi(posterior, credMass = 0.67)[2]\n            )\n\n# print out\nposteriors2_agg\n\n\n# A tibble: 2 × 4\n  parameter mean_posterior `67lowerCrI` `67higherCrI`\n  &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n1 b_RT              0.0623       0.0473        0.0762\n2 b_correct       -15.1        -39.0          11.0   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2e\n\n\n\n\n\nPlot the scatterplot from 2c and plot 50 sample tuples for the regression lines for correct and incorrect responses.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# sample 50 random numbers from 4000 samples\nrandom_50 &lt;- sample(1:4000, 50, replace = FALSE)\n  \n# wrangle data frame\nposteriors3 &lt;- model2 |&gt;\n  spread_draws(b_Intercept, b_RT, b_correct) |&gt;\n  select(b_Intercept, b_RT, b_correct) |&gt; \n  # filter by the row numbers in random_50\n  slice(random_50)\n  \n# plot\nggplot(data = dolphin_agg, \n       aes(x = RT, \n           y = MAD, \n           color = correct)) + \n  geom_abline(data = posteriors3,\n              aes(intercept = b_Intercept, slope = b_RT), \n              color = \"orange\", size  = 0.1) +\n  geom_abline(data = posteriors3,\n              aes(intercept = b_Intercept + b_correct, slope = b_RT), \n              color = \"lightblue\", size  = 0.1) +\n  geom_point(size = 3, alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2f\n\n\n\n\n\nGiven our model and our data, calculate the evidence ratio of correct responses exhibiting larger MADs than incorrect responses. How would you interpret the result?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nhypothesis(model2, 'correct &gt; 0')\n\n\nHypothesis Tests for class b:\n     Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1 (correct) &gt; 0   -15.08     26.28   -58.72    27.46       0.39      0.28     \n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\n\n\n\n\n\n\n\n\nExercise 3: Metric and categorical variables, and their interaction\nHere is an aggregated data set dolphin_agg for you.\n\n\nToggle code\n# aggregate\ndolphin_agg &lt;- dolphin %&gt;% \n  group_by(group, exemplar) %&gt;% \n  dplyr::summarize(MAD = median(MAD, na.rm = TRUE),\n                   RT = median(RT, na.rm = TRUE)) %&gt;% \n  mutate(log_RT = log(RT))\n\n\n\n\n\n\n\n\nExercise 3a\n\n\n\n\n\nStandardize (“z-transform”) log_RT such that the mean is at zero and 1 unit corresponds to the standard deviation. Name it log_RT_s. (Hint: use function scale().)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_agg$log_RT_s &lt;- scale(dolphin_agg$log_RT, scale = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3b\n\n\n\n\n\nRun a linear model with brms that predicts MAD based on log_RT_s, group, and their two-way interaction.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nmodel1 = brm(\n  MAD ~ log_RT_s * group, \n  data = dolphin_agg\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3c\n\n\n\n\n\nPlot MAD (y) against log_RT_s (x) in a scatter plot and color-code for group. Plot the regression lines for the click and the touch group into the plot and don’t forget to take possible interactions into account.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# extract posterior means for model coefficients\nIntercept = summary(model1)$fixed[1,1]\nlog_RT = summary(model1)$fixed[2,1]\ngroup = summary(model1)$fixed[3,1]\ninteraction = summary(model1)$fixed[4,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = log_RT_s, \n           y = MAD, \n           color = group)) + \n  geom_point(size = 3, alpha = 0.3) +\n  geom_vline(xintercept = 0, lty = \"dashed\") +\n  geom_abline(intercept = Intercept, slope = log_RT, \n              color = project_colors[1], size = 2) +\n  geom_abline(intercept = Intercept + group, slope = log_RT + interaction, \n              color = project_colors[2], size = 2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3d\n\n\n\n\n\nSpecify very skeptic priors for all three coefficients. Use a normal distribution with mean = 0, and sd = 10. Rerun the model with those priors.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# specify priors\npriors_model2 &lt;- c(\n  set_prior(\"normal(0,10)\", class = \"b\", coef = \"log_RT_s\"),\n  set_prior(\"normal(0,10)\", class = \"b\", coef = \"grouptouch\"),\n  set_prior(\"normal(0,10)\", class = \"b\", coef = \"log_RT_s:grouptouch\")\n)\n\n# model\nmodel2 = brm(\n  MAD ~ log_RT_s * group, \n  data = dolphin_agg,\n  prior = priors_model2\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3e\n\n\n\n\n\nCompare the model output of model1 to model2. What are the differences and what are the reasons for these differences?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# We can compare the model predictions by looking at the coefficients / plotting them:\nsummary(model1)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MAD ~ log_RT_s * group \n   Data: dolphin_agg (Number of observations: 38) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              66.56      7.64    51.96    82.15 1.00     2796     2323\nlog_RT_s               16.59      7.64     1.38    31.49 1.00     2262     2240\ngrouptouch            -35.46     10.95   -57.49   -14.08 1.00     3493     2792\nlog_RT_s:grouptouch   -11.24     11.30   -33.22    11.26 1.00     2513     2414\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    27.65      3.38    21.90    34.97 1.00     3162     2551\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nToggle code\nsummary(model2)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MAD ~ log_RT_s * group \n   Data: dolphin_agg (Number of observations: 38) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              60.33      6.18    48.20    72.36 1.00     4377     2544\nlog_RT_s               13.71      5.25     3.46    24.08 1.00     3590     2974\ngrouptouch            -17.79      7.38   -32.08    -2.88 1.00     4320     2518\nlog_RT_s:grouptouch    -0.96      7.23   -14.97    13.30 1.00     4071     3225\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    28.53      3.51    22.64    36.08 1.00     3855     2858\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nToggle code\n# extract posterior means for model coefficients\nIntercept = summary(model2)$fixed[1,1]\nlog_RT = summary(model2)$fixed[2,1]\ngroup = summary(model2)$fixed[3,1]\ninteraction = summary(model2)$fixed[4,1]\n\n# plot\nggplot(data = dolphin_agg, \n       aes(x = log_RT_s, \n           y = MAD, \n           color = group)) + \n  geom_point(size = 3, alpha = 0.3) +\n  geom_vline(xintercept = 0, lty = \"dashed\") +\n  geom_abline(intercept = Intercept, slope = log_RT, \n              color = project_colors[1], size = 2) +\n  geom_abline(intercept = Intercept + group, slope = log_RT + interaction, \n              color = project_colors[1], size = 2) \n\n\n\n\n\nThe magnitude of the coefficients is much smaller in model2, with the interaction term being close to zero. As a result, the lines in the plot are closer together and run in parallel. The reason for this change lies in the priors. We defined the priors of model2 rather narrowly, down-weighing data points larger or smaller than zero. This is a case of the prior dominating the posterior."
  },
  {
    "objectID": "practice-sheets/06a-model-comparison.html",
    "href": "practice-sheets/06a-model-comparison.html",
    "title": "06: Model comparison",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nToggle code\ndolphin &lt;- aida::data_MT\nrerun_models = FALSE"
  },
  {
    "objectID": "practice-sheets/06a-model-comparison.html#normal-and-robust-regression-models",
    "href": "practice-sheets/06a-model-comparison.html#normal-and-robust-regression-models",
    "title": "06: Model comparison",
    "section": "Normal and robust regression models",
    "text": "Normal and robust regression models\nA normal regression model uses a normal error function.\n\n\nToggle code\nfit_n &lt;- brm(\n  formula = y ~ x,\n  data = data_robust,\n  # student prior for slope coefficient\n  prior = prior(\"student_t(1,0,30)\", class = \"b\"),\n)\n\n\nWe will want to compare this normal regression model with a robust regression model, which uses a Student’s t distribution instead as the error function around the linear predictor:\n\n\nToggle code\nfit_r &lt;- brm(\n  formula = y ~ x,\n  data = data_robust,\n  # student prior for slope coefficient\n  prior = prior(\"student_t(1,0,30)\", class = \"b\"),\n  family = student()\n)\n\n\nLet’s look at the posterior inferences of both models about the true (known) parameters of the regression line:\n\n\nToggle code\nprep_summary &lt;- function(fit, model) {\n  tidybayes::summarise_draws(fit) |&gt; \n    mutate(model = model) |&gt; \n    select(model, variable, q5, mean, q95) |&gt; \n    filter(grepl(variable, pattern = '^b'))  \n}\n\nrbind(prep_summary(fit_n, \"normal\"), prep_summary(fit_r, \"robust\"))\n\n\n# A tibble: 4 x 5\n  model  variable       q5  mean   q95\n  &lt;chr&gt;  &lt;chr&gt;       &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1 normal b_Intercept  2.31  7.54 13.0 \n2 normal b_x          7.22 13.4  19.4 \n3 robust b_Intercept  1.81  2.49  3.24\n4 robust b_x          4.98  6.09  7.23\n\n\nRemember that the true intercept is 2 and the true slope is 4. Clearly the robust regression model has recovered the ground-truth parameters much better."
  },
  {
    "objectID": "practice-sheets/06a-model-comparison.html#leave-one-out-cross-validation",
    "href": "practice-sheets/06a-model-comparison.html#leave-one-out-cross-validation",
    "title": "06: Model comparison",
    "section": "Leave-one-out cross validation",
    "text": "Leave-one-out cross validation\nWe can use the loo package to compare these two models based on their posterior predictive fit. Here’s how:\n\n\nToggle code\nloo_comp &lt;- loo_compare(list(normal = loo(fit_n), robust = loo(fit_r)))\nloo_comp\n\n\n       elpd_diff se_diff\nrobust    0.0       0.0 \nnormal -131.7      26.0 \n\n\nWe see that the robust regression model is better by ca. -132 points of expected log predictive density. The table shown above is ordered with the “best” model on top. The column elpd_diff lists the difference in ELPD of every model to the “best” one. In our case, th estimated ELPD difference has a standard error of about 26. Computing a \\(p\\)-value for this using Lambert’s \\(z\\)-score method, we find that this difference is “significant” (for which we will use other terms like “noteworthy” or “substantial” in the following):\n\n\nToggle code\n1 - pnorm(-loo_comp[2,1], loo_comp[2,2])\n\n\n[1] 0\n\n\nWe conclude from this that the robust regression model is much better at predicting the data (from a posterior point of view)."
  },
  {
    "objectID": "practice-sheets/06a-model-comparison.html#bayes-factor-model-comparison-with-bridge-sampling",
    "href": "practice-sheets/06a-model-comparison.html#bayes-factor-model-comparison-with-bridge-sampling",
    "title": "06: Model comparison",
    "section": "Bayes factor model comparison (with bridge sampling)",
    "text": "Bayes factor model comparison (with bridge sampling)\nWe use bridge sampling, as implemented in the formidable bridgesampling package, to estimate the (log) marginal likelihood of each model. To do this, we need also samples from the prior. To do this reliably, we need many more samples than we would normally need for posterior inference. We can update() existing fitted models, so that we do not have to copy-paste all specifications (formula, data, prior, …) each time. It’s important for bridge_sampler() to work that we save all parameters (including prior samples).\n\n\nToggle code\nif (rerun_models) {\n  # refit normal model\n  fit_n_4Bridge &lt;- update(\n    fit_n,\n    iter = 5e5,\n    save_pars = save_pars(all = TRUE)\n  )\n  # refit robust model\n  fit_r_4Bridge &lt;- update(\n    fit_r,\n    iter = 5e5,\n    save_pars = save_pars(all = TRUE)\n  )\n  normal_bridge &lt;- bridge_sampler(fit_n_4Bridge, silent = T)\n  write_rds(normal_bridge, \"06-normal_bridge.rds\")\n  robust_bridge &lt;- bridge_sampler(fit_r_4Bridge, silent = T)  \n  write_rds(robust_bridge, \"06-robust_bridge.rds\")\n} else {\n  normal_bridge &lt;- read_rds(\"06-normal_bridge.rds\")  \n  robust_bridge &lt;- read_rds(\"06-robust_bridge.rds\")\n}\n\nbf_bridge &lt;- bridgesampling::bf(robust_bridge, normal_bridge)\n\n\nWe can then use the bf (Bayes factor) method from the bridgesampling package to get the Bayes factor (here: in favor of the robust regression model):\n\n\nToggle code\nbf_bridge\n\n\nEstimated Bayes factor in favor of robust_bridge over normal_bridge: 41136407426471809154394622543225576928422395904.00000\n\n\nAs you can see, this is a very clear result. If we had equal levels of credence in both models, after seeing the data, our degree of belief in the robust regression model should … well, virtually infinitely higer than our degree of belief in the normal model."
  },
  {
    "objectID": "practice-sheets/03d-MLM-pooling.html",
    "href": "practice-sheets/03d-MLM-pooling.html",
    "title": "Group-level effects, pooling & smoothing",
    "section": "",
    "text": "We can motivate the inclusion of group-level effects in terms of otherwise violated independence assumptions (the reaction times of a single individual are not necessarily independent of each other; some individuals are just slower or faster than others tout court). We can also motivate group-level effects by appeal to their effect of attenuating inference by flexibly weighing information from different groups. A good example of this latter effect arises when the number of observations in each group is not the same. Let’s go and explore!\n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nDifferent ways to a mean\nWe will use the data set of (log) radon measurements that ships with the rstanarm package.\n\n\nToggle code\ndata_radon &lt;- rstanarm::radon |&gt; as_tibble()\ndata_radon\n\n\n# A tibble: 919 x 4\n   floor county log_radon log_uranium\n   &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1     1 AITKIN    0.833       -0.689\n 2     0 AITKIN    0.833       -0.689\n 3     0 AITKIN    1.10        -0.689\n 4     0 AITKIN    0.0953      -0.689\n 5     0 ANOKA     1.16        -0.847\n 6     0 ANOKA     0.956       -0.847\n 7     0 ANOKA     0.470       -0.847\n 8     0 ANOKA     0.0953      -0.847\n 9     0 ANOKA    -0.223       -0.847\n10     0 ANOKA     0.262       -0.847\n# i 909 more rows\n\n\nSuppose that we are interested in an estimate of the mean log-radon measured. Easy! We can just take the empirical mean of each measurement:\n\n\nToggle code\n# empirical mean: 1.265\nempirical_mean &lt;- data_radon |&gt; pull(log_radon) |&gt; mean()\nempirical_mean\n\n\n[1] 1.264779\n\n\nBut should we not, somehow, also take into account that these measurements are from different counties? Okay, so let’s just calculate the mean log-random measured for each county, and then take the mean of all those means:\n\n\nToggle code\n# empirical mean of means: 1.38\nemp_mean_of_means &lt;- data_radon |&gt; \n  group_by(county) |&gt; \n  summarize(mean_by_county = mean(log_radon)) |&gt; \n  ungroup() |&gt; \n  pull(mean_by_county) |&gt; \n  mean()\nemp_mean_of_means\n\n\n[1] 1.37991\n\n\nAha! There is a difference between the empirical mean (sample mean; mean of all data points) and the mean-of-means (a.k.a., grand mean). This is because there are different numbers of observations for each county:\n\n\nToggle code\ndata_radon |&gt; \n  group_by(county) |&gt; \n  summarize(mean_by_county = mean(log_radon),\n            n_obs_by_county = n())\n\n\n# A tibble: 85 x 3\n   county    mean_by_county n_obs_by_county\n   &lt;fct&gt;              &lt;dbl&gt;           &lt;int&gt;\n 1 AITKIN             0.715               4\n 2 ANOKA              0.891              52\n 3 BECKER             1.09                3\n 4 BELTRAMI           1.19                7\n 5 BENTON             1.28                4\n 6 BIGSTONE           1.54                3\n 7 BLUEEARTH          1.93               14\n 8 BROWN              1.65                4\n 9 CARLTON            0.977              10\n10 CARVER             1.22                6\n# i 75 more rows\n\n\nThe sample mean does not distinguish at all which observation came from which county. The mean-of-means, on the other hand, puts “counties first”, so to speak, and does not acknowledge that the number of observations each county contributed might be different. To understand how this can yield a difference, look at this picture:\n\n\nToggle code\ndata_radon |&gt; \n  group_by(county) |&gt; \n  summarize(mean_by_county = mean(log_radon),\n            n_obs_by_county = n()) |&gt;\n  mutate(county = fct_reorder(county, mean_by_county)) |&gt; \n  ggplot(aes(x = mean_by_county, y = county)) +\n  theme(legend.position=\"none\") +\n  xlab(\"mean log-radon\") + ylab(\"\") +\n  geom_vline(aes(xintercept = empirical_mean), color = project_colors[2], size = 1.5) + \n  geom_vline(aes(xintercept = emp_mean_of_means), color = project_colors[3], size = 1.5) +\n  geom_point(aes(size = n_obs_by_county), alpha = 0.7) +\n  annotate(\"text\", x = empirical_mean - 0.4, y = 70, \n           label = \"sample mean\", color = project_colors[2], size = 7) +\n  annotate(\"text\", x = emp_mean_of_means + 0.52, y = 20, \n           label = \"mean-of-means\", color = project_colors[3], size = 7) +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\n\nThe \\(x\\)-position of the dots represents the mean for each county. The size of the dots represents the number of observations for each county. The sample mean (in red) is lower because some “heavier dots pull it to the left”. Reversely, the mean-of-means (in yellow) is “pulled more towards the right by the lighter dots” (since it doesn’t care about the size of the dots).\nWhich measure is correct? Neither! Or better: both! Or actually: it depends … on what we want. But actually: maybe we should let the data decide.\n\n\nGroup-level effects as smoothing terms\nSuppose we want a Bayesian measure (with quantified uncertainty) of the sample mean and the mean-of-means, how would we do it? (Maybe you want to think about this for a moment, before you uncover the solution below!)\n\n\n\n\n\n\nExercise\n\n\n\n\n\nRetrieve a Bayesian estimate of the sample mean and of the mean-of-means for the log-randon measure.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sample mean is estimable with an intercept-only model.\n\n\nToggle code\nfit_InterOnly &lt;- brms::brm(\n  formula = log_radon ~ 1,\n  data    = data_radon\n)\n\n\nThe Bayesian estimate of the sample mean is given by the intercept term:\n\n\nToggle code\nsample_mean_estimate &lt;- \n  fit_InterOnly |&gt; tidybayes::tidy_draws() |&gt; \n  pull(b_Intercept) |&gt; \n  aida::summarize_sample_vector(name = \"sample mean\")\nsample_mean_estimate\n\n\n# A tibble: 1 x 4\n  Parameter   `|95%`  mean `95%|`\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 sample mean   1.21  1.26   1.32\n\n\nThe mean-of-means can be estimated by running a regression model with county as population-level effect.\n\n\nToggle code\n# We need quite high iterations for this to fit properly \n# (likely to do the few observations in many of the groups).\nfit_FE &lt;- brms::brm(\n  formula = log_radon ~ county,\n  prior   = prior(student_t(1,0,10)),\n  iter    = 20000,\n  thin    = 5,\n  data    = data_radon\n)\n\n\nThis model yields estimates for the mean of each county. (There is some data wrangling to do to get at them, given the way categorical factors are treated internally, but that is a different matter). If we average the estimates properly (here using helper functions from the faintr package), we get an estimate of the mean-of-means:\n\n\nToggle code\n# estimated sample mean: 1.381\nmeanOmean_estimate &lt;- \n  # obtain samples from the grand mean\n  faintr::filter_cell_draws(fit_FE, colname = \"draws\") |&gt; \n  pull(draws) |&gt; \n  # summarize them\n  aida::summarize_sample_vector(name = \"mean-of-means\")\nrbind(sample_mean_estimate, meanOmean_estimate)\n\n\n# A tibble: 2 x 4\n  Parameter     `|95%`  mean `95%|`\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 sample mean     1.21  1.26   1.32\n2 mean-of-means   1.30  1.38   1.46\n\n\n\n\n\n\n\n\nIn between these two options (no county variable vs. county as a population-level effect), there is a middle path: treating county as a group-level random intercept.\n\n\nToggle code\nfit_RE &lt;- brms::brm(\n  formula = log_radon ~ (1 | county),\n  data    = data_radon\n)\n\n\nIn this model, the intercept term is an estimate of the mean, but it is in between the estimated sample mean and the estimated mean-of-means:\n\n\nToggle code\npooled_mean &lt;- fit_RE |&gt; tidybayes::tidy_draws() |&gt; \n  pull(\"b_Intercept\") |&gt; \n  aida::summarize_sample_vector(name = \"pooled mean\")\nrbind(sample_mean_estimate, meanOmean_estimate, pooled_mean)\n\n\n# A tibble: 3 x 4\n  Parameter     `|95%`  mean `95%|`\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 sample mean     1.21  1.26   1.32\n2 mean-of-means   1.30  1.38   1.46\n3 pooled mean     1.25  1.35   1.44\n\n\nThis estimate is nuanced. It does take all data points into account (unlike the mean-of-mean). But (unlike the sample mean), it does weigh some data observations more heavily than others. In particular, counties with few but extreme observations receive, so to speak, less attention because “we explain away these observations as flukes for a given county”. In other words, we can think of group-level modeling also as a way of regularizing inference to differentially weigh observations, depending on which group they originated from.\nOkay, this may sound like a plausible rationalization of what one could do, but how do we know that the model actually behaves in this way? – By looking at what the model would predict for different counties. So, here is a plot of the a posteriori expected measurement for each county (ordered by size) together with the empirically observed mean-by-county:\n\n\nToggle code\nplotData_RE &lt;- data_radon |&gt; \n  group_by(county) |&gt; \n  summarize(mean_by_county = mean(log_radon),\n            n_observations = n()) |&gt; \n  tidybayes::add_epred_draws(\n    object = fit_RE\n  ) |&gt; \n  group_by(county, mean_by_county, n_observations) |&gt; \n  summarize(\n    prediction_lower = tidybayes::hdi(.epred)[1],\n    prediction_mean = mean(.epred),\n    prediction_higher = tidybayes::hdi(.epred)[2]\n    ) |&gt; \n  ungroup() |&gt; \n  mutate(county = fct_reorder(county, n_observations))\n\nplotData_RE |&gt; \n  ggplot(aes(x = prediction_mean , y = county), size = 2) +\n  geom_point(aes(x = mean_by_county), color = project_colors[2]) +\n  geom_errorbar(aes(xmin = prediction_lower, xmax = prediction_higher), \n                color = \"gray\", alpha = 0.8) +\n  geom_segment(aes(y = county, yend=county, x = mean_by_county, xend=prediction_mean),\n              color = project_colors[2]) +\n  geom_point() +\n  ylab(\"\") +\n  xlab(\"mean log-radon\")\n\n\n\n\n\nThis graph shows the counties on the \\(y\\)-axis, ordered number of observation from highest on the top to lowest on the bottom. The black dots are the means of the posterior predictive for each county (the gray error bars are 95% credible intervals for these estimates. The red dots are the empirically observed means for each county (the red lines indicating the differences between prediction and observation for each county).\nThis graph shows:\n\nThe higher the number of observations, the less uncertainty about the prediction.\nThe higher the number of observations, the less difference between prediction and observation.\n\nIt is the latter observation that lends credence to the interpretation above: the effect of random effects is differential weighing of observations in a data-driven manner; low certainty cases receive less weight, as they should."
  },
  {
    "objectID": "practice-sheets/10c-Gaussian-processes.html",
    "href": "practice-sheets/10c-Gaussian-processes.html",
    "title": "Gaussian process regression in brms",
    "section": "",
    "text": "This tutorial provides both a brief conceptual introduction into Gaussian process regression. It develops intuitions about how, from a generalization of multi-variate normal distributions, we can obtain something like a “prior over functions”. It also demonstrates how a Gaussian process regression can be implemented in brms.\n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nGaussian processes\nA Gaussian process is a generalization of a multi-variate normal distribution, which in turn is a generalization of a simple normal distribution. A simple normal distribution provides a likelihood for a single response variable \\(y\\) based on a pair of single numbers for mean \\(x\\) and standard deviation \\(\\sigma\\):\n\\[\ny \\sim \\mathcal{N}(x, \\sigma)\n\\]\nA multivariate Gaussian extends this to provide a probability of a vector \\(\\mathbf{y}\\) of \\(k\\) finite numbers for a \\(k\\)-place vector of means \\(\\mathbf{x}\\) and a \\(k \\times k\\) covariance matrix \\(\\Sigma\\):\n\\[\n\\mathbf{y} \\sim \\text{MV-Normal}(\\mathbf{x}, \\Sigma)\n\\]\nOne heuristic way of thinking of a Gaussian process is as a whole (infinite) set of systematically related multi-variate normal distributions. For each vector \\(\\mathbf{x}\\) of means, with arbitrary finite length \\(k\\), this set contains a multi-variate normal distribution for us. So, we are not stuck with one specific \\(k\\), but we have exactly one for each \\(\\mathbf{x}\\) no matter what \\(k\\), as long as it is finite.\nTo obtain such a set, we construct a function which, for a given \\(\\mathbf{x}\\), constructs a covariance matrix \\(\\Sigma\\) for us in a systematic way. This is done via a so-called kernel. The kernel is what gives us the “systematicity” in our set of multi-variate normal distributions. (It also regulates the overal shape of functions implied by the Gaussian process; more on this below.)\nThere are many different useful kernels, but the most salient one is perhaps the radial basis function kernel. It is defined as follows:\n\\[\nk(\\mathbf{x},\\mathbf{x}') = \\sigma_f^2 \\ \\exp \\left [ - \\frac{||\\mathbf{x} - \\mathbf{x}'||^2}{2 \\lambda}\\right]\n\\]\nHere \\(||\\cdot||\\) is the Euclidean norm, defined as \\(||\\mathbf{x}|| = \\sqrt{x_1^2 + \\dots + x_k^2}\\), an expression of the length of a vector. There are two parameters in the radial basis kernel:\n\n\\(\\sigma_f\\) is signal variance, and\n\\(\\lambda\\) is the *length scale.\n\nFor a given vector \\(\\mathbf{x}\\), we can use the kernel to construct finite multi-variate normal distribution associated with it like so:\n\\[\n\\mathbf{x} \\mapsto_{GP} \\text{MV-Norm}(m(\\mathbf{x}), k(\\mathbf{x},  \\mathbf{x}))\n\\]\nwhere \\(m\\) is a function that specifies the mean for the distribution associated with \\(\\mathbf{x}\\). This mapping is essentially the Gaussian process: a systematic association of vectors of arbitrary length with a suitable multi-variate normal distribution.\n\n\nA compact prior over functions\nCool, but why do we care? We care because a Gaussian process (GP) allows us to specify a vast amount of non-linear curves, so to speak. More concretely, a GP, defined by a kernel \\(k(\\cdot)\\), a mean function \\(m(\\cdot)\\), and the parameters \\(\\sigma_f\\) and \\(\\lambda\\), implies a prior of functions. This is very abstract and best explored through simulation.\nHere are two convenience functions. The first is called get_GP_simulation and it samples from a Gaussian process regression. It takes as input a vector x to generated predictions for, values for the kernel parameters sigma_f and lambda and also the usual simple linear regression parameters Intercept, slope and sigma. The second function is called plot_GP_simulation takes as input what the first function delivers and provides a plot.\nUsing these functions we can explore how we can “generate wiggly lines” for different input vectors x and parameter settings.\n\n\nToggle code\nget_GP_simulation &lt;- \n  function(x = seq(0,10, by = 0.1), \n           Intercept = 0, \n           slope = 1, \n           sigma = 1, \n           sigma_f=0.5, \n           lambda=100, \n           seed = NULL) {\n    \n    if (! is.null(seed)){\n      set.seed(seed)\n    }\n    \n    # number of points to generate prediction for\n    N &lt;- length(x)\n    \n    # linear predictor (vanilla LM)\n    eta = Intercept + slope * x\n    \n    # kernel function (here: radial basis function kernel)\n    get_covmatrix &lt;- function(x, sigma_f, lambda) {\n      K = matrix(0, nrow=N, ncol=N)\n      for (i in 1:N) {\n        for (j in 1:N) {\n          K[i,j] = sigma_f^2 * exp(sqrt((i-j)^2) / (-2 *lambda))\n        }\n      }\n      return(K)\n    }\n    \n    # covariance matrix\n    K &lt;- get_covmatrix(x, sigma_f, lambda)\n    \n    # Gaussian process wiggles\n    epsilon_gp &lt;- mvtnorm::rmvnorm(\n      n     = 1, \n      mean  = rep(0, N),\n      sigma = K)[1,]\n    \n    # central tendency\n    mu &lt;- epsilon_gp + eta\n    \n    # data prediction\n    y &lt;- rnorm(N, mu, sigma)\n      \n    tibble(x, eta, mu, y) \n  }\n\nplot_GP_simulation &lt;- function(GP_simulation) {\n  GP_simulation |&gt; \n    ggplot(aes(x = x, y = eta )) + \n    geom_line(color = project_colors[1], size = 1.25) +\n    geom_line(aes(y = mu), color = project_colors[2], size = 1.25) +\n    geom_point(aes(y = y), color = project_colors[3], alpha = 0.7, size = 1.2)\n}\n\n\nHere is an example:\n\n\nToggle code\nget_GP_simulation(\n  x = seq(-1,1, length.out = 500), \n  Intercept = 0, \n  slope = 0.3, \n  sigma_f = 0.05, \n  lambda  = 20, \n  sigma = 0.05) |&gt; \n  plot_GP_simulation()\n\n\n\n\n\nThe blue line is a regular, simple linear regression line, the linear predictor part, if you will. The red line is the predictor of central tendency, obtained by overlaying the linear predictor with “wiggles” sampled from the Gaussian process. The yellow dots are actual samples (obtained from a normal distribution) around the central predictor line.\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUsing these functions get intuitions for the kinds of curves you generate for different input vectors and parameter constellations.\n\n\n\nCaveat. This generation script is a simplified procedural illustration of a Gaussian process regression (an intuition gym). The actual implementation of a Bayesian Gaussian process regression (e.g., in brms) is much more involved. Nevertheless, we can appreciate the most important idea: for a set of parameter values supplied to the function get_GP_simulation we get different wiggly regression lines. So, reverting this generative process (or one similar to it), we can ask: which parameter values are likely to have generated a set of \\(x,y\\) observations? And that is the simple and elegant idea behing Bayesian Gaussian process regression.\n\n\nGP regression in brms\nTo implement GP regression in brms (using a radial basis function kernel; the only kernel currently implemented), just need to specify gp() for the predictor for which we want “GP wiggles”, so to speak.\nIn practice, GPR can be slow and parameters hard to identify. Let’s therefore try a simple example of parameter recovery, keeping in mind that the underyling implementation in brms may be slightly different from the heuristic protocol used for intuition-building here. It will therefore be particularly interesting to see if we can recover the intercept and slope of the generating model, i.e., the “linear core” from which we are most likely to draw relevant conclusions eventually (e.g., is there a main (linear) effect of factor XYZ underneath the wiggliness).\n\n\nToggle code\nGP_simulation &lt;- get_GP_simulation(\n  x = seq(-1,1, length.out = 100), \n  Intercept = 0, \n  slope = 1, \n  sigma_f = 0.5, \n  lambda  = 2, \n  sigma = 0.0001,\n  seed = 1996)\n\nplot_GP_simulation(GP_simulation)\n\n\n\n\n\nHere is the GP-regression model in brms:\n\n\nToggle code\n# results: hide\nfit_GPR &lt;- \n  brms::brm(\n    formula = y ~ gp(x) + x,\n    data    = GP_simulation,\n    iter    = 4000\n  )\n\n\nIt seems that we have recoverd the “linear core” parameters reasonably well, even if the estimates of the other parameter diverge from those we used to create the data (which is because the data-generating models are actually different).\n\n\nToggle code\nsummary(fit_GPR)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ gp(x) + x \n   Data: GP_simulation (Number of observations: 100) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nGaussian Process Terms: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsdgp(gpx)       0.44      0.08     0.32     0.61 1.00     1864     2144\nlscale(gpx)     0.03      0.00     0.02     0.04 1.01      563      823\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.07      0.12    -0.16     0.30 1.00     3897     3082\nx             1.16      0.19     0.80     1.55 1.00     4104     3568\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.21      0.02     0.17     0.25 1.00     2044     2958\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe fitted curve also looks very reasonable.\n\n\nToggle code\nconditional_effects(fit_GPR)"
  },
  {
    "objectID": "practice-sheets/04a-GLM-tutorial.html",
    "href": "practice-sheets/04a-GLM-tutorial.html",
    "title": "03a: Generalized linear models",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nToggle code\ndata_MT &lt;- aida::data_MT\n\n\nThis tutorial covers common types of generalized linear regression models (GLMs):\n\nlogistic regression\nmultinomial regression\nordinal regression\nPoisson regression\n\nThe shared form of all of these GLMs is the following “feed-forward computation” (here illustrated for a single datum of the predicted variable \\(y\\) for a vector \\(\\mathbf{x}\\) of predictor variables and a vector of coefficients \\(\\vec{\\beta}\\):\n\ncompute a linear predictor: \\(\\eta = \\mathbf{x} \\cdot \\beta\\);\ncompute a predictor of central tendency using an appropriate link function \\(\\text{LF}\\): \\(\\xi = \\text{LF}(\\eta ; \\theta_{\\text{LF}})\\);\ndetermine the likelihood function \\(\\text{LH}\\): \\(y \\sim \\text{LH}(\\xi; \\theta_{\\text{LH}})\\).\n\nLink function and likelihood function may have additional free parameters, \\(\\theta_{\\text{LF}}\\) and \\(\\theta_{\\text{LH}}\\), to be fitted alongside the regression coefficients.\nSimple linear regression is the special case of this scheme where the link function is just the identity function and the likelihood is given by \\(y \\sim \\mathcal{N}(\\xi; \\sigma)\\). Different types of regression are used to account for different kinds predicted variable \\(y\\):\n\n\n\n\n\n\n\n\ntype of \\(y\\)\n(inverse) link function\nlikelihood function\n\n\n\n\nmetric\n\\(\\xi = \\eta\\)\n\\(y \\sim \\text{Normal}(\\xi; \\sigma)\\)\n\n\nbinary\n\\(\\xi = \\text{logistic}(\\eta)\\)\n\\(y \\sim \\text{Bernoulli}(\\xi)\\)\n\n\nnominal\n\\(\\xi = \\text{soft-max}(\\eta)\\)\n\\(y \\sim \\text{Categorical}({\\xi})\\)\n\n\nordinal\n\\(\\xi = \\text{cumulative-logit}(\\eta; {\\mathbf{d}})\\)\n\\(y \\sim \\text{Categorical}({\\xi})\\)\n\n\ncount\n\\(\\xi = \\exp(\\eta)\\)\n\\(y \\sim \\text{Poisson}(\\xi)\\)"
  },
  {
    "objectID": "practice-sheets/04a-GLM-tutorial.html#explanation",
    "href": "practice-sheets/04a-GLM-tutorial.html#explanation",
    "title": "03a: Generalized linear models",
    "section": "Explanation",
    "text": "Explanation\nIn logistic regression, the response variable \\(y\\) is binary, i.e., we want to predict the probability \\(p\\) with which one of two possible outcomes (henceforth: the reference outcome) occurs. The likelihood function for this case is the Bernoulli distribution. This requires a link function \\(LF\\) that maps real-valued linear predictor values \\(\\eta\\) onto the unit interval. A common choice is the logistic function:\n\\[\n\\text{logistic}(\\eta) = \\frac{1}{1+ \\exp(-\\eta)} = \\xi\n\\]\n\n\n\n\n\nThe logistic regression model is then defined as (for a single observation \\(y\\) and predictor vector\\(\\mathbf{x}\\)):\n\\[\n\\begin{align*}\n\\eta &= \\mathbf{x} \\cdot \\beta    && \\color{gray}{\\text{[linear predictor]}} \\\\\n\\xi  &= \\text{logistic}(\\eta)     && \\color{gray}{\\text{[predictor of central tendency]}} \\\\\ny    & \\sim \\text{Bernoulli}(\\xi) && \\color{gray}{\\text{[likelihood]}}\n\\end{align*}\n\\]\nThe linear predictor values \\(\\eta\\) can be interpreted directly, as the log odds-ratio of the predicted probability \\(\\xi\\). This is because the inverse of the logistic function is the logit function, which has the following form:\n\\[\n\\text{logit}(\\xi) = \\log \\frac{\\xi}{1-\\xi} = \\eta\n\\]\n\n\n\n\n\nThis means that differences between linear predictor parameters can be interpreted directly as something like the “evidence ratio” or “Bayes factor”. It is the log of the factor by which to transform log odds-ratios (e.g., changing beliefs from \\(\\xi_1\\) to \\(\\xi_2\\):\n\\[\n\\begin{align*}\n& \\eta_1 - \\eta_2 = \\log \\frac{\\xi_1}{1-\\xi_1} - \\log \\frac{\\xi_2}{1-\\xi_2} = \\log \\left ( \\frac{\\xi_1}{1-\\xi_1} \\frac{1-\\xi_2}{\\xi_2}\\right ) \\\\\n\\Leftrightarrow & \\frac{\\xi_1}{1-\\xi_1} = \\exp (\\eta_1 - \\eta_2) \\ \\frac{\\xi_2}{1-\\xi_2}\n\\end{align*}\n\\]\nFor the purposes of understanding which priors are weakly or strongly informative, a unit difference in the linear predictor can be interpreted as a log Bayes factor (changing prior odds to posterior odds). So a unit difference in the predictor value corresponds to a Bayes factor of around 2.72."
  },
  {
    "objectID": "practice-sheets/04a-GLM-tutorial.html#example",
    "href": "practice-sheets/04a-GLM-tutorial.html#example",
    "title": "03a: Generalized linear models",
    "section": "Example",
    "text": "Example\nConsider the mouse-tracking data again. Our hypothesis is that typical examples are easier to classify, so they should have higher accuracy than atypical ones. We are also interested in additional effects of group on accuracy.\nAs usual, we begin by plotting the relevant data.\n\n\nToggle code\nsum_stats &lt;- data_MT |&gt; \n  group_by(group, condition) |&gt; \n  tidyboot::tidyboot_mean(correct) |&gt; \n  rename(accuracy = empirical_stat)\n  \nsum_stats\n\n\n# A tibble: 4 × 7\n# Groups:   group [2]\n  group condition     n accuracy ci_lower  mean ci_upper\n  &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 click Atypical    318    0.874    0.837 0.873    0.908\n2 click Typical     689    0.964    0.950 0.964    0.977\n3 touch Atypical    330    0.909    0.878 0.909    0.939\n4 touch Typical     715    0.941    0.922 0.942    0.960\n\n\nToggle code\nsum_stats |&gt; \n  ggplot(aes(x = condition, y = accuracy, group = group, color = group)) +\n  geom_line(size = 1, position = position_dodge(0.2)) +\n  geom_point(size = 3, position = position_dodge(0.2)) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), \n                width = 0.1, size = 0.35, position = position_dodge(0.2))\n\n\n\n\n\nVisually, there might be a hint that typical trials had higher accuracy, but we cannot judge with the naked eye whether this is substantial.\nA logistic regression, regressing correct against group * condition, may tell us more. To run the logistic regression, we must tell the brms that we want to treat 0 and 1 as categories. To be sure, and also to directly dictate which of the two categories is the reference level, we use a factor (of strings) with explicit ordering.\n\n\nToggle code\nfit_logistic &lt;- brm(\n  formula = correct ~ group * condition,\n  data = data_MT |&gt; \n    mutate(correct = factor(ifelse(correct, \"correct\", \"incorrect\"),\n                            levels = c(\"incorrect\", \"correct\"))),\n  family = bernoulli(link=\"logit\")\n)\n\n\nHere is a summary of the fit:\n\n\nToggle code\nsummary(fit_logistic)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: correct ~ group * condition \n   Data: mutate(data_MT, correct = factor(ifelse(correct, \" (Number of observations: 2052) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                       1.95      0.17     1.62     2.30 1.00     2634\ngrouptouch                      0.36      0.26    -0.14     0.87 1.00     1971\nconditionTypical                1.34      0.26     0.84     1.87 1.00     1851\ngrouptouch:conditionTypical    -0.87      0.36    -1.58    -0.17 1.00     1666\n                            Tail_ESS\nIntercept                       2873\ngrouptouch                      2612\nconditionTypical                2261\ngrouptouch:conditionTypical     2434\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nThis exercise tries to highlight the difference between the linear predictor and the prediction of central tendency.\nUse the function faintr::extract_cell_draws to get posterior samples for the linear predictor \\(\\eta\\) for each cell. Transform each sample with the logistic function, this gives you a sample of the predictor of central tendency \\(\\xi\\), and take the mean over these.\nCompare the estimated cell means to the empirical means.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nlogistic &lt;- function(x) {\n  1 / (1+ exp(-x))\n}\n\nfaintr::extract_cell_draws(fit_logistic) |&gt; \n  # drop variables for chain, iteration, draw\n  select(-starts_with(\".\")) |&gt; \n  # apply logistic function to each column\n  mutate(across(c(1:4), logistic)) |&gt; \n  # that the mean for each col\n  summarize(across(everything(), mean))\n\n\n# A tibble: 1 × 4\n  `touch:Atypical` `touch:Typical` `click:Atypical` `click:Typical`\n             &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;\n1            0.908           0.941            0.874           0.964\n\n\nThere is no reason to believe (given model and data) that this conjecture is true.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b\n\n\n\n\n\nWe can get samples from the predicted central tendency for each cell also more efficiently, by using a function like tidybayes::add_epred_draws.\nTo do this, first create a tibble with each combination of levels from group and condition, e.g., by select-ing only these columns from the data frame, and then using unique to only keep the unique entries. Next, use the function `tidybayes::add_epred_draws to obtain samples from the posterior predictive distribution of the central tendency.\nInspect the means for each cell (combination of group and condition) to the empirical means and the results from Exercise 1a.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nsamples_centralTendency &lt;- data_MT |&gt; \n  dplyr::select(group, condition) |&gt; \n  unique() |&gt; \n  tidybayes::add_epred_draws(\n    fit_logistic,\n    ndraws = 20\n    )\n\nsamples_centralTendency |&gt; \n  group_by(group, condition) |&gt; \n  reframe(aida::summarize_sample_vector(.epred)[-1])\n\n\n# A tibble: 4 × 5\n  group condition `|95%`  mean `95%|`\n  &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 click Atypical   0.836 0.875  0.900\n2 click Typical    0.948 0.964  0.979\n3 touch Atypical   0.884 0.913  0.944\n4 touch Typical    0.919 0.943  0.953\n\n\nThe means of predictors of central tendency correspond to the empirical means and the results from Exercise 1a.\n\n\n\n\n\n\nTo test whether typical examples had credibly higher accuracy, the faintr package can be used like so:\n\n\nToggle code\ncompare_groups(\n  fit_logistic,\n  higher = condition == \"Typical\",\n  lower  = condition == \"Atypical\"\n)\n\n\nOutcome of comparing groups: \n * higher:  condition == \"Typical\" \n * lower:   condition == \"Atypical\" \nMean 'higher - lower':  0.9099 \n95% HDI:  [ 0.5505 ; 1.263 ]\nP('higher - lower' &gt; 0):  1 \nPosterior odds:  Inf \n\n\nBased on these results, we may conclude that, given the model and the data, we should believe that typical examples had higher accuracy.\n\n\n\n\n\n\nExercise 1c\n\n\n\n\n\nTest whether there is reason to believe, given model and data, that the touch group was more accurate than the click group. (After all, the click group could change their minds until the very last moment.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ncompare_groups(\n  fit_logistic,\n  higher = group == \"click\",\n  lower  = group == \"touch\"\n)\n\n\nOutcome of comparing groups: \n * higher:  group == \"click\" \n * lower:   group == \"touch\" \nMean 'higher - lower':  0.07327 \n95% HDI:  [ -0.2959 ; 0.4163 ]\nP('higher - lower' &gt; 0):  0.6582 \nPosterior odds:  1.926 \n\n\nThere is no reason to believe (given model and data) that this conjecture is true.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1d\n\n\n\n\n\nIf you look back at the plot of accuracy, it looks as if the change from atypical to typical condition does not have the same effect, at least not at the same level of strength, for the click and the touch group, i.e., it seems that there is an interaction between these two variables (group and condition). Use the function brms::hypothesis() to examine the interaction term of the model fit. What do you conclude from this?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nbrms::hypothesis(fit_logistic, \"grouptouch:conditionTypical &lt; 0\")\n\n\nHypothesis Tests for class b:\n                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio\n1 (grouptouch:condi... &lt; 0    -0.87      0.36    -1.46    -0.28     120.21\n  Post.Prob Star\n1      0.99    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities.\n\n\nGiven model and data, it is very plausible to believe that there is an interaction between these two variables."
  },
  {
    "objectID": "practice-sheets/04a-GLM-tutorial.html#explanation-1",
    "href": "practice-sheets/04a-GLM-tutorial.html#explanation-1",
    "title": "03a: Generalized linear models",
    "section": "Explanation",
    "text": "Explanation\nIn multinomial regression the predicted variable is categorical with more than two levels: \\(c_1, \\dots, c_k\\), \\(k &gt; 2\\). We want to predict probabilities for each category \\(p_1, \\dots, p_k\\) (with some linear predictors, more on this in a moment). To obtain the probabilities, we estimate a set of weights (so-called logits): \\(s_1, \\dots, s_k\\). By default, we set \\(s_1 = 0\\), because we only need \\(k-1\\) numbers to define a \\(k\\)-place probability vector (given that it must sum to one). For all \\(1 \\le j \\le k\\), we define the probability \\(p_i\\) of category \\(i\\) via the following (so-called soft-max operation):\n\\[\np_j = \\frac{\\exp s_j}{ \\sum_{j'=1}^k \\exp s_j'}\n\\]\nThis entails that for every \\(1 &lt; j \\le k\\), the score \\(s_j\\) can be interpreted as the log-odds of category \\(c_j\\) over the reference category \\(c_1\\):\n\\[\ns_j = \\log \\frac{p_j}{p_1}\n\\]\nFinally, we do not just estimate any-old vector of logits, but we assume that each logit \\(s_j\\) (\\(1 &lt; j \\le k\\)) is estimated as a linear predictor (based on the usual linear regression predictor coefficients, appropriate to the type of the \\(l\\) explanatory variables):\n\\[\ns_j = \\mathbf{x} \\cdot \\beta^j\n\\]\nTwo things are important for interpreting the outcome of a multinomial regression fit:\n\neach category (beyond the reference category) receives its own (independent) set of regression coefficients;\nthe linear predictor predictor \\(s_j\\) for category \\(c_j\\) can be interpreted as the log-odds of the \\(j\\)-th category over the first, reference category."
  },
  {
    "objectID": "practice-sheets/04a-GLM-tutorial.html#example-1",
    "href": "practice-sheets/04a-GLM-tutorial.html#example-1",
    "title": "03a: Generalized linear models",
    "section": "Example",
    "text": "Example\nOur next research question is slightly diffuse: we want to explore whether the distribution of trajectory types is affected by whether the correct target was on the right or the left. We only consider three types of categories (curved, straight and ‘change of mind’) and prepare the data to also give us the information whether the ‘correct’ target was left or right.\n\n\nToggle code\ndata_MT_prepped &lt;-\n  data_MT |&gt;\n  mutate(\n    prototype_label = case_when(\n     prototype_label %in% c('curved', 'straight') ~ prototype_label,\n     TRUE ~ 'CoM'\n    ),\n    prototype_label = factor(prototype_label,\n                             levels = c('straight', 'curved', 'CoM')),\n    target_position = ifelse(category_left == category_correct, \"left\", \"right\")\n    )\n\n\nThe relevant data now looks as follows:\n\n\nToggle code\ndata_MT_prepped |&gt; \n  select(prototype_label, target_position)\n\n\n# A tibble: 2,052 × 2\n   prototype_label target_position\n   &lt;fct&gt;           &lt;chr&gt;          \n 1 straight        left           \n 2 straight        right          \n 3 curved          right          \n 4 curved          left           \n 5 CoM             left           \n 6 CoM             right          \n 7 CoM             right          \n 8 straight        left           \n 9 straight        left           \n10 straight        left           \n# ℹ 2,042 more rows\n\n\nThe counts and proportions we care about are these:\n\n\nToggle code\nsum_stats &lt;- data_MT_prepped |&gt; \n  count(target_position, prototype_label) |&gt;\n  group_by(target_position) |&gt; \n  mutate(proportion = n / sum(n))\n\nsum_stats\n\n\n# A tibble: 6 × 4\n# Groups:   target_position [2]\n  target_position prototype_label     n proportion\n  &lt;chr&gt;           &lt;fct&gt;           &lt;int&gt;      &lt;dbl&gt;\n1 left            straight          751     0.734 \n2 left            curved            136     0.133 \n3 left            CoM               136     0.133 \n4 right           straight          793     0.771 \n5 right           curved             93     0.0904\n6 right           CoM               143     0.139 \n\n\nAnd here is a plot that might be useful to address your current issue:\n\n\nToggle code\nsum_stats |&gt; \n  ggplot(aes(x = prototype_label, y = proportion, fill = prototype_label)) +\n  geom_col() +\n  facet_grid(. ~ target_position)\n\n\n\n\n\nIt is hard to say from visual inspection alone, whether there are any noteworthy differences. We might consider the following:\n\nConjecture: the difference in probability between straight vs curved is higher when the target is on the right than when it is on the left.\n\nThis is not a real “research hypothesis” but a conjecture about the data. Let’s still run a multinomial regression model to address this conjecture.\n\n\nToggle code\nfit_multinom &lt;- brm(\n  formula = prototype_label ~ target_position,\n  data = data_MT_prepped,\n  family = categorical()\n)\n\n\nThe summary of this model fit is a bit unwieldy:\n\n\nToggle code\nsummary(fit_multinom)\n\n\n Family: categorical \n  Links: mucurved = logit; muCoM = logit \nFormula: prototype_label ~ target_position \n   Data: data_MT_prepped (Number of observations: 2052) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                              Estimate Est.Error l-95% CI u-95% CI Rhat\nmucurved_Intercept               -1.71      0.09    -1.90    -1.53 1.00\nmuCoM_Intercept                  -1.71      0.10    -1.89    -1.52 1.00\nmucurved_target_positionright    -0.44      0.15    -0.73    -0.14 1.00\nmuCoM_target_positionright       -0.00      0.13    -0.26     0.24 1.00\n                              Bulk_ESS Tail_ESS\nmucurved_Intercept                4686     3270\nmuCoM_Intercept                   3852     3124\nmucurved_target_positionright     3715     2662\nmuCoM_target_positionright        4252     3241\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFor better visibility here is a plot of the posteriors over relevant model parameters.\n\n\nToggle code\n# there MUST be a nicer way of doing this, but ...\nordered_names &lt;- c(\n  \"b_mucurved_Intercept\", \n  \"b_muCoM_Intercept\",\n  \"b_mucurved_target_positionright\",\n  \"b_muCoM_target_positionright\"\n)\n\nfit_multinom |&gt; \n  tidybayes::tidy_draws() |&gt; \n  pivot_longer(cols = starts_with(\"b_\")) |&gt; \n  select(name, value) |&gt; \n  mutate(name = factor(name, levels = rev(ordered_names))) |&gt; \n  ggplot(aes(x = value, y = name)) +\n  tidybayes::stat_halfeye() +\n  geom_vline(aes(xintercept = 0), color = project_colors[3], alpha= 1, size = 1)\n\n\n\n\n\n\n\n\n\n\n\nExercise 2a\n\n\n\n\n\nLook at the names of the coefficients in the fit summary to find out what the reference level is for the categorical predictor variable?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt’s the ‘left’ position, because there is a coefficient for the ‘right’ position.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2b\n\n\n\n\n\nLook at the names of the coefficients in the fit summary to find out what the reference level is of the categories to be predicted in the multinomial model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe reference category is ‘straight’ because we have regression coeffiecient for all but the ‘straight’ category.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2c\n\n\n\n\n\nCan you extract information about our conjecture from this plot (or the summary of the model fit)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes! Our conjecture is about the difference in probability of the ‘straight’ vs the ‘curved’ category. This difference is directly encoded in regression coefficients. Concretely, the coefficient ‘mucurved_Intercept’ gives us the log odds of the ‘straight’ vs the ‘curved’ category for the ‘left’-position cases. The difference of log odds for the ‘right’-position cases is simply the coefficient ‘mucurved_target_positionright’. The is credibly smaller than zero (by a margin), so we may conclude that model and data provide support for our conjecture.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2d\n\n\n\n\n\nUse the posterior means of the regression coefficients to compute the corresponding scores \\(s_i\\) and class probabilities \\(c_i\\). Compare these to the observed frequencies.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# extract mean posteriors\nposterior_means &lt;- fit_multinom |&gt; tidybayes::summarise_draws() |&gt; \n  select(variable, mean) |&gt; \n  pivot_wider(names_from = variable, values_from = mean)\n\nas.numeric(posterior_means, names = colnames(posterior_means))  \n\n\n[1] -1.711450e+00 -1.710425e+00 -4.384644e-01 -4.594958e-03 -4.488553e+00\n[6] -1.499866e+03\n\n\nToggle code\nscores_left &lt;- c(\n  0,\n  posterior_means[1,\"b_mucurved_Intercept\"] |&gt; as.numeric(),\n  posterior_means[1,\"b_muCoM_Intercept\"] |&gt; as.numeric()\n)\n\nscores_right &lt;- c(\n  0,\n  posterior_means[1,\"b_mucurved_Intercept\"] |&gt; as.numeric() + posterior_means[1,\"b_mucurved_target_positionright\"] |&gt; as.numeric(),\n  posterior_means[1,\"b_muCoM_Intercept\"] |&gt; as.numeric() + posterior_means[1,\"b_muCoM_target_positionright\"] |&gt; as.numeric()\n)\n\nprobabilities_left &lt;- prop.table(exp(scores_left))\nprobabilities_right &lt;- prop.table(exp(scores_right))\n\nsum_stats |&gt; ungroup() |&gt; \n  mutate(prediction = c(probabilities_left, probabilities_right))\n\n\n# A tibble: 6 × 5\n  target_position prototype_label     n proportion prediction\n  &lt;chr&gt;           &lt;fct&gt;           &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 left            straight          751     0.734      0.735 \n2 left            curved            136     0.133      0.133 \n3 left            CoM               136     0.133      0.133 \n4 right           straight          793     0.771      0.771 \n5 right           curved             93     0.0904     0.0899\n6 right           CoM               143     0.139      0.139"
  },
  {
    "objectID": "practice-sheets/04a-GLM-tutorial.html#explanation-2",
    "href": "practice-sheets/04a-GLM-tutorial.html#explanation-2",
    "title": "03a: Generalized linear models",
    "section": "Explanation",
    "text": "Explanation\nWhen \\(k&gt;2\\) categories have a natural ordering, the problem of predicting probabilities for each category can be simplified by taking this ordering into account. A common choice of link function for this case is the cumulative logit function which takes the linear predictor and a vector \\(\\mathbf{d} = \\langle \\delta_1, \\dots, \\delta_{k-1} \\rangle\\) of \\(k-1\\) thresholds as arguments to return a probability vector, here denoted as \\(\\mathbf{p} = \\langle \\xi_{1}, \\dots, \\xi_{k} \\rangle\\), whose components are defined like so:\n\\[\n\\xi_j = \\text{cumulative-logit}(\\eta, j; \\mathbf{d}) =\n\\begin{cases}\n\\text{logistic}(\\delta_1 - \\eta)             & \\text{if } j=1 \\\\\n\\text{logistic}(\\delta_{i} - \\eta) - p_{j-1} & \\text{if } j&gt;1 \\\\\n\\end{cases}\n\\]\nTo see what is going on, consider a case with three categories. Fix the two thresholds \\(\\delta_1=-0.75\\) and \\(\\delta_2=1.6\\) just for illustration. Now assume that we have a case there the linear predictor value \\(\\eta\\) is zero. The cumulative logit function above then entails the category probabilities as shown in this plot, as the length of the colored bar segments:\n\n\n\n\n\nIf the linear predictor \\(\\eta\\) is estimated to be bigger than zero, this intuitively means that we shift all of the threshold to the left (by the same amount). For example, the plot below shows the case of \\(\\eta=1\\) where the probability of the first category decreases while that of the third increases.\n\n\n\n\n\nIn sum, the cumulative-logit model for ordinal regression, is defined as follows (for a single observation \\(y\\) and predictor vector\\(\\mathbf{x}\\)):\n\\[\n\\begin{align*}\n\\eta       &= \\mathbf{x} \\cdot \\beta                       && \\color{gray}{\\text{[linear predictor]}} \\\\\n\\mathbf{p} &= \\langle \\xi_1, \\dots, \\xi_k \\rangle          && \\color{gray}{\\text{[predictor of central tendency]}} \\\\\n\\xi_j      &= \\text{cumulative-logit}(\\eta, j; \\mathbf{d}) && \\color{gray}{\\text{[predictor for category $j$] }} \\\\\ny          & \\sim \\text{Categorical}(\\mathbf{p})           && \\color{gray}{\\text{[likelihood]}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "practice-sheets/04a-GLM-tutorial.html#example-2",
    "href": "practice-sheets/04a-GLM-tutorial.html#example-2",
    "title": "03a: Generalized linear models",
    "section": "Example",
    "text": "Example\nThe kind of mouse-trajectories, as categorized in variable prototype_label, are plausibly ordered by the “amount of deviation”. The following therefore tries to predict the ordered category prototype_label from the numerical measure MAD. Here is a plot of how this would look like:\n\n\nToggle code\n# prepare data by making 'prototype_label' an ordered factor\ndata_MT_prepped2 &lt;- data_MT_prepped |&gt; \n    mutate(prototype_label = factor(prototype_label, ordered = T))\n\n# plotting the ordered categories as a function of MAD\ndata_MT_prepped2 |&gt; \n  ggplot(aes(x = MAD, y = prototype_label, \n             color = prototype_label)) +\n  geom_jitter(alpha = 0.3,height = 0.3, width = 0)\n\n\n\n\n\nTo run an ordinal regression model, we specify family = cumulative(). This runs the default cumulative-logit model introduced at the beginning of the session.\n\n\nToggle code\nfit_ordinal &lt;- brm(\n  formula = prototype_label ~ MAD,\n  data = data_MT_prepped2,\n  family = cumulative()\n)\n\n\nThe summary output for this fitted model gives information about the slope of the predictor variable MAD as usual. But it also supplies information about two (!) intercepts: these are the cutoff points for the different categories in the cumulative link function.\n\n\nToggle code\nsummary(fit_ordinal)\n\n\n Family: cumulative \n  Links: mu = logit; disc = identity \nFormula: prototype_label ~ MAD \n   Data: data_MT_prepped2 (Number of observations: 2052) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]     4.04      0.17     3.72     4.39 1.00     2333     2381\nIntercept[2]     9.50      0.51     8.55    10.55 1.00     1740     1911\nMAD              0.02      0.00     0.02     0.03 1.00     2172     2499\n\nFamily Specific Parameters: \n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe can operate with the linear regression coefficients as usual, e.g., asking whether there is any reason to believe, given model and data, that the higher MAD, the higher the probability of seeing a more ‘uncertain’ trajectory type.\n\n\nToggle code\nfit_ordinal |&gt; \n  tidybayes::gather_draws(b_MAD) |&gt; \n  ggplot(aes(x = .value, y = .variable)) +\n  tidybayes::stat_halfeye() +\n  ylab(\"\") + xlab(\"\") + ggplot2::xlim(0,0.03)\n\n\n\n\n\n\n\nToggle code\nbrms::hypothesis(fit_ordinal, \"MAD &gt; 0\")\n\n\nHypothesis Tests for class b:\n  Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n1  (MAD) &gt; 0     0.02         0     0.02     0.03        Inf         1    *\n---\n'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n'*': For one-sided hypotheses, the posterior probability exceeds 95%;\nfor two-sided hypotheses, the value tested against lies outside the 95%-CI.\nPosterior probabilities of point hypotheses assume equal prior probabilities."
  },
  {
    "objectID": "practice-sheets/04a-GLM-tutorial.html#explanation-3",
    "href": "practice-sheets/04a-GLM-tutorial.html#explanation-3",
    "title": "03a: Generalized linear models",
    "section": "Explanation",
    "text": "Explanation\nThe Poisson distribution is the common choice for count data. It is defined as:\n\\[\n\\text{Poisson}(k ; \\lambda) = \\frac{\\lambda^k \\ \\exp( -\\lambda)} {k!}\n\\]\nThe link function is the exponential function (so the inverse link function is the logarithmic function). The Poisson regression model is defined as:\n\\[\n\\begin{align*}\n\\eta_i  &= \\mathbf{x}_i \\beta       && \\color{gray}{\\text{[linear predictor]}} \\\\\n\\xi_i &= \\exp(\\eta_i) && \\color{gray}{\\text{[predictor of central tendency]}} \\\\\ny_i & \\sim \\text{Poisson}(\\xi_i) && \\color{gray}{\\text{[likelihood]}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "practice-sheets/04a-GLM-tutorial.html#example-3",
    "href": "practice-sheets/04a-GLM-tutorial.html#example-3",
    "title": "03a: Generalized linear models",
    "section": "Example",
    "text": "Example\nThere are examples in the next exercise sheet. For a tutorial on Poisson regression specifically geared towards linguists see here."
  },
  {
    "objectID": "practice-sheets/10a-nonLinear.html",
    "href": "practice-sheets/10a-nonLinear.html",
    "title": "Non-linear models in brms",
    "section": "",
    "text": "It is possible to supply non-linear predictor terms in brms. These are of the form:\n\\[\n\\eta = X \\beta + F(X', \\theta_1, \\dots, \\theta_n)\n\\]\nwhere \\(X'\\) are the predictor terms feeding into non-linear function \\(F\\), with parameters \\(\\theta_1, \\dots, \\theta_n\\). These parameters can themselves be predicted by linear terms, e.g., in the form:\n\\[\n\\theta_i = X'' \\beta_{\\theta_i}\n\\]\n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nForgetting: power or exponential\nHere is a very small data set from (Murdoch 1961), as used in this tutorial paper on MLE. We have recall rates y for 100 subjects at six different time points t (in seconds) after after memorization:\n\n\nToggle code\ndata_forget &lt;- tibble(\n  y = c(.94, .77, .40, .26, .24, .16),\n  t = c(  1,   3,   6,   9,  12,  18),\n  N = 100,\n  k = y * N\n)\n\ndata_forget |&gt;\n  ggplot(aes(x = t, y = y)) +\n  geom_line(color = project_colors[6]) +\n  geom_point(color = project_colors[2], size = 2.5) +\n  ylim(0,1) +\n  xlab(\"time after memorization\") +\n  ylab(\"recall rate\") \n\n\n\n\n\nThere are two competing models on the table. The exponential model assumes that recall rates are predicted by exponential decay:\n\\[\n\\begin{align*}\nk & \\sim \\text{Binomial}( \\theta, 100) \\\\\n\\theta &= a \\exp (-bt) \\\\\na,b & \\sim \\text{log-Normal}(0, 0.5)\n\\end{align*}\n\\]\nIn contrast, the power model assumes that forgetting curves follow a power function:\n\\[\n\\begin{align*}\nk & \\sim \\text{Binomial}( \\theta, 100) \\\\\n\\theta &= ct^{-d} \\\\\nc,d &\\sim \\text{log-Normal}(0, 0.5)\n\\end{align*}\n\\]\nWe can think of these models as composed of a regular likelihood function (Binomial) with a non-linear predictor function \\(F(x,y)\\) (exponential or power), instead of the usual logistic-transformed linear predictor we know from logistic regression.\nTo define these non-linear models in brms, we use special syntax in the brmsformula. For the exponential model, for example, we use:\n\n\nToggle code\nbrms::bf(k | trials(N) ~ a * exp(-b * t), \n         a + b ~ 1, \n         nl=TRUE),\n\n\nNoteworthy here is that we can define the regressor part as a Stan function, introducing hitherto unknown variables a and b, which we can then regress against liner predictor terms. Since in the currenct case, we only want to fit these two parameters, we write this like an intercept-only model. Importantly, we must declare the model explicitly as a non-linear model using the parameter nl=TRUE.\nThe full function call also specifies priors for the parameters a and b (notice the explicit setting of a lower bound at zero). We also must declare the likelihood function (Binomial) and the link function (here: identity, because the non-linear predictor is the expected value already; not need to transform it with another link function).\n\n\nToggle code\nfit_exponential &lt;- brms::brm(\n    formula = brms::bf(k | trials(N) ~ a * exp(-b * t), \n                       a + b ~ 1, \n                       nl=TRUE),\n    data    = data_forget,\n    prior   = prior(lognormal(0,0.5), nlpar = \"a\", lb = 0) + \n              prior(lognormal(0,0.5), nlpar = \"b\", lb = 0),\n    family  = binomial(link = \"identity\"),\n    control = list(adapt_delta = 0.99)\n  )\n\n\nThe power model is set-up in parallel.\n\n\nToggle code\nfit_power &lt;- brms::brm(\n    formula = brms::bf(k | trials(N) ~ c * t^(-d), \n                       c + d ~ 1, \n                       nl=TRUE),\n    data    = data_forget,\n    prior   = prior(lognormal(0,0.5), nlpar = \"c\", lb = 0) + \n              prior(lognormal(0,0.5), nlpar = \"d\", lb = 0),\n    family  = binomial(link = \"identity\"),\n    control = list(adapt_delta = 0.99)\n  )\n\n\nLet’s try using leave-one-out cross-validation to address the question which model provides a fit to this data set.\n\n\nToggle code\nloo_compare &lt;- \n  loo_compare(\n    loo(fit_exponential), \n    loo(fit_power))\n\n\nThere is a problem. The loo function throws a warning that at least one observation is problematic under the Pareto-k diagnostic. We see that this is the first one:\n\n\nToggle code\nplot(loo(fit_power))\n\n\n\n\n\nBut that plot also shows that we may have done something weird! We have treated as a single data point all 100 observations for each time step. No wonder that some of these observations heavily influence to total likelihood. These observations are huge chunks of atomic observations.\nLet’s run these models again, but not as a Binomial model, but with a Bernoulli likelihood function, so that in LOO-based model comparison single observations are individual trials, not all trials from a given time point. Towards this end, we first unrol the data using uncount().\n\n\nToggle code\ndata_forget_long &lt;- \n  data_forget |&gt; \n  mutate(l = N-k) |&gt; \n  dplyr::select(t,l,k) |&gt; \n  pivot_longer(c(l,k), names_to = \"y\") |&gt; \n  uncount(value) |&gt; \n  mutate(y = ifelse(y == \"k\", TRUE, FALSE))\n\n\nAnd then we fit the models again:\n\n\nToggle code\nfit_exponential &lt;- brms::brm(\n    formula = brms::bf(y ~ a * exp(-b * t), \n                       a + b ~ 1, \n                       nl=TRUE),\n    data    = data_forget_long,\n    prior   = prior(lognormal(0,0.4), nlpar = \"a\", lb = 0) + \n              prior(lognormal(0,0.4), nlpar = \"b\", lb = 0),\n    family  = bernoulli(link = \"identity\"),\n    control = list(adapt_delta = 0.99)\n  )\n\n\n\n\nToggle code\nfit_power &lt;- brms::brm(\n    formula = brms::bf(y ~ c * t^(-d), \n                       c + d ~ 1, \n                       nl=TRUE),\n    data    = data_forget_long,\n    prior   = prior(lognormal(0,0.4), nlpar = \"c\", lb = 0) + \n              prior(lognormal(0,0.4), nlpar = \"d\", lb = 0),\n    family  = bernoulli(link = \"identity\"),\n    control = list(adapt_delta = 0.999)\n  )\n\n\nNow we can try the LOO-based model comparison again:\n\n\nToggle code\nloo_comp &lt;- \n  loo_compare(\n    loo(fit_exponential), \n    loo(fit_power))\nloo_comp\n\n\n                elpd_diff se_diff\nfit_exponential  0.0       0.0   \nfit_power       -7.6       5.6   \n\n\nThat worked smoothly. The results show that the power model has a worse LOO-fit, and that the difference in expected log-probability density is substantial (a smaller standard error than the difference itself).\nWe may use Ben Lambert’s test for significance, just to out a number (a \\(p\\)-value) to it:\n\n\nToggle code\n1- pnorm(-loo_compare[2,1], loo_comp[2,2])\n\n\n[1] 0.0001370782\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nRun a (linear) logistic regression model y ~ t and compare it to the two non-linear models with loo_compare. Interpret your results.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nfit_logistic &lt;- \n  brms::brm(\n    formula = y ~ t,\n    data    = data_forget_long,\n    family  = bernoulli(link = \"logit\")\n  )\n\n\n\n\nToggle code\nloo_comp &lt;- \n  loo_compare(\n    list(\n      loo(fit_logistic),\n      loo(fit_exponential),\n      loo(fit_power)\n    ))\nloo_comp\n\n\n                elpd_diff se_diff\nfit_exponential   0.0       0.0  \nfit_power        -7.6       5.6  \nfit_logistic    -16.2       4.3  \n\n\nModels are ordered from better to worse. Differences are with respect to the best model (not the previous). We must therefore also compare the latter two directly:\n\n\nToggle code\nloo_comp &lt;- \n  loo_compare(\n    list(\n      loo(fit_logistic),\n      loo(fit_power)\n    ))\nloo_comp\n\n\n             elpd_diff se_diff\nfit_power     0.0       0.0   \nfit_logistic -8.6       8.3   \n\n\nThe non-linear power model and the linear logistic model seem to be on a par.\n\n\nToggle code\n1- pnorm(-loo_compare[2,1], loo_comp[2,2])\n\n\n[1] 0.1764362\n\n\nThe linear model seems slightly worse, but that difference may not be substantial."
  },
  {
    "objectID": "practice-sheets/00a-examples-WebPPL.html#standard-model",
    "href": "practice-sheets/00a-examples-WebPPL.html#standard-model",
    "title": "Building Bayesian intuitions: Sampling from models of the data-generating process",
    "section": "Standard model",
    "text": "Standard model\nLet us first look at the data and a data-generating model that does not take the scientists’ individual measuring abilities into account. The model assumes that the measurements are reflections of the same quantity, which is measured with Gaussian noise, so the true value that everybody tried to measure is the mean mu. But since every datum is only a noisy measurement, the data points are generated by a Gaussian distribution centered at mu with some (unknown) standard deviation sigma.\n// 7 scientists measure the same quantity; some may be better measurers than others\nvar data = [-27.020, 3.570, 8.191, 9.898, 9.603, 9.945, 10.056]\n\nvar model = function() {\n\n  var mu = gaussian(0, 30)\n  var sigma = uniform(0, 20)\n  \n  mapIndexed(function(i, d){\n    observe(Gaussian({mu, sigma: sigma}), d)\n  }, data)\n\n//   return {sigma}\n  return {mu}\n}\n\nviz.marginals(Infer({model, method: \"MCMC\", samples: 10000}))\n\nNotice that the inference of mu seems heavily influenced by the first two measurements, which appear to be less in line with the predictions of the other scientists."
  },
  {
    "objectID": "practice-sheets/00a-examples-WebPPL.html#varying-sigma-model",
    "href": "practice-sheets/00a-examples-WebPPL.html#varying-sigma-model",
    "title": "Building Bayesian intuitions: Sampling from models of the data-generating process",
    "section": "Varying-sigma model",
    "text": "Varying-sigma model\nTo account for variable ability of the scientists to measure correctly, we can infer a different standard deviation sigma for each scientist.\n// 7 scientists measure the same quantity; some may be better measurers than others\nvar data = [-27.020, 3.570, 8.191, 9.898, 9.603, 9.945, 10.056]\n\nvar model = function() {\n\n  var mu = gaussian(0, 30);\n  var sigmas = repeat(7, function(){ uniform(0, 20) });\n  \n  mapIndexed(function(i, d){\n    observe(Gaussian({mu, sigma: sigmas[i]}), d)\n  }, data)\n\n//   return sigmas\n  return {mu}\n}\n\nviz.marginals(Infer({model, method: \"MCMC\", samples: 10000}))\n\n\nNotice that the estimated means are now more inline with the measurements of the last five scientists.\n\n\n\n\n\n\nExercise 4a\n\n\n\n\n\n\nGo back to the previous model. Make the model function return the inferred standard deviation. (You simple have to comment-out the return statement and remove the comment for the other.) You should see a large range of rather high credible values here. Try to understand why this is so.\nNow make the second model, with varying sigmas, also show the posteriors over each scientist’s estimated standard deviation. Compare this against the results from the previous model and try to understand what’s going on."
  },
  {
    "objectID": "practice-sheets/00a-examples-WebPPL.html#finite-mixture-model",
    "href": "practice-sheets/00a-examples-WebPPL.html#finite-mixture-model",
    "title": "Building Bayesian intuitions: Sampling from models of the data-generating process",
    "section": "Finite-mixture model",
    "text": "Finite-mixture model\nHere is another model to deal with sloppy scientists. While the previous model assigned a standard deviation to each individual scientist, intuitively, there are really to kinds of scientists: sloppy and proper ones. We have intuitions about what the difference is (e.g., in terms of a prior on the sigma parameter), but we would ideally like to let the data decide who belongs to which category. We can do this with a so-called finite-mixture model which has a discrete latent parameter for each data observation, assigning it to a type.\n// 7 scientists measure the same quantity; some may be better measurers than others\nvar data = [-27.020, 3.570, 8.191, 9.898, 9.603, 9.945, 10.056]\n\nvar model = function() {\n\n  // prior true measurement\n  var mu = gaussian(0, 30)\n  \n  // prior over sigma for sloppy or proper scientists\n  var sigma_sloppy_science = gamma(5,5)\n  var sigma_proper_science = gamma(2,2)\n  \n  // type of each scientist: sloppy or proper? (prior: unbiased 50/50)\n  var type_of_scientist = repeat(\n    7, \n    function() {categorical({vs: [\"sloppy\", \"proper\"]})} )\n\n  // likelihood function / conditioning\n  mapIndexed(function(i, d){\n    type_of_scientist[i] == \"sloppy\" ? \n      observe(Gaussian({mu, sigma: sigma_sloppy_science}), d) :\n      observe(Gaussian({mu, sigma: sigma_proper_science}), d)\n  }, data)\n\n    return {mu, sigma_sloppy_science, sigma_proper_science}\n//   return type_of_scientist\n}\n\nviz.marginals(Infer({model, method: \"MCMC\", samples: 10000,\n                     callbacks: [editor.MCMCProgress()]}))\n\n\n\n\n\n\n\n\nExercise 4b\n\n\n\n\n\n\nComment out parts of the code to obtain samples from the priors over parameters. Compare this to the plot for the posteriors, just to make sure that something was learned, and that what was learned makes sense.\nBring the code back to its original state and use the other return statement that is currently commented out. What is are these plots telling you? Does it make sense to you?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou should see seven barplots, one for each scientist, showing the posterior probability that any given scientist is sloppy or proper."
  },
  {
    "objectID": "practice-sheets/07a-model-criticism.html",
    "href": "practice-sheets/07a-model-criticism.html",
    "title": "Model criticism: predictive checks & Bayesian \\(p\\)-values",
    "section": "",
    "text": "Model criticism is an integral part of a rigid Bayesian workflow. In a previous tutorial, we already looked at how to obtain samples from a model’s predictive distributions, e.g., in order to assess whether the model’s prior assumptions are sound. That is part of model criticism. In general, model criticism asks: is my model adequate? Many aspects can influence what makes a model adequate. Prior assumptions and what they entail are one aspect. The other is whether a given model is compatible with some observed aspect of the data. In this tutorial we focus on this latter aspect. We look particularly at two concepts to criticize (check for adequacy) a model given some observed data:"
  },
  {
    "objectID": "practice-sheets/07a-model-criticism.html#basic-vpcs",
    "href": "practice-sheets/07a-model-criticism.html#basic-vpcs",
    "title": "Model criticism: predictive checks & Bayesian \\(p\\)-values",
    "section": "Basic VPCs",
    "text": "Basic VPCs\nWithout additional argument pp_check compares the overall observed distribution of the response variable to the prior/posterior predictive distribution. Check the observed distribution (marginal of \\(y\\)) first:\n\n\nToggle code\naida::data_WorldTemp |&gt; \n  ggplot(aes(x = avg_temp)) + geom_density()\n\n\n\n\n\nThe following code produces 50 samples from the prior predictive distribution of our model, and plots the marginal distribution of predicted \\(y\\) values for each (in blue) together with that of the observed data:\n\n\nToggle code\nbrms::pp_check(fit_prior, ndraws = 50)\n\n\n\n\n\nThis plot shows that this prior is way less “opinionated” or biased than its name may suggest, at least when it comes to data-point predictions. The range of a priori plausible \\(y\\)-values is very broad, compared to the range that we actually observe.\nLet’s now turn to the posterior predictive check, which can reveal systematic problems with the model, such as here: an inability to capture the bimodal-ish shape of the data.\n\n\nToggle code\nbrms::pp_check(fit_posterior, ndraws = 50)"
  },
  {
    "objectID": "practice-sheets/07a-model-criticism.html#more-specific-vpcs",
    "href": "practice-sheets/07a-model-criticism.html#more-specific-vpcs",
    "title": "Model criticism: predictive checks & Bayesian \\(p\\)-values",
    "section": "More specific VPCs",
    "text": "More specific VPCs\nThere are number of different plots pp_check is able to produce. For fine-grained plotting and exploring, the bayesplot package offers flexible plotting tools. These come in pairs: predicitve distributions only show the predictions, while predictive checks also show the data. See help(\"PPD-overview\") and help(\"PPC-overview\") for more information.\nThe general workflow is that you first extract samples from the relevant predictive distribution (in matrix form), like so:\n\n\nToggle code\npredictive_samples &lt;- brms::posterior_predict(fit_posterior, ndraws = 1000)\npredictive_samples[1:5, 1:5] \n\n\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 7.074478 7.454911 6.633051 7.113664 7.155443\n[2,] 7.449695 7.659168 7.379796 7.304069 7.084542\n[3,] 7.672518 7.738930 7.652186 7.799965 7.381869\n[4,] 7.807242 6.854406 7.490582 7.435162 7.367614\n[5,] 7.691041 7.530517 7.190798 7.904615 7.758474\n\n\nAnd then you can, for example, compare the distribution of some test statistic (here: the standard deviation), using a function like ppc_stat:\n\n\nToggle code\nbayesplot::ppc_stat(\n  y    = aida::data_WorldTemp$avg_temp, \n  yrep = predictive_samples,\n  stat = sd)\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\nInterpret this plot.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe light blue histogram indicates the distribution of the values of the test statistics under the predictive distribution (here: posterior). The darker blue line indicates the value of the test statistic for the observed data.\nIn this case, the observed test value is rather central in the posterior predictive distribution, thus suggesting that, as far as the standard deviation is concerned, the model cannot be criticized for its posterior predictions.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\nTry a similar ppc_stat plot for the prior predictive. Can you find a test statistic for which the model looks adequate?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLooking at the prior predicted mean is not too bad (at least visually).\n\n\nToggle code\npredictive_samples &lt;- brms::posterior_predict(fit_prior, ndraws = 1000)\nbayesplot::ppc_stat(\n  y    = aida::data_WorldTemp$avg_temp, \n  yrep = predictive_samples,\n  stat = mean)\n\n\n\n\n\nThat is because the predictions are very wide. There is nothing wrong about that! But, of course, another criterion that the prior predictive distribution blatantly fails is to predict the deviation in the data adequately (again this is, arguably, how it should be if we want to learn from the data):\n\n\nToggle code\nbayesplot::ppc_stat(\n  y    = aida::data_WorldTemp$avg_temp, \n  yrep = predictive_samples,\n  stat = sd)"
  },
  {
    "objectID": "practice-sheets/07a-model-criticism.html#standard-deviation-as-test-statistics",
    "href": "practice-sheets/07a-model-criticism.html#standard-deviation-as-test-statistics",
    "title": "Model criticism: predictive checks & Bayesian \\(p\\)-values",
    "section": "Standard deviation as test statistics",
    "text": "Standard deviation as test statistics\nLet’s focus on the posterior model for the temperature data and apply a rigorous (data-informed) test statistic: the standard deviation for all data recorded up to 1800.\n\n\nToggle code\n# get 4000 sets of posterior predictive samples;\n#   one set for all data points up to 1800\npostPred_y &lt;- \n  tidybayes::predicted_draws(\n    object  = fit_posterior,\n    newdata = aida::data_WorldTemp |&gt; select(year) |&gt; filter(year &lt;= 1800),\n    value   = \"avg_temp\",\n    ndraws  = 4000) |&gt; ungroup() |&gt; \n  select(.draw,year, avg_temp)\n\n# calculate the standard deviation for each set of samples\nsd_postPred &lt;- postPred_y |&gt; \n  group_by(.draw) |&gt; \n  summarize(sd_post_pred = sd(avg_temp)) |&gt; \n  pull(sd_post_pred)\n\n# calculate the SD of the $y$-measurements in\n#   the data (up to 1800)\nsd_data &lt;- aida::data_WorldTemp |&gt; filter(year &lt;= 1800) |&gt; pull(avg_temp) |&gt; sd()\n\n# approx. p-value &lt;- proportion of samples that have\n#   value of the test statistic that is more extreme\n#   than that of the data\nmean(sd_data &lt; sd_postPred)\n\n\n[1] 0.00025\n\n\n\n\n\n\n\n\nExercise 3:\n\n\n\n\n\nMake sure you understand how the code in the last code block works. Interpret the numerical result.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe see an estimated \\(p\\)-value of (close to) zero, which is really bad (for the chosen test statistic). It means that the model never predicts data with a value of the test statistic that is that extreme. Notice that “extremeness” here means “very high or very low”. So in this case, we would clearly have ground of accusing the model to fail to predict the aspect captured by this test statistic."
  },
  {
    "objectID": "practice-sheets/07a-model-criticism.html#excursion-likelihood-as-a-test-statistics",
    "href": "practice-sheets/07a-model-criticism.html#excursion-likelihood-as-a-test-statistics",
    "title": "Model criticism: predictive checks & Bayesian \\(p\\)-values",
    "section": "[Excursion:] Likelihood as a test statistics",
    "text": "[Excursion:] Likelihood as a test statistics\nWith some more effort, we can use the code above to calculate a Bayesian \\(p\\) value for the same data and model but assuming that the likelihood of the data is the test statistic. The calculation now becomes more intricate, because not only do we need to obtain samples of predicted data observations from the (fitted or prior) model, but we also need to approximate the likelihood for each predicted data observation with (a second set of) samples, thus using a nested Monte Carlo simulation. To implement the latter step, note that brms::log_lik is a handy function for obtaining the likelihood of some \\(y'\\) –be it observed, predicted or made up– given a model (prior or posterior)\n(Another example of Bayesian prior \\(p\\)-value computation is included in the chapter on hypothesis testing.)\n\n\nToggle code\nget_LH &lt;- function(avg_temp, ndraws = 1000) {\n  LH_ys &lt;- brms::log_lik(\n    object  = fit_posterior,\n    newdata = tibble(avg_temp = avg_temp, \n                     year = aida::data_WorldTemp$year),\n    ndraws  = ndraws)\n  mean(matrixStats::rowLogSumExps(LH_ys) - log(dim(LH_ys)[2]))\n}\n\npostPred_y &lt;- \n  tidybayes::predicted_draws(\n    object  = fit_posterior,\n    newdata = aida::data_WorldTemp |&gt; select(year),\n    value   = \"avg_temp\",\n    ndraws  = 100) |&gt; ungroup() |&gt; \n  select(.draw, year, avg_temp)\n\nLH_postPred &lt;- postPred_y |&gt; \n  group_by(.draw) |&gt; \n  summarize(LH_post_pred = get_LH(avg_temp)) |&gt; \n  pull(LH_post_pred)\n\nLH_data &lt;- get_LH(aida::data_WorldTemp$avg_temp)\n\nmean(LH_data &gt; LH_postPred)\n\n\n[1] 0.79\n\n\n\n\n\n\n\n\nExercise 4:\n\n\n\n\n\nInterpret the result you get (also in relation to the results from the previous exercise).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe Bayesian posterior predictive \\(p\\) value for likelihood as a test statistic is around 0.8. We should not interpret in the same binary style as in the frequentist domain: we are not looking for a hard cut-off at which to reject the model / assumption. In this case, a value of about 0.8 is not concerning. It means that in about 80% of the samples, the likelihood of data samples was lower than that of the data itself (from the point of view of the posterior model). Predicting data which is frequently less likely is actually good: it means that the data iself lies in a high-likelihood region of the model’s posterior predictive."
  },
  {
    "objectID": "practice-sheets/03a-hierarchical-models-tutorial.html",
    "href": "practice-sheets/03a-hierarchical-models-tutorial.html",
    "title": "Hierarchical regression models (tutorial)",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nToggle code\ndolphin &lt;- aida::data_MT\nmy_scale &lt;- function(x) c(scale(x))\n\n\nThis tutorial takes you through one practical example, showing the use of multilevel models. The main learning goals are:\n\nlearning how to implement multilevel linear models with brms including\nunderstanding random intercept models\nunderstanding random slope models\n\n\n\nOne way to motivate multi-level modeling is by noting that, without group-level terms, the model would be making strong (possibly) implausible independence assumptions.\nAs a motivating example, let us look at the probability of observing a straight trajectory predicted by response latency in the mouse tracking data set. Here a the plot for all data in the click group plus a logistic smooth term:\n\n\nToggle code\n# set up data frame\ndolphin_agg &lt;- dolphin %&gt;% \n  filter(correct == 1,\n         group == \"click\") %&gt;% \n  mutate(straight = as.factor(ifelse(prototype_label == \"straight\", 1, 0)),\n         log_RT_s = my_scale(log(RT)),\n         AUC_s = my_scale(AUC))\n\ndolphin_agg$straight_numeric &lt;- as.numeric(as.character(dolphin_agg$straight))\n\n# plot predicted values against data\nggplot(data = dolphin_agg,\n       aes(x = log_RT_s, y = straight_numeric)) +\n  geom_point(position = position_jitter(height = 0.02), alpha = 0.2) +\n  geom_smooth(method = \"glm\", color = project_colors[2],\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ggtitle(\"overall relationship\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\nThis picture suggest a negative relationship between the probability of observing straight trajectories (y) and people’s response times (x) (i.e. line goes down).\nBut this analysis looked at all responses at once and disregarded that responses came from groups of sources. For example, responses that come from one and the same participant are dependent on each other because participants (subject_id) might differ in characteristics relevant to the task, like how fast they move and how many times they move to the target in a straight trajectory. Another group of data points is related to different stimuli (exemplars). Different stimuli might have some inherent properties that lead to different response times and different proportions of straight trajectories. So analyzing the data without telling the model about these groups violates an important assumption of linear models. The independence assumption.\nLet’s look at these groups individually, starting by aggregating over over subject_ids and exemplars and plot the results.\n\n\nToggle code\n# aggregate over subjects\ndolphin_agg2 &lt;- dolphin_agg %&gt;% \n  group_by(subject_id) %&gt;% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight_numeric),\n            total = n()) \n\n# plot predicted values for subjects\nggplot(data = dolphin_agg2,\n       aes(x = log_RT_s, y = straights/total)) +\n  geom_point(size = 2, alpha = 0.5) +\n  # we use the geom_smooth function here as a rough proxy of the relationship \n  geom_smooth(method = \"glm\", \n              formula = y ~ x, color = project_colors[2],\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5) +\n  ggtitle(\"subject aggregates\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\nHuh. That is interesting. So if we aggregate over subjects, i.e. each data point is one subject reacting to all exemplars, we get a positive relationship between response latency and the proportion of straight trajectories. The slower the reaction the more likely a straight trajectory. That could mean that those participants that are generally slower are also the ones that tend to move more often in a straight fashion. It also makes sense to some extent. Maybe those participants seem to wait until they have made their decision and then move to the target immediately, while other participants move upwards right away and make their decision on the fly during the decision.\nNow, let’s aggregate over exemplars:\n\n\nToggle code\n# aggregate over exemplars\ndolphin_agg3 &lt;- dolphin_agg %&gt;% \n  group_by(exemplar) %&gt;% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight_numeric),\n            total = n()) \n\n# plot predicted values for exemplars\nggplot(data = dolphin_agg3,\n       aes(x = log_RT_s, y = straights/total)) +\n  geom_point(size = 2, alpha = 0.5) +\n  geom_smooth(method = \"glm\", \n              formula = y ~ x, color = project_colors[2],\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5) +\n  ggtitle(\"stimuli aggregates\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\nIf we look at the stimuli aggregates, i.e. each data point is one exemplar that all subjects have reacted to, we get a negative relationship between response latency and the proportion of straight trajectories. The quicker the reaction the more likely a straight trajectory. This could potentially reflect the difficulty of the categorization task. Maybe those exemplars that are inherently less ambiguous, for example the typical exemplars, don’t exhibit any response competition and are thus faster and more often straight.\nUltimately, we use our models to make a generalizing statement about a population. If our theory predicts a relationship between straight trajectories and response latency (without further nuance), we should find this relationship across the population of people AND the population of stimuli. But if we say, “there are more straight trajectories in faster responses”, this claim seems to be only true for within-participant behavior. So we need to inform our models about such groupings in our data, or we might overconfidently make predictions."
  },
  {
    "objectID": "practice-sheets/03a-hierarchical-models-tutorial.html#the-independence-assumption",
    "href": "practice-sheets/03a-hierarchical-models-tutorial.html#the-independence-assumption",
    "title": "Hierarchical regression models (tutorial)",
    "section": "",
    "text": "One way to motivate multi-level modeling is by noting that, without group-level terms, the model would be making strong (possibly) implausible independence assumptions.\nAs a motivating example, let us look at the probability of observing a straight trajectory predicted by response latency in the mouse tracking data set. Here a the plot for all data in the click group plus a logistic smooth term:\n\n\nToggle code\n# set up data frame\ndolphin_agg &lt;- dolphin %&gt;% \n  filter(correct == 1,\n         group == \"click\") %&gt;% \n  mutate(straight = as.factor(ifelse(prototype_label == \"straight\", 1, 0)),\n         log_RT_s = my_scale(log(RT)),\n         AUC_s = my_scale(AUC))\n\ndolphin_agg$straight_numeric &lt;- as.numeric(as.character(dolphin_agg$straight))\n\n# plot predicted values against data\nggplot(data = dolphin_agg,\n       aes(x = log_RT_s, y = straight_numeric)) +\n  geom_point(position = position_jitter(height = 0.02), alpha = 0.2) +\n  geom_smooth(method = \"glm\", color = project_colors[2],\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ggtitle(\"overall relationship\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\nThis picture suggest a negative relationship between the probability of observing straight trajectories (y) and people’s response times (x) (i.e. line goes down).\nBut this analysis looked at all responses at once and disregarded that responses came from groups of sources. For example, responses that come from one and the same participant are dependent on each other because participants (subject_id) might differ in characteristics relevant to the task, like how fast they move and how many times they move to the target in a straight trajectory. Another group of data points is related to different stimuli (exemplars). Different stimuli might have some inherent properties that lead to different response times and different proportions of straight trajectories. So analyzing the data without telling the model about these groups violates an important assumption of linear models. The independence assumption.\nLet’s look at these groups individually, starting by aggregating over over subject_ids and exemplars and plot the results.\n\n\nToggle code\n# aggregate over subjects\ndolphin_agg2 &lt;- dolphin_agg %&gt;% \n  group_by(subject_id) %&gt;% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight_numeric),\n            total = n()) \n\n# plot predicted values for subjects\nggplot(data = dolphin_agg2,\n       aes(x = log_RT_s, y = straights/total)) +\n  geom_point(size = 2, alpha = 0.5) +\n  # we use the geom_smooth function here as a rough proxy of the relationship \n  geom_smooth(method = \"glm\", \n              formula = y ~ x, color = project_colors[2],\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5) +\n  ggtitle(\"subject aggregates\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\nHuh. That is interesting. So if we aggregate over subjects, i.e. each data point is one subject reacting to all exemplars, we get a positive relationship between response latency and the proportion of straight trajectories. The slower the reaction the more likely a straight trajectory. That could mean that those participants that are generally slower are also the ones that tend to move more often in a straight fashion. It also makes sense to some extent. Maybe those participants seem to wait until they have made their decision and then move to the target immediately, while other participants move upwards right away and make their decision on the fly during the decision.\nNow, let’s aggregate over exemplars:\n\n\nToggle code\n# aggregate over exemplars\ndolphin_agg3 &lt;- dolphin_agg %&gt;% \n  group_by(exemplar) %&gt;% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight_numeric),\n            total = n()) \n\n# plot predicted values for exemplars\nggplot(data = dolphin_agg3,\n       aes(x = log_RT_s, y = straights/total)) +\n  geom_point(size = 2, alpha = 0.5) +\n  geom_smooth(method = \"glm\", \n              formula = y ~ x, color = project_colors[2],\n    method.args = list(family = \"binomial\"), \n    se = FALSE, fullrange = TRUE) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5) +\n  ggtitle(\"stimuli aggregates\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\nIf we look at the stimuli aggregates, i.e. each data point is one exemplar that all subjects have reacted to, we get a negative relationship between response latency and the proportion of straight trajectories. The quicker the reaction the more likely a straight trajectory. This could potentially reflect the difficulty of the categorization task. Maybe those exemplars that are inherently less ambiguous, for example the typical exemplars, don’t exhibit any response competition and are thus faster and more often straight.\nUltimately, we use our models to make a generalizing statement about a population. If our theory predicts a relationship between straight trajectories and response latency (without further nuance), we should find this relationship across the population of people AND the population of stimuli. But if we say, “there are more straight trajectories in faster responses”, this claim seems to be only true for within-participant behavior. So we need to inform our models about such groupings in our data, or we might overconfidently make predictions."
  },
  {
    "objectID": "practice-sheets/04b-GLM-exercises.html",
    "href": "practice-sheets/04b-GLM-exercises.html",
    "title": "03b: Generalized linear models (exercises)",
    "section": "",
    "text": "Preamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nExercise 1: logistic regression\nUse the following data frame:\n\n\nToggle code\n# set up data frame\ndolphin &lt;- aida::data_MT\ndolphin_agg &lt;- dolphin %&gt;% \n  filter(correct == 1) %&gt;% \n  mutate(straight = as.factor(ifelse(prototype_label == \"straight\", 1, 0)),\n         log_RT_s = scale(log(RT)))\n\n\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nPlot straight (straight == 1) vs. non-straight (straight == 0) trajectories (y-axis) against log_RT_s and color-code by group.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_agg$straight_numeric &lt;- as.numeric(as.character(dolphin_agg$straight))\n\nggplot(data = dolphin_agg) +\n  geom_point(aes(x = log_RT_s, y = straight_numeric, color = group), \n             # we add a little bit of jitter to make the points better visible\n             position = position_jitter(height = 0.02), alpha = 0.2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b\n\n\n\n\n\nRun the appropriate generalized linear model in brms that predicts straight vs. non-straight trajectories based on group, log_RT_s, and their two-way interaction.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nGlmMdl &lt;- brm(straight ~ log_RT_s * group, \n                 dolphin_agg, cores = 4,\n              family = \"bernoulli\",\n              seed = 123)\nGlmMdl\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: straight ~ log_RT_s * group \n   Data: dolphin_agg (Number of observations: 1915) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               0.86      0.07     0.71     1.00 1.00     3539     2963\nlog_RT_s               -0.23      0.07    -0.37    -0.10 1.00     2859     2671\ngrouptouch              0.69      0.11     0.47     0.91 1.00     3300     2810\nlog_RT_s:grouptouch    -0.01      0.11    -0.22     0.20 1.00     2857     2702\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1c\n\n\n\n\n\nDescribe the model predictions based on the posterior means of the population coefficients.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe model predicts that the log odds for the mean log_RT_s in the click group (Intercept = reference level) is 0.86. With every unit of log_RT_s these log odds become smaller by 0.23. The model predicts that the log odds for the mean log_RT_s in the touch group is 1.56 (0.86 + 0.70), i.e. much higher than in the click group. With every unit of log_RT_s these log odds become smaller by 0.22.\nThe baseline difference between click and touch group is compelling with more straight trajectories in the touch group. The effect of log_RT_s is also compelling with less straight trajectories for slower responses. This relationship is not compellingly modulated between the touch and the click group (virtually identical).\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1d\n\n\n\n\n\nExtract the posteriors means and 95% CrIs for the relationships between straight, log_RT_s and group for representative range of log_RT_s values. Plot the logistic regression lines for both groups into one graph. Color code the regression lines according to group.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# extract posterior means for model coefficients\npredicted_values &lt;- GlmMdl %&gt;%\n  spread_draws(b_Intercept, b_log_RT_s, b_grouptouch, `b_log_RT_s:grouptouch`) %&gt;%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-3, 7, 0.2))) %&gt;% \n  unnest(log_RT) %&gt;%\n  # transform into proportion space\n  mutate(pred_click = plogis(b_Intercept + b_log_RT_s * log_RT),\n         pred_touch = plogis(b_Intercept + b_log_RT_s * log_RT +\n                               b_grouptouch + `b_log_RT_s:grouptouch` * log_RT)\n         ) %&gt;%\n  group_by(log_RT) %&gt;%\n  summarise(pred_click_m = mean(pred_click, na.rm = TRUE),\n            pred_click_low = quantile(pred_click, prob = 0.025),\n            pred_click_high = quantile(pred_click, prob = 0.975),\n            pred_touch_m = mean(pred_touch, na.rm = TRUE),\n            pred_touch_low = quantile(pred_touch, prob = 0.025),\n            pred_touch_high = quantile(pred_touch, prob = 0.975)\n            ) \n\n# plot predicted values against data\nggplot(data = predicted_values) +\n  geom_hline(yintercept = c(0,1), lty = \"dashed\", color = \"grey\") +\n  geom_point(data = dolphin_agg,\n             aes(x = log_RT_s, y = straight_numeric, color = group), \n             position = position_jitter(height = 0.02), alpha = 0.2) +\n  geom_ribbon(aes(x = log_RT, ymin = pred_click_low, ymax = pred_click_high), alpha = 0.2) +\n  geom_ribbon(aes(x = log_RT, ymin = pred_touch_low, ymax = pred_touch_high), alpha = 0.2) +\n  geom_line(aes(x = log_RT, y = pred_click_m), color = \"#E69F00\", size = 2) +\n  geom_line(aes(x = log_RT, y = pred_touch_m), color = \"#56B4E9\", size = 2) +\n  ylab(\"Predicted prob of straight trajs\") +\n  ylim(-0.3,1.3) +\n  xlim(-3,7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1e\n\n\n\n\n\nAssume we want to predict correct responses based on condition. We look at the touch group only. Set up a data frame and plot the data as a point plot. (Remember how to jitter the data points)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# set up data frame\ndolphin_agg2 &lt;- dolphin %&gt;% \n filter(group == \"touch\")\n\ndolphin_agg2$correct_numeric &lt;- as.numeric(as.character(dolphin_agg2$correct))\n\nggplot(data = dolphin_agg2) +\n  geom_point(aes(x = condition, y = correct_numeric, color = condition), \n             # we add a little bit of jitter to make the points better visible\n             position = position_jitter(height = 0.02, width = 0.1), alpha = 0.2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1f\n\n\n\n\n\nRun the appropriate generalized linear model in brms that predicts correct responses based on condition. Extract the posterior means and 95% CrIs for the effect of condition on correct and plot them as points and whiskers into one plot superimposed on the data.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nGlmMdl2 &lt;- brm(correct ~ condition, \n                 dolphin_agg2, cores = 4,\n              family = \"bernoulli\")\nGlmMdl2\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: correct ~ condition \n   Data: dolphin_agg2 (Number of observations: 1045) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            2.31      0.19     1.95     2.70 1.00     3786     2713\nconditionTypical     0.46      0.25    -0.03     0.93 1.00     3076     2403\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nToggle code\n# extract posterior means for model coefficients\npredicted_values &lt;- GlmMdl2 %&gt;%\n  spread_draws(b_Intercept, b_conditionTypical) %&gt;%\n  # transform into proportion space\n  mutate(Atypical = plogis(b_Intercept),\n         Typical = plogis(b_Intercept + b_conditionTypical)\n         ) %&gt;%\n  select(Atypical, Typical) %&gt;% \n  gather(parameter, posterior) %&gt;% \n  group_by(parameter) %&gt;%\n  summarise(mean = mean(posterior, na.rm = TRUE),\n            lower = quantile(posterior, prob = 0.025),\n            upper = quantile(posterior, prob = 0.975)\n            ) \n\n# plot predicted values against data\nggplot(data = predicted_values) +\n  geom_point(data = dolphin_agg2, aes(x = condition, y = correct_numeric, color = condition), \n             # we add a little bit of jitter to make the points better visible\n             position = position_jitter(height = 0.02, width = 0.1), alpha = 0.2) +\n  geom_errorbar(aes(x = parameter, ymin = lower, ymax = upper), \n                width = 0.1, color = \"black\") +\n  geom_point(aes(x = parameter, y = mean, fill = parameter),\n             size = 4, color = \"black\", pch = 21) +\n  ylab(\"Predicted prob of correct responses\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Poisson regression\nWe will continue to use dolphin_agg in this exercise.\n\n\n\n\n\n\nExercise 2a\n\n\n\n\n\nPlot the relationship between xpos_flips and log_RT_s in a scatterplot and visually differentiate between conditions as you see fit.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nggplot(data = dolphin_agg) +\n  geom_point(aes(x = log_RT_s, y = xpos_flips, color = condition), \n             # we add a little bit of jitter to make the points better visible\n             position = position_jitter(height = 0.2), alpha = 0.2) +\n  ylim(-1,8) +\n  xlim(-5,10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2b\n\n\n\n\n\nRun an appropriate generalized regression model for xflips with brms to predict xpos_flips based on log_RT_s, condition, and their two-way interaction.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nGlmMdl3 &lt;- brm(xpos_flips ~ log_RT_s * condition, \n                 dolphin_agg, cores = 4,\n              family = \"poisson\")\nGlmMdl3\n\n\n Family: poisson \n  Links: mu = log \nFormula: xpos_flips ~ log_RT_s * condition \n   Data: dolphin_agg (Number of observations: 1915) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                    -0.09      0.05    -0.19     0.00 1.00     2687\nlog_RT_s                      0.26      0.03     0.19     0.32 1.00     2078\nconditionTypical             -0.19      0.06    -0.30    -0.07 1.00     2665\nlog_RT_s:conditionTypical     0.10      0.04     0.02     0.18 1.00     2074\n                          Tail_ESS\nIntercept                     2577\nlog_RT_s                      2567\nconditionTypical              2551\nlog_RT_s:conditionTypical     2239\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2c\n\n\n\n\n\nExtract the posterior means and 95% CrIs across a range of representative values of log_RT_s (see walkthrough) for both conditions and plot them against the data (as done before in walkthrough and exercise 1).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\npredicted_Poisson_values &lt;- GlmMdl3 %&gt;%\n  spread_draws(b_Intercept, b_log_RT_s, b_conditionTypical, `b_log_RT_s:conditionTypical`) %&gt;%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-5, 10, 0.5))) %&gt;% \n  unnest(log_RT) %&gt;%\n  mutate(pred_atypical = exp(b_Intercept + b_log_RT_s * log_RT),\n         pred_typical = exp(b_Intercept + b_log_RT_s * log_RT +\n                              b_conditionTypical + `b_log_RT_s:conditionTypical` * log_RT)) %&gt;%\n  group_by(log_RT) %&gt;%\n  summarise(pred_atypical_m = mean(pred_atypical, na.rm = TRUE),\n            pred_atypical_low = quantile(pred_atypical, prob = 0.025),\n            pred_atypical_high = quantile(pred_atypical, prob = 0.975),\n            pred_typical_m = mean(pred_typical, na.rm = TRUE),\n            pred_typical_low = quantile(pred_typical, prob = 0.025),\n            pred_typical_high = quantile(pred_typical, prob = 0.975)) \n\n\nggplot(data = predicted_Poisson_values, aes(x = log_RT)) +\n  geom_point(data = dolphin_agg, aes(x = log_RT_s, y = xpos_flips, color = condition), \n             position = position_jitter(height = 0.2), alpha = 0.2) +\n  geom_ribbon(aes(ymin = pred_atypical_low, ymax = pred_atypical_high), alpha = 0.1) +\n  geom_ribbon(aes(ymin = pred_typical_low, ymax = pred_typical_high), alpha = 0.1) +\n  geom_line(aes(y = pred_atypical_m), color = \"#E69F00\", size = 2) +\n  geom_line(aes(y = pred_typical_m),color = \"#56B4E9\", size = 2) +\n  ylab(\"Predicted prob of xflips\") +\n  ylim(-1,10) +\n  xlim(-3,6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3: Logistic regression with binomial likelihood\nBinary logistic regression assumes that the outcome variable comes from a Bernoulli distribution which is a special case of a binomial distribution where the number of trials is \\(n = 1\\) and thus the outcome variable can only be 1 or 0. In contrast, binomial logistic regression assumes that the number of the target events follows a binomial distribution with \\(n\\) trials and probability \\(q\\).\nTake the following subset of the dolphin data frame that only contains correct responses (= 1).\n\n\nToggle code\n# set up data frame\ndolphin_sub &lt;- dolphin %&gt;% \n  filter(correct == 1) %&gt;% \n  mutate(straight = (ifelse(prototype_label == \"straight\", 1, 0)),\n         log_RT_s = scale(log(RT)))\n\n\n\n\n\n\n\n\nExercise 3a\n\n\n\n\n\nFor each subject_id in each group, aggregate the mean log_RT_s, the number of trials that are classified as straight trajectories, and the total number of trials. Plot the proportion of trials that are classified as straight (vs. all trials) trajectories for each subject.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# set up data frame\ndolphin_agg3 &lt;- dolphin_sub %&gt;% \n  group_by(subject_id, group) %&gt;% \n  summarize(log_RT_s = mean(log_RT_s),\n            straights = sum(straight),\n            total = n()) \n\n# plot predicted values against data\nggplot(data = dolphin_agg3) +\n  geom_point(aes(x = log_RT_s, y = straights/total, color = group), size = 2, alpha = 0.5) +\n  ylab(\"Proportion of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3c\n\n\n\n\n\nFormulate a binomial logistic regression model to predict the proportion of straight trajectories based on log_RT_s, group, and their two-way interaction. Note that these proportional data are not assumed to be generated by a Bernoulli distribution, but a binomial distribution. Take that into account by setting family = binomial(link = \"logit\"). You also need to tell brms about the number of observations by using formula syntax like this: k | trials(N) ~ ... where k is the variable containing the number of “successes” and N the variable containing the number of trials.\nExtract posterior means and 95% CrIs for the effect of log_RT_s for both groups and plot them across a representative range of log_RT_s.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# We specify both the number of target events (straights) and the total number of trials (total) wrapped in trials(), which are separated by |. In addition, the family should be “binomial” instead of “bernoulli”.\nGlmMdl4 &lt;- brm(\n  straights | trials(total) ~ log_RT_s * group,  \n  data = dolphin_agg3, \n  family = binomial(link = \"logit\"))\n\nsummary(GlmMdl4)\n\n\n Family: binomial \n  Links: mu = logit \nFormula: straights | trials(total) ~ log_RT_s * group \n   Data: dolphin_agg3 (Number of observations: 108) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               0.69      0.07     0.54     0.83 1.00     3773     3073\nlog_RT_s                0.47      0.13     0.21     0.73 1.00     2376     2608\ngrouptouch              0.94      0.12     0.71     1.19 1.00     3248     2453\nlog_RT_s:grouptouch    -0.32      0.20    -0.71     0.06 1.00     2613     2631\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nToggle code\n# extract posteriors means and 95% CrIs\npredicted_values &lt;- GlmMdl4 %&gt;%\n  spread_draws(b_Intercept, b_log_RT_s, b_grouptouch, `b_log_RT_s:grouptouch`) %&gt;%\n  # make a list of relevant value range of logRT\n  mutate(log_RT = list(seq(-3, 3, 0.2))) %&gt;% \n  unnest(log_RT) %&gt;%\n  # transform into proportion space\n  mutate(pred_click = plogis(b_Intercept + b_log_RT_s * log_RT),\n         pred_touch = plogis(b_Intercept + b_log_RT_s * log_RT +\n                               b_grouptouch + `b_log_RT_s:grouptouch` * log_RT)\n         ) %&gt;%\n  group_by(log_RT) %&gt;%\n  summarise(pred_click_m = mean(pred_click, na.rm = TRUE),\n            pred_click_low = quantile(pred_click, prob = 0.025),\n            pred_click_high = quantile(pred_click, prob = 0.975),\n            pred_touch_m = mean(pred_touch, na.rm = TRUE),\n            pred_touch_low = quantile(pred_touch, prob = 0.025),\n            pred_touch_high = quantile(pred_touch, prob = 0.975)\n            ) \n\n# plot predicted values against data\nggplot(data = predicted_values) +\n  geom_point(data = dolphin_agg3,\n             aes(x = log_RT_s, y = straights / total, color = group),\n             alpha = 0.2, size = 2) +\n  geom_ribbon(aes(x = log_RT, ymin = pred_click_low, ymax = pred_click_high), alpha = 0.2) +\n  geom_ribbon(aes(x = log_RT, ymin = pred_touch_low, ymax = pred_touch_high), alpha = 0.2) +\n  geom_line(aes(x = log_RT, y = pred_click_m), color = \"#E69F00\", size = 2) +\n  geom_line(aes(x = log_RT, y = pred_touch_m), color = \"#56B4E9\", size = 2) +\n  ylab(\"Predicted prob of straight trajs\") +\n  ylim(0,1) +\n  xlim(-2.5,2.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3c\n\n\n\n\n\nNow compare the results from this analysis to the results from the model 1b above which you plotted in 1d. How do the model results differ and why could that be? (Feel free to explore the data to understand what is going on.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe model in 1b suggested a negative coefficient of reaction time, i.e. slower responses lead to less straight trajectories. The model here suggests a positive coefficient for reaction time, i.e. slower responses lead to more straight trajectories. Given the data, the model, and the priors, this effect is compelling for at least the click group.\nA major difference in the two analyses is that the former analysis looked at all data and disregarded that responses came from clusters of sources. For example, responses that come from one and the same participant are dependent on each other because participants might differ in characteristics relevant to the task, like how fast they move and how many times they move to the target in a straight trajectory. The latter analysis aggregated participants behavior by looking at the proportion of straight trajectories within each subject, thus one data point corresponds to one participant, resulting in data points being independent (at least regarding the participant identity). If all participants showed a negative effect of reaction time on the likelihood of straight trajectories, but participants systematically differ in terms of their baseline correlation between reaction time and likelihood of producing straight trajectories in the opposite direction (positive relationship), we might get discrepancies between these different models. What we ultimately need is to take multiple levels of the data into account simultaneously, which is the topic of next week."
  },
  {
    "objectID": "practice-sheets/01c-priors.html",
    "href": "practice-sheets/01c-priors.html",
    "title": "Priors and predictives in brms",
    "section": "",
    "text": "This tutorial covers how to inspect, set and sample priors in Bayesian regression models with brms. We also look at how to sample from the prior and posterior distribution. The main conceptual take-home message of this tutorial is: The choice of prior should be informed by their effect on the prior (and possibly also the posterior) predictive distribution. This emphasises the role of model criticism (prior and posterior) which a later tutorial will enlarge on.\n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nInspect & set priors\nWe work with the mouse-tracking data from the aida package. As a running example, we look at the linear relation between (aggregates of) area-under-the-curve AUC and MAD. Here is the relevant data plot:\n\n\nToggle code\n# catchy name for the data\ndolphin &lt;- aida::data_MT\n\n# create aggregate data\ndolphin_agg &lt;- dolphin |&gt; \n  filter(correct == 1) |&gt; \n  group_by(subject_id) |&gt; \n  dplyr::summarize(\n    AUC = median(AUC, na.rm = TRUE),\n    MAD = median(MAD, na.rm = TRUE))\n\ndolphin_agg |&gt; \nggplot(aes(x = MAD, y = AUC)) + \n  geom_point(size = 3, alpha = 0.3) \n\n\n\n\n\nLet’s fit a simple Bayesian regression model:\n\n\nToggle code\n# run the model\nmodel1 = brm(\n  AUC ~ MAD, \n  data = dolphin_agg)\n\n\nWe can inspect the priors used in in a model fit like so:\n\n\nToggle code\nbrms::prior_summary(model1)\n\n\n                          prior     class coef group resp dpar nlpar lb ub\n student_t(3, 14864.2, 32772.3) Intercept                                 \n                         (flat)         b                                 \n                         (flat)         b  MAD                            \n       student_t(3, 0, 32772.3)     sigma                             0   \n       source\n      default\n      default\n (vectorized)\n      default\n\n\nThe table gives information about the kind of prior used for different parameters. Parameters are classified into different classes (column “class”). The “b” class contains the slope coeffiecients. Here, we only have one slope (for MAD), which is identified in the “coef” column. For more complex models, the other colums may be important (e.g., identifying groups in multi-level models, for parameters in distributional and non-linear models, as well as lower and upper bounded paramters).\nThis particular output tells us that the priors for the slope coefficient for the variable MAD was “flat”. Per default, brms uses so-called improper priors for slope coefficients, i.e., not specifying any prior at all, so that every parameter value is equally weighted (even if this is not a proper probability distribution since the support is infinite).\nIn contrast, brms /does/ use more specific, in fact rather smart, priors for the intercept and for the standard deviation. These priors are informed by the data. Look:\n\n\nToggle code\ndolphin_agg |&gt; pull(AUC) |&gt; median()\n\n\n[1] 14864.25\n\n\nToggle code\ndolphin_agg |&gt; pull(AUC) |&gt; sd()\n\n\n[1] 49258.31\n\n\nWe can change the priors used to fit the model with the prior attribute and the brms::prior() function. Here, we set it to a normal (with ridiculously small standard deviation).\n\n\nToggle code\nmodel2 &lt;- brm(\n  AUC ~ MAD, \n  data = dolphin_agg,\n  prior = brms::prior(normal(0,10), class = \"b\")\n)\n\n\nThe brms::prior() function expects the prior to be specified as a Stan expression. Full documentation of the available functions for priors is in the Stan Functions Reference.\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nFit a third model model3 as the previous ones, but set the prior for the slope coefficient to a Student’s \\(t\\) distribution with mean 0, standard deviation 100 and one degree of freedom.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nmodel3 &lt;- brm(\n  AUC ~ MAD, \n  data = dolphin_agg,\n  prior = brms::prior(student_t(1,0,100), class = \"b\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b\n\n\n\n\n\nCompare the mean posteriors for all three main parameters (intercept, slope for MAD and sigma) in all three models. What effect did changing the prior on the slope parameter have for models 2 and 3? Remember that the priors for these models are quite “unreasonable” in the sense that they are far away from the posterior obtained for model 1.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nextract_info &lt;- function(model_fit, label) {\n  tidybayes::summarise_draws(model_fit) |&gt; \n    mutate(model = label) |&gt; \n    select(model, variable, q5, mean, q95) |&gt; \n    filter(variable %in% c('b_Intercept', 'b_MAD', 'sigma'))\n}\n\nrbind(\n  extract_info(model1, \"model 1\"),\n  extract_info(model2, \"model 2\"),\n  extract_info(model3, \"model 3\")\n) |&gt; arrange(variable, model)\n\n\n# A tibble: 9 × 5\n  model   variable          q5    mean     q95\n  &lt;chr&gt;   &lt;chr&gt;          &lt;num&gt;   &lt;num&gt;   &lt;num&gt;\n1 model 1 b_Intercept -2559.     479.   3561. \n2 model 2 b_Intercept 14626.   22216.  30018. \n3 model 3 b_Intercept -2424.     583.   3497. \n4 model 1 b_MAD         428.     455.    483. \n5 model 2 b_MAD           4.55    21.6    38.3\n6 model 3 b_MAD         427.     453.    480. \n7 model 1 sigma       15384.   17241.  19352. \n8 model 2 sigma       42099.   47472.  53426. \n9 model 3 sigma       15362.   17199.  19324. \n\n\nWe see that the Student-t prior in model 3 gives a very similar fit as for model 1. This is likely due to the heavier tails of the Student-t distribution.\nWe also see that the more restricted model 2 has a much lower mean posterior for the slope coefficient (because this parameter is “leashed close to zero” by the prior). Instead, model 2 compensates with a much higher intercept estimate.\n\n\n\n\n\n\nThe important upshot of this exercise is that since all parameters jointly condition the likelihood function, it can happen that changing the priors for just one parameter will also affect the posterior inferences for other parameters (who have to “go out of their way” to compensate for what the other parameter can or cannot do, so to speak).\nThis raises the important, and controversial question of how to determine “good priors”. A simple, evasive but plainly true answer is: it depends on what you want to do with your model. Things you might want to do with your model include:\n\nexplore or monkey-around,\nmake serious predictions about the future (e.g., disease spread, market development),\nor draw theoretical conclusions from data (e.g., which theory of reading-times in garden-path sentences is supported better by some data)).\n\nIn almost all cases, however, it is good advice to remember this: priors should be evaluated in the context of the (prior) predictions they entail. In other words, we need to think of priors as having a dual role:\n\npriors encode assumptions about likely parameter values, and\npriors inform the prior predictive distribution, and\npriors may also bias our posterior estimates.\n\nFor complex models, with myriads of (hierarchically nested) parameters it may not be feasible to have, express or defend particular ideas about what plausible sets of parameter values are. In it, however, often much more feasible to have intuitions about the implications of particular choices of priors for the predictions that the model makes, e.g, about the data that would be likely to observe from an a prior point of view. Finally, the choice of priors can matter for parameter estimation. For one, complex models may require choices of adequate priors in order for us to be able to fit them with whatever technique we are using. So, we might consider a form of regularizing prior simply in order to make the model “well-behaved”, while at the same time making sure that we are not sneaking in unwarranted assumptions (back again to checking the prior predictive distribution). Moreover, the choice of priors might be biasing in ways that we did not intend. For these reasons, it may also be prudent to inspect the impact of the choice of priors on the posterior distribution. In the following, we therefore first look at how to take samples from the priors as such, and then how to obtain samples from the prior and posterior distribution.\n\n\nSample from prior\nHere is how we can obtain samples from the prior distribution over parameters of a model. Sampling from the prior only works if priors are not the improper (flat) default priors. Firstly, we can use the option sample_prior = \"only\" to obtain only samples from the prior. (NB: we still need to supply the data because it is used for the setting up the model; e.g., specifying the prior for the intercept.)\n\n\nToggle code\nmodel2_priorOnly &lt;- brm(\n  AUC ~ MAD, \n  data = dolphin_agg,\n  prior = brms::prior(normal(0,10), class = \"b\"),\n  sample_prior = 'only'\n)\n\n\n\n\nToggle code\nmodel2_priorOnly |&gt; tidybayes::summarise_draws() |&gt; select(1:6)\n\n\n# A tibble: 5 × 6\n  variable         mean    median       sd      mad       q5\n  &lt;chr&gt;           &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 b_Intercept 14526.    14765.    52797.   39139.   -66221. \n2 b_MAD           0.106     0.251     9.55     9.66    -15.7\n3 sigma       36088.    25518.    40930.   23280.     2452. \n4 lprior        -27.3     -26.9       1.62     1.32    -30.7\n5 lp__          -17.4     -17.0       1.47     1.21    -20.4\n\n\nIt is also possible to obtain a posterior fit /and/ prior samples at the same time, but that is a bit more fickle, as the prior samples will have other names, and (AFAICS) other functions are required than for posterior samples, entailing other formatting of the returned samples.\n\n\nToggle code\nmodel2_priorAdded &lt;- brm(\n  AUC ~ MAD, \n  data = dolphin_agg,\n  prior = brms::prior(normal(0,10), class = \"b\"),\n  sample_prior = TRUE\n)\n\n\n\n\nToggle code\n# posterior samples\nmodel2_priorAdded |&gt; tidybayes::summarise_draws() |&gt; select(1:6)\n\n\n# A tibble: 8 × 6\n  variable             mean    median       sd      mad        q5\n  &lt;chr&gt;               &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;     &lt;num&gt;\n1 b_Intercept     22216.    22234.     4508.    4514.    14869.  \n2 b_MAD              21.5      21.3      10.2      9.91      5.43\n3 sigma           47430.    47282.     3401.    3314.    42050.  \n4 prior_Intercept 14653.    15396.    55792.   38039.   -67173.  \n5 prior_b            -0.193    -0.373    10.1     10.3     -16.6 \n6 prior_sigma     35986.    25249.    45545.   23883.     2176.  \n7 lprior            -29.3     -28.7       2.32     2.00    -33.9 \n8 lp__            -1335.    -1334.        1.21     1.01  -1337.  \n\n\nToggle code\n# prior samples\nbrms::prior_samples(model2_priorAdded) |&gt; summary()\n\n\n   Intercept             b                sigma          \n Min.   :-673877   Min.   :-35.0468   Min.   :      0.4  \n 1st Qu.: -10848   1st Qu.: -7.0631   1st Qu.:  11685.0  \n Median :  15396   Median : -0.3727   Median :  25249.3  \n Mean   :  14652   Mean   : -0.1930   Mean   :  35985.8  \n 3rd Qu.:  40355   3rd Qu.:  6.8809   3rd Qu.:  46537.7  \n Max.   : 706521   Max.   : 32.8908   Max.   :1374258.5  \n\n\nA third possibility is to use stats::update() to draw additional prior samples from an already fitted object, like so:\n\n\nToggle code\n# this fit only contains priors but keeps them with the same names and structure\n# as the posterior samples in `model2`\nmodel2_priorUpdate &lt;- stats::update(model2, sample_prior = \"only\")\n\n\n\n\nToggle code\nmodel2_priorUpdate |&gt; tidybayes::summarise_draws() |&gt; select(1:6)\n\n\n# A tibble: 5 × 6\n  variable         mean    median        sd      mad       q5\n  &lt;chr&gt;           &lt;num&gt;     &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 b_Intercept 15657.    15859.     52636.   36520.   -59016. \n2 b_MAD          -0.325    -0.198      9.95     9.84    -16.9\n3 sigma       37291.    25178.    130315.   23155.     2167. \n4 lprior        -27.3     -26.9        1.66     1.35    -30.6\n5 lp__          -17.4     -17.0        1.57     1.23    -20.5\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\nRun a linear regression model on R’s cars data set, setting the priors exactly as we did for the WebPPL model a previous tutorial in this chapter, i.e., a normal prior (mean -18, standard deviation 5) for the intercept, a normal prior (mean 0, standard 10) for the slope, and a gamma prior (parameters 5 and 5) for the standard deviation. Then rerun the model to obtain only samples from the priors (using stats::update). Plot the fits both with bayesplot::mcmc_dens.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nfit_cars &lt;- brms::brm(\n  data = cars, \n  formula = dist ~ speed,\n  prior = c(\n    brms::prior(\"normal(-18,5)\", class = \"Intercept\"),\n    brms::prior(\"normal(0,10)\", class = \"b\"),\n    brms::prior(\"gamma(5,5)\", class = \"sigma\")\n  )\n)\n\n\n\n\nToggle code\nbayesplot::mcmc_dens(fit_cars)\n\n\n\n\n\n\n\nToggle code\nfit_cars_priors &lt;- stats::update(fit_cars, sample_prior = \"only\")\n\n\n\n\nToggle code\nbayesplot::mcmc_dens(fit_cars_priors)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSamples from the predictive distributions\nFor a simple Bayesian linear regression model, there are different predictive functions we need to distinguish. First, we distinguish the prior and the posterior perspective, of course. Second, we may interested in different kinds or locations of predictions. Concretely, here we look at predictions of central tendency (i.e., predictions of best fitting linear regression lines) and predictions of observations (i.e., data points around the predicted linear regression lines). Notice that the latter also takes the standard deviation into account, the former does not.\nLet’s look at a data set of average world temperatures from the middle of the 18th century to the early 21st century. Here is a plot of our data points.\n\n\nToggle code\n  ggplot() + \n    geom_point(data = aida::data_WorldTemp, \n               aes(x = year, y = avg_temp),\n               color = project_colors[2],\n               size = 1,\n               alpha = 0.8) +\n    ylab(\"average temperature\")\n\n\n\n\n\nTo obtain samples from predictive distributions from brms, we first need a model fit. So, let’s obtain a posterior fit and then use sample_prior = \"only\" to get a similarly structured model fit object with samples from the prior.\n\n\nToggle code\nprior_baseline &lt;- c(prior(\"normal(0, 0.02)\", class = \"b\"),\n                    prior(\"student_t(3, 8, 5)\", class = \"Intercept\"))\n\n# get the posterior fit\nfit &lt;- brm(\n  avg_temp ~ year,\n  prior = prior_baseline,\n  data = aida::data_WorldTemp,\n  silent = TRUE,\n  refresh = 0\n)\n\n# retrieve prior samples from the posterior fit\nfit_prior_only &lt;- update(\n  fit,\n  silent = TRUE,\n  refresh = 0,\n  sample_prior = \"only\"\n)\n\n\n\nSamples of linear predictor values\nWe can obtain samples for values of the linear predictor using the function tidybayes::add_linpred_draws. Below we supply:\n\nthe fitted object (posterior or prior),\nthe \\(x\\)-values for which we want the predictions (here we use the \\(x\\)-values in the data set),\na number of samples\nthe name of the column that contains the samples in the returned data frame.\n\n\n\nToggle code\npostPred_linPredictor &lt;- tidybayes::add_linpred_draws(\n  fit, \n  newdata = tibble(year = aida::data_WorldTemp$year),\n  ndraws = 2000,\n  value = 'avg_tmp'\n) |&gt; \n  ungroup() |&gt; \n  select(year, .draw, avg_tmp)\n\npostPred_linPredictor\n\n\n# A tibble: 538,000 × 3\n    year .draw avg_tmp\n   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1  1750     1    7.56\n 2  1750     2    7.34\n 3  1750     3    7.46\n 4  1750     4    7.31\n 5  1750     5    7.40\n 6  1750     6    7.44\n 7  1750     7    7.48\n 8  1750     8    7.44\n 9  1750     9    7.50\n10  1750    10    7.38\n# ℹ 537,990 more rows\n\n\nSimilarly, for samples from the prior predictive:\n\n\nToggle code\npriorPred_linPredictor &lt;- tidybayes::add_linpred_draws(\n  fit_prior_only, \n  newdata = tibble(year = aida::data_WorldTemp$year),\n  ndraws = 2000,\n  value = 'avg_tmp'\n) |&gt; \n  ungroup() |&gt; \n  select(year, .draw, avg_tmp)\n\npriorPred_linPredictor\n\n\n# A tibble: 538,000 × 3\n    year .draw avg_tmp\n   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1  1750     1   8.98 \n 2  1750     2   8.10 \n 3  1750     3 -10.3  \n 4  1750     4   6.08 \n 5  1750     5  -0.463\n 6  1750     6  18.4  \n 7  1750     7  -8.16 \n 8  1750     8  12.1  \n 9  1750     9   2.53 \n10  1750    10   5.05 \n# ℹ 537,990 more rows\n\n\nThe following plot shows each sample from the linear predictors as a thin grey line, and the actual data as red points. We see that the prior model makes a very wide range of predictions for regression lines, arguably quite implausibly so. The shape of the data points is hardly recognizable for the case of the prior because of that (see also the values on the \\(y\\)-axes). The second plot, for the posterior, shows that a posteriori credible regression lines are much more confined.\n\n\nToggle code\npriorPred_linPredictor |&gt; mutate(type = \"prior\") |&gt; \n  rbind(postPred_linPredictor |&gt; mutate(type = \"posterior\")) |&gt; \n  mutate(type = factor(type, levels = c(\"prior\", \"posterior\"))) |&gt; \n  ggplot() + \n  facet_grid(type ~ ., scales = \"free\") +\n  geom_line(aes(x = year, y = avg_tmp, group = .draw), \n            color = \"gray\", alpha = 0.3) +\n  geom_point(data = aida::data_WorldTemp, \n             aes(x = year, y = avg_temp), color = project_colors[2], size = 1, alpha = 0.8) +\n  ylab(\"average temperature\")\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\n\nLet’s use prior / posterior predictives to investigate the plausibility of different priors for our regression model. We can use the following function plot_predictPriPost to conveniently visualize the predictives for different priors:\n\n\nToggle code\nplot_predictPriPost &lt;- function(prior_spec, ndraws = 1000) {\n  \n  # get the posterior fit\n  fit &lt;- brm(\n    avg_temp ~ year,\n    prior = prior_spec,\n    data = aida::data_WorldTemp,\n    silent = TRUE,\n    refresh = 0\n  )\n  \n  # retrieve prior samples from the posterior fit\n  fit_prior_only &lt;- update(\n    fit,\n    silent = TRUE,\n    refresh = 0,\n    sample_prior = \"only\"\n  )\n  \n  get_predictions &lt;- function(fit_object, type = \"prior prediction\") {\n    \n    tidybayes::add_linpred_draws(\n      fit_object, \n      newdata = tibble(year = aida::data_WorldTemp$year),\n      ndraws = ndraws,\n      value = 'avg_tmp'\n    ) |&gt; \n      ungroup() |&gt; \n      select(year, .draw, avg_tmp) |&gt; \n      mutate(type = type)\n    \n  }\n  \n  get_predictions(fit, \"posterior prediction\") |&gt; \n    rbind(get_predictions(fit_prior_only, \"prior prediction\")) |&gt; \n    mutate(type = factor(type, levels = c(\"prior prediction\", \"posterior prediction\"))) |&gt; \n    ggplot() + \n    facet_grid(type ~ ., scales = \"free\") +\n    geom_line(aes(x = year, y = avg_tmp, group = .draw), \n              color = \"gray\", alpha = 0.3) +\n    geom_point(data = aida::data_WorldTemp, \n               aes(x = year, y = avg_temp), color = project_colors[2], size = 1, alpha = 0.8) +\n    ylab(\"average temperature\")\n}\n\n\n\nInspect, interpret and evaluate the impact of the following “opinionated prior”.\n\n\n\nToggle code\nprior_opinionated &lt;- c(prior(\"normal(0.2, 0.05)\", class = \"b\"),\n                       prior(\"student_t(3, 8, 5)\", class = \"Intercept\"))\nplot_predictPriPost(prior_opinionated)\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThese priors put stronger constraints on a priori plausible regression lines than the previous. This is still rather unconstrained, though, so it has little (visual) effect on the posterior.\n\n\n\n\nInspect, interpret and evaluate the impact of the following “crazy prior”.\n\n\n\nToggle code\nprior_crazy &lt;- c(prior(\"normal(-1, 0.005)\", class = \"b\"),\n                 prior(\"student_t(3, 8, 5)\", class = \"Intercept\"))\nplot_predictPriPost(prior_crazy)\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThese priors are very strong and make a priori plausible regression lines that predict a strongly decreasing world temperature over time. These priors are so strong that the data is not able to overturn the strong bias in these priors. This is a case where inspecting the posterior to evaluate the prior might also be useful. While this case is simple and transparent, sometimes the strength of prior assumptions shows clearly in the posterior fit. However, strongly biased priors may be used for a reason (e.g., the actually test this assumption, e.g., with Bayesian \\(p\\)-values).\n\n\n\n\n\n\n\n\nSamples from the data-predictive distribution\nWe can also obtain samples of predicted hypothetical data \\(y\\) for a given set of values \\(x\\), either from the prior or posterior perspective. We use the function tidybayes::add_predicted_draws for that, like so:\n\n\nToggle code\npostPred_linPredictor &lt;- tidybayes::add_predicted_draws(\n  fit, \n  newdata = tibble(year = aida::data_WorldTemp$year),\n  ndraws = 1, # we only want 1 sample of y for each x-values\n  value = 'avg_tmp'\n) |&gt; \n  ungroup() |&gt; \n  select(year, .draw, avg_tmp)\n\npostPred_linPredictor\n\n\n# A tibble: 269 × 3\n    year .draw avg_tmp\n   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1  1750     1    7.81\n 2  1751     1    7.12\n 3  1753     1    7.86\n 4  1754     1    8.12\n 5  1755     1    7.95\n 6  1756     1    7.40\n 7  1757     1    7.16\n 8  1758     1    6.86\n 9  1759     1    7.60\n10  1760     1    7.25\n# ℹ 259 more rows\n\n\nThis gives us one sample of a hypothetical average temperature for each year (\\(x\\)-value) in the data set. Here is a visualization, where the bigger blue points are predicted values (from the posterior), and the red points are the actually observed data points.\n\n\nToggle code\nggplot() + \n  geom_point(data = postPred_linPredictor, \n             aes(x = year, y = avg_tmp), \n             color = project_colors[1], size = 2, alpha = 0.9) +\n  geom_point(data = aida::data_WorldTemp, \n             aes(x = year, y = avg_temp), \n             color = project_colors[2], size = 1, alpha = 0.8)\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\n\nBelow is another convenience function plot_predictPriPost_data to visualize the data-predictives for different priors.\n\n\nToggle code\nplot_predictPriPost_data &lt;- function(prior_spec) {\n  \n  # get the posterior fit\n  fit &lt;- brm(\n    avg_temp ~ year,\n    prior = prior_spec,\n    data = aida::data_WorldTemp,\n    silent = TRUE,\n    refresh = 0\n  )\n  \n  # retrieve prior samples from the posterior fit\n  fit_prior_only &lt;- update(\n    fit,\n    silent = TRUE,\n    refresh = 0,\n    sample_prior = \"only\"\n  )\n  \n  get_predictions_data &lt;- function(fit_object, type = \"prior prediction\") {\n    \n    tidybayes::add_predicted_draws(\n      fit_object, \n      newdata = tibble(year = aida::data_WorldTemp$year),\n      ndraws = 1,\n      value = 'avg_tmp'\n    ) |&gt; \n      ungroup() |&gt; \n      select(year, .draw, avg_tmp) |&gt; \n      mutate(type = type)\n    \n  }\n  \n  get_predictions_data(fit, \"posterior prediction\") |&gt; \n    rbind(get_predictions_data(fit_prior_only, \"prior prediction\")) |&gt; \n    mutate(type = factor(type, levels = c(\"prior prediction\", \"posterior prediction\"))) |&gt; \n    ggplot() + \n    facet_grid(type ~ ., scales = \"free\") +\n    geom_point(aes(x = year, y = avg_tmp), color = project_colors[1], size = 2, alpha = 0.9) +\n    geom_point(data = aida::data_WorldTemp, \n               aes(x = year, y = avg_temp), color = project_colors[2], size = 1, alpha = 0.8) +\n    ylab(\"average temperature\")\n}\n\n\nInspect, interpret and evaluate the impact of the following piors.\n\n\nToggle code\nprior_baseline &lt;- c(prior(\"normal(0, 0.02)\", class = \"b\"),\n                    prior(\"student_t(3, 8, 5)\", class = \"Intercept\"))\nplot_predictPriPost_data(prior_baseline)\n\n\n\n\n\n\n\nToggle code\nprior_opinionated &lt;- c(prior(\"normal(0.2, 0.05)\", class = \"b\"),\n                       prior(\"student_t(3, 8, 5)\", class = \"Intercept\"))\nplot_predictPriPost_data(prior_opinionated)\n\n\n\n\n\n\n\nToggle code\nprior_crazy &lt;- c(prior(\"normal(-1, 0.005)\", class = \"b\"),\n                 prior(\"student_t(3, 8, 5)\", class = \"Intercept\"))\nplot_predictPriPost_data(prior_crazy)"
  },
  {
    "objectID": "practice-sheets/04d-distributional-models.html",
    "href": "practice-sheets/04d-distributional-models.html",
    "title": "XX: Distributional models",
    "section": "",
    "text": "The term “distributional model” is not sharply defined and not altogether common. Alternatively, one may read “models for location, scale and shape” or similar verbiage. The general idea, however, is simple: when our model has a likelihood function with additional parameters, e.g., the standard deviation \\(\\sigma\\) in the Gaussian likelihood function of a vanilla linear regression, we can not only infer these parameters, but also make them dependent, e.g., on other predictors.\nFor example, when a standard linear regression model would look like this:\n\\[\n\\begin{align*}\ny & \\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\mu & = X \\ \\beta \\\\\n\\beta, \\sigma & \\sim \\dots \\text{some priors} \\dots\n\\end{align*}\n\\]\na simple distributional model could look like this:\n\\[\n\\begin{align*}\ny & \\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\mu & = X \\ \\beta^{\\mu} \\\\\n\\sigma & = \\exp \\left ( X \\ \\beta^{\\sigma} \\right) \\\\\n\\beta^{\\mu}, \\beta^{\\sigma} & \\sim \\dots \\text{some priors} \\dots\n\\end{align*}\n\\]\nthereby assuming that \\(\\sigma\\) itself depends on the predictors \\(X\\) in a linear way.\n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nExample: World temperature data\nThe World Temperature data, included in the aida package provides a useful minimal example. We want to regress avg_temp on year, but we see that early measurements appear to be much more noisy, so that a linear fit will be better for data between 1900 and 1980, and worse for data between 1750 to 1900, simply because of differences related to differently noise measurements at different times.\n\n\nToggle code\naida::data_WorldTemp |&gt; \n  ggplot(aes(x = year, y = avg_temp)) +\n  geom_point() +\n  ylab(\"average temperature\") + geom_smooth(method = \"lm\")\n\n\n\n\n\nA simple linear regression model is easy to fit:\n\n\nToggle code\n# vanilla regression\nfit_temp_vanilla &lt;- \n  brms::brm(\n    avg_temp ~ year,\n    data = aida::data_WorldTemp)\n\n\nBut there are clear indicators that this is not a very good model, e.g., using a posterior predictive \\(p\\)-value with the standard deviation for predictions of all data points between 1750-1800 as a test statistic:\n\n\nToggle code\npostPred_y &lt;- \n  tidybayes::predicted_draws(\n    object  = fit_temp_vanilla,\n    newdata = aida::data_WorldTemp |&gt; dplyr::select(year) |&gt; filter(year &lt;= 1800),\n    value   = \"avg_temp\",\n    ndraws  = 4000) |&gt; ungroup() |&gt; \n  dplyr::select(.draw, year, avg_temp)\n\nsd_postPred &lt;- postPred_y |&gt; \n  group_by(.draw) |&gt; \n  summarize(sd_post_pred = sd(avg_temp)) |&gt; \n  pull(sd_post_pred)\n\nsd_data &lt;- aida::data_WorldTemp |&gt; filter(year &lt;= 1800) |&gt; pull(avg_temp) |&gt; sd()\n\nmean(sd_data &gt; sd_postPred)\n\n\n[1] 1\n\n\nIf we care about faithful prediction in this early period, including accuracy about the noisiness of the data, the vanilla linear model is clearly bad.\n\n\nA simple distributional model\nWe want to fit the distributional model sketched in the beginning:\n\\[\n\\begin{align*}\ny & \\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n\\mu & = X \\ \\beta^{\\mu} \\\\\n\\sigma & = \\exp \\left ( X \\ \\beta^{\\sigma} \\right) \\\\\n\\beta^{\\mu}, \\beta^{\\sigma} & \\sim \\dots \\text{some priors} \\dots\n\\end{align*}\n\\]\nTo fit this model with brms, we need to specify the formula for the regression as follows:\n\n\nToggle code\nformula_temp_distributional = brms::bf(avg_temp ~ year, sigma ~ year)\n\n\nThis formula first declares that avg_temp is to be regressed on year, as usual, and also declares that sigma is supposed to be regressed (in quite the same way) on year as well. The variable sigma does not occur in the data, but is recognized as an internal variable, namely the standard deviation of the Gaussian likelihood function.\nSampling with Stan can get troublesome if parameters are on quite different scales, so we should make sure that the two estimands are roughly on the same scale.\n\n\nToggle code\n# to run a distributional model, we need to rescale 'year'\n#   division by a factor of 1000 is sufficient\ndata_WorldTemp &lt;- aida::data_WorldTemp |&gt; \n  mutate(year = (year)/1000)\n\n\n\n\nToggle code\nfit_temp_distributional &lt;- \n  brms::brm( \n    formula = brms::bf(avg_temp ~ year, sigma ~ year),\n    data    = data_WorldTemp\n  )\n\n\nThis model provides us with information about intercepts and slopes for both components, the regression of avg_temp and sigma.\n\n\nToggle code\nsummary(fit_temp_distributional)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: avg_temp ~ year \n         sigma ~ year\n   Data: data_WorldTemp (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          -5.07      0.67    -6.42    -3.79 1.00     3149     2915\nsigma_Intercept     4.08      1.00     2.12     6.06 1.00     2929     2745\nyear                7.09      0.35     6.42     7.79 1.00     3191     2938\nsigma_year         -2.67      0.53    -3.71    -1.64 1.00     2916     2728\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere is a plot that shows the posterior (means and credible interval) of \\(\\sigma\\) as a function of year.\n\n\nToggle code\ntidybayes::tidy_draws(fit_temp_distributional) |&gt; \n  dplyr::select(.draw, b_sigma_Intercept, b_sigma_year) |&gt; \n  cross_join(tibble(year = (1750:2020) / 1000)) |&gt;\n  mutate(sigma_predicted = exp(b_sigma_Intercept + b_sigma_year * year)) |&gt; \n  group_by(year) |&gt; \n  summarize(\n    lower  = tidybayes::hdi(sigma_predicted)[1],\n    mean   = mean(sigma_predicted),\n    higher = tidybayes::hdi(sigma_predicted)[2]) |&gt; \n  ggplot(aes(x = year * 1000, y = mean)) +\n  geom_ribbon(aes(ymin=lower, ymax=higher), fill = project_colors[2], alpha = 0.2) +\n  geom_line(color = project_colors[2], linewidth = 2) +\n  xlab(\"year\") +\n  ylab(\"estimated sigma\")\n\n\n\n\n\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nLet’s inspect how brms sets up the priors for this model:\n\n\nToggle code\nbrms::get_prior(\n  formula = brms::bf(avg_temp ~ year, sigma ~ year),\n  data    = data_WorldTemp)\n\n\n                  prior     class coef group resp  dpar nlpar lb ub\n student_t(3, 8.3, 2.5) Intercept                                  \n                 (flat)         b                                  \n                 (flat)         b year                             \n   student_t(3, 0, 2.5) Intercept                 sigma            \n                 (flat)         b                 sigma            \n                 (flat)         b year            sigma            \n       source\n      default\n      default\n (vectorized)\n      default\n      default\n (vectorized)\n\n\nUsing this information set a prior on the slope coefficient for both components of the model. Use a Student-t distribution with one degree of freedom, mean 0 and standard deviation 10.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nprior_temp_distributional &lt;- \n  c(prior(student_t(1,0,10), class = \"b\", coef = \"year_c\"),\n    prior(student_t(1,0,10), class = \"b\", dpar = \"sigma\")\n    )"
  },
  {
    "objectID": "practice-sheets/05a-MCMC-diagnostics.html",
    "href": "practice-sheets/05a-MCMC-diagnostics.html",
    "title": "MCMC diagnostics (demonstrations)",
    "section": "",
    "text": "Preamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nToggle code\ndolphin &lt;- aida::data_MT\nmy_scale &lt;- function(x) c(scale(x))\n\n\nThis tutorial provides demonstrations of how to check the quality of MCMC samples obtained from brms model fits.\n\n\nA good model\nTo have something to go on, here are two model fits, one of this is good, the other is … total crap. The first model fits a smooth line to the average world temperature. (We need to set the seed here to have reproducible results.)\n\n\nToggle code\nfit_good &lt;- brm(\n  formula = avg_temp ~ s(year), \n  data = aida::data_WorldTemp,\n  seed = 1969\n) \n\n\nHere is a quick visualization of the model’s posterior prediction:\n\n\nToggle code\nconditional_effects(fit_good)\n\n\n\n\n\nThe good model is rather well behaved. Here is a generic plot of its posterior fits and traceplots:\n\n\nToggle code\nplot(fit_good)\n\n\n\n\n\nTraceplots look like hairy caterpillar madly-in-love with each other. The world is good.\nWe can check \\(\\hat{R}\\) and effective sample sizes also in the summary of the model:\n\n\nToggle code\nsummary(fit_good)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ s(year) \n   Data: aida::data_WorldTemp (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     3.68      1.12     2.01     6.31 1.00     1070     1610\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     8.31      0.02     8.27     8.35 1.00     3759     2821\nsyear_1      14.56      2.34    10.14    19.13 1.00     2005     2552\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.02     0.30     0.36 1.00     2944     2567\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nImportantly, the summary of the model contains a warning message about one divergent transition. We are recommended to check the pairs() plot, so here goes:\n\n\nToggle code\npairs(fit_good)\n\n\n\n\n\nThis is actually not too bad. (Wait until you see a terrible case below!)\nWe can try to fix this problem with a single divergent transition by doing as recommended by the warning message, namely increasing the adapt_delta parameter in the control structure:\n\n\nToggle code\nfit_good_adapt &lt;- brm(\n  formula = avg_temp ~ s(year), \n  data = aida::data_WorldTemp,\n  seed = 1969,\n  control = list(adapt_delta=0.9),\n) \n\nsummary(fit_good_adapt)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ s(year) \n   Data: aida::data_WorldTemp (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     3.67      1.15     2.04     6.49 1.00      886     1311\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     8.31      0.02     8.27     8.35 1.00     3524     2788\nsyear_1      14.57      2.30    10.23    19.18 1.00     2137     2321\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.02     0.30     0.36 1.00     2703     2301\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThat looks better, but what did we just do? — When the sampler “warms up”, it tries to find good parameter values for the case at hand. The adapt_delta parameter is the minimum amount of accepted proposals (where to jump next) before “warm up” counts as done and successfull. So with a small problem like this, just making the adaptation more ambitious may have have solved the problem. It has also, however, made the sampling slower, less efficient.\nA powerful interactive tool for exploring a fitted model (diagnostics and more) is shinystan:\n\n\nToggle code\nshinystan::launch_shinystan(fit_good_adapt)\n\n\n\n\nA terrible model\nThe main (maybe only) reason for serious problems with the NUTS sampling is this: sampling issues arise for bad models. So, let’s come up with a really stupid model.\nHere’s a model that is like the previous but adds a second predictor., This second predictor is intended to be a normal (non-smoothed) regression coefficient that is almost identical to the original year information. You may already intuit that this cannot possibly be a good idea; the model is notionally deficient. So, we expect nightmares during sampling:\n\n\nToggle code\nfit_bad &lt;-\n  brms::brm(\n  formula = avg_temp ~ s(year) + year_perturbed, \n  data = aida::data_WorldTemp |&gt; mutate(year_perturbed = rnorm(1,year,0.001)),\n  seed = 1969\n) \n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\nThis model is deliberately set up to be stupid (and to mislead you). If you don’t like being held in the dark, try to find the mistake already.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSee below.\n\n\n\n\n\n\n\n\nToggle code\nsummary(fit_bad)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ s(year) + year_perturbed \n   Data: mutate(aida::data_WorldTemp, year_perturbed = rnor (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     3.62      1.08     2.03     6.18 1.00     1171     1682\n\nPopulation-Level Effects: \n                        Estimate         Est.Error           l-95% CI\nIntercept      -2693691333829.80 12145265317737.51 -37056374746302.24\nyear_perturbed     1539251474.03     6940148378.55     -7252904977.78\nsyear_1                    14.61              2.34              10.19\n                        u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      12692589621258.01 2.94        5       12\nyear_perturbed    21175061423.71 2.94        5       12\nsyear_1                    19.36 1.00     2769     2729\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.01     0.30     0.36 1.00     3642     2568\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIndeed, that looks pretty bad. We managed to score badly on all major accounts:\n\nlarge \\(\\hat{R}\\)\nextremely poor efficient sample size\nridiculously far ranging posterior estimates for the main model components\ntons of divergent transitions\nmaximum treedepth reached more often than hipster touches their phone in a week\n\nSome of these caterpillars look like they are in a vicious rose war:\n\n\nToggle code\nplot(fit_bad)\n\n\n\n\n\nWe also see that that the intercept of and the slope for year_perturbed are the main troublemakers (in terms of traceplots).\nInterestingly, a simple posterior check doesn’t look too bad:\n\n\nToggle code\npp_check(fit_bad)\n\n\n\n\n\nThis shows that the warning messages (from Stan) shoult be taken seriously. The samples cannot be trusted, even if a posterior predictive check looks agreeable.\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\nExtract information about \\(\\hat{R}\\) and the ratio of efficient samples with functions brms::rhat and brms::neff_ratio.\nInterpret what you see: why are these numbers not good.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor R-hat, we do:\n\n\nToggle code\nbrms::rhat(fit_bad)\n\n\n     b_Intercept b_year_perturbed       bs_syear_1      sds_syear_1 \n       2.9426863        2.9426863        1.0025218        1.0037162 \n           sigma     s_syear_1[1]     s_syear_1[2]     s_syear_1[3] \n       1.0007585        1.0018977        1.0006639        1.0020012 \n    s_syear_1[4]     s_syear_1[5]     s_syear_1[6]     s_syear_1[7] \n       1.0005904        1.0012444        1.0005373        1.0005873 \n    s_syear_1[8]           lprior             lp__ \n       0.9999082        1.0037192        1.0047583 \n\n\nThese numbers are bad, since they ought to be close to 1, which is the ideal case when chains are indistinguishable (roughly put).\nFor the efficient-sample ratio:\n\n\nToggle code\nbrms::neff_ratio(fit_bad)\n\n\n     b_Intercept b_year_perturbed       bs_syear_1      sds_syear_1 \n     0.001153224      0.001153224      0.682138561      0.292844859 \n           sigma     s_syear_1[1]     s_syear_1[2]     s_syear_1[3] \n     0.641937607      0.645693901      0.521896532      0.756250723 \n    s_syear_1[4]     s_syear_1[5]     s_syear_1[6]     s_syear_1[7] \n     0.585514962      0.790196797      0.832935697      0.678273027 \n    s_syear_1[8]           lprior             lp__ \n     0.790823858      0.293125594      0.242826922 \n\n\nThese numbers are also poor, because we would like them, ideally, to be 1. However, low efficiency of samples is not necessary a sign that the fit cannot be trusted, just that the sampler has a hard time beating autocorrelation.\n\n\n\n\n\n\nHave a look at the pairs() plot:\n\n\nToggle code\npairs(fit_bad)\n\n\n\n\n\nAha, there we see a clear problem! The joint posterior for the intercept and the slope for year_perturbed looks like a line. This means that these parameters could in principle do the same “job”.\nThis suggests a possible solution strategy. The model is too unconstrained. It can allow these two parameters meander to wherever they want (or so it seems). We could therefore try honing them in by specifying priors, like so:\n\n\nToggle code\nfit_bad_wPrior &lt;- brm(\n  formula = avg_temp ~ s(year) + year_perturbed, \n  data = aida::data_WorldTemp |&gt; mutate(year_perturbed = rnorm(1,year,0.001)),\n  seed = 1969,\n  prior = prior(\"student_t(1,0,5)\", coef = \"year_perturbed\")\n) \n\nsummary(fit_bad_wPrior)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ s(year) + year_perturbed \n   Data: mutate(aida::data_WorldTemp, year_perturbed = rnor (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     3.61      1.09     1.93     6.17 1.01      774     1324\n\nPopulation-Level Effects: \n               Estimate Est.Error  l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept       1710.68  56636.65 -67751.31 74280.43 1.01     3201      846\nyear_perturbed    -0.97     32.36    -42.44    38.72 1.01     3201      846\nsyear_1           14.39      2.32      9.84    18.86 1.00     2122     1889\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.01     0.30     0.36 1.00     3433     2978\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWell, alright! That isn’t too bad anymore. But it is still clear from the posterior pairs plot that this model has two parameters that steal each other’s show. The model remains a bad model … for our data.\n\n\nToggle code\npairs(fit_bad)\n\n\n\n\n\nHere’s what’s wrong: year_perturbed is a constant! The model is a crappy model of the data, because the data is not what we thought it would be. Check it out:\n\n\nToggle code\naida::data_WorldTemp |&gt; mutate(year_perturbed = rnorm(1,year,0.001))\n\n\n# A tibble: 269 × 5\n    year anomaly uncertainty avg_temp year_perturbed\n   &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n 1  1750  -1.41        NA        7.20          1750.\n 2  1751  -1.52        NA        7.09          1750.\n 3  1753  -1.07         1.3      7.54          1750.\n 4  1754  -0.614        1.09     8.00          1750.\n 5  1755  -0.823        1.24     7.79          1750.\n 6  1756  -0.547        1.28     8.06          1750.\n 7  1757  -0.438        1.31     8.17          1750.\n 8  1758  -2.42         1.76     6.19          1750.\n 9  1759  -1.53         2.25     7.08          1750.\n10  1760  -2.46         2.75     6.14          1750.\n# ℹ 259 more rows\n\n\nSo, we basically ran a model with two intercepts!?! 😳\nLet’s try again:\n\n\nToggle code\ndata_WorldTemp_perturbed &lt;- aida::data_WorldTemp |&gt; \n    mutate(year_perturbed = rnorm(nrow(aida::data_WorldTemp),year, 50))\ndata_WorldTemp_perturbed\n\n\n# A tibble: 269 × 5\n    year anomaly uncertainty avg_temp year_perturbed\n   &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n 1  1750  -1.41        NA        7.20          1705.\n 2  1751  -1.52        NA        7.09          1753.\n 3  1753  -1.07         1.3      7.54          1761.\n 4  1754  -0.614        1.09     8.00          1769.\n 5  1755  -0.823        1.24     7.79          1769.\n 6  1756  -0.547        1.28     8.06          1758.\n 7  1757  -0.438        1.31     8.17          1805.\n 8  1758  -2.42         1.76     6.19          1752.\n 9  1759  -1.53         2.25     7.08          1777.\n10  1760  -2.46         2.75     6.14          1759.\n# ℹ 259 more rows\n\n\nThat’s more like what we thought it was: year_perturbed is supposed to be noisy version of the actual year. So, let’s try again, leaving out the smoothing, just for some more chaos-loving fun:\n\n\nToggle code\nfit_bad_2 &lt;- brm(\n  formula = avg_temp ~ year + year_perturbed, \n  data = data_WorldTemp_perturbed,\n  seed = 1969,\n  prior = prior(\"student_t(1,0,5)\", coef = \"year_perturbed\")\n) \n\nsummary(fit_bad_2)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: avg_temp ~ year + year_perturbed \n   Data: data_WorldTemp_perturbed (Number of observations: 269) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         -3.50      0.58    -4.65    -2.37 1.00     3989     3346\nyear               0.01      0.00     0.01     0.01 1.00     3864     2228\nyear_perturbed     0.00      0.00    -0.00     0.00 1.00     3660     2684\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.41      0.02     0.37     0.44 1.00     1267     1130\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThere are no warnings, so this model must be good, right? – No!\nIf we check the pairs plot, we see that we now have introduced a fair correlation between the two predictor variables.\n\n\nToggle code\npairs(fit_bad_2)\n\n\n\n\n\nWe should just not have year_perturbed; it’s nonsense, and it shows in the diagnostics.\nYou can diagnose more using shinystan:\n\n\nToggle code\nshinystan::launch_shinystan(fit_bad)"
  },
  {
    "objectID": "practice-sheets/00b-wrangling-plotting.html",
    "href": "practice-sheets/00b-wrangling-plotting.html",
    "title": "Wrangling & plotting",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}"
  },
  {
    "objectID": "practice-sheets/00b-wrangling-plotting.html#loading-and-inspecting-the-data",
    "href": "practice-sheets/00b-wrangling-plotting.html#loading-and-inspecting-the-data",
    "title": "Wrangling & plotting",
    "section": "Loading and inspecting the data",
    "text": "Loading and inspecting the data\nThe data is part of the aida package, but we can give it a fancy new name:\n\n\nToggle code\ndolphin &lt;- aida::data_MT\n\n\nTo get some information about the data set, we can use the help function:\n\n\nToggle code\nhelp(\"data_MT\")\n\n\nHere is some more information we can get about the data:\n\n\nToggle code\n# number of rows in the data set\nnrow(dolphin)\n\n\n[1] 2052\n\n\nToggle code\n# number of columns in the data set\nncol(dolphin)\n\n\n[1] 16\n\n\nToggle code\n# names of the columns\nnames(dolphin)\n\n\n [1] \"X1\"               \"trial_id\"         \"MAD\"              \"AUC\"             \n [5] \"xpos_flips\"       \"RT\"               \"prototype_label\"  \"subject_id\"      \n [9] \"group\"            \"condition\"        \"exemplar\"         \"category_left\"   \n[13] \"category_right\"   \"category_correct\" \"response\"         \"correct\"         \n\n\nToggle code\n# number of unique `subject_id`s\ndolphin$subject_id |&gt; unique() |&gt; length()\n\n\n[1] 108\n\n\nToggle code\n# number of types each subject saw different `conditions`\ndolphin |&gt; with(table(subject_id, condition)) |&gt; head()\n\n\n          condition\nsubject_id Atypical Typical\n      1001        6      13\n      1002        6      13\n      1003        6      13\n      1004        6      13\n      1005        6      13\n      1006        6      13"
  },
  {
    "objectID": "practice-sheets/00b-wrangling-plotting.html#a-closer-look-at-the-columns",
    "href": "practice-sheets/00b-wrangling-plotting.html#a-closer-look-at-the-columns",
    "title": "Wrangling & plotting",
    "section": "A closer look at the columns",
    "text": "A closer look at the columns\nLet’s take a closer look at the columns and the information inside them.\nWe can get a glimpse of all columns like so:\n\n\nToggle code\nglimpse(dolphin)\n\n\nRows: 2,052\nColumns: 16\n$ X1               &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16~\n$ trial_id         &lt;chr&gt; \"id0001\", \"id0002\", \"id0003\", \"id0004\", \"id0005\", \"id~\n$ MAD              &lt;dbl&gt; 82.53319, 44.73484, 283.48207, 138.94863, 401.93988, ~\n$ AUC              &lt;dbl&gt; 40169.5, 13947.0, 84491.5, 74084.0, 223083.0, 308376.~\n$ xpos_flips       &lt;dbl&gt; 3, 1, 2, 0, 2, 2, 1, 0, 2, 0, 2, 2, 0, 0, 3, 1, 0, 1,~\n$ RT               &lt;dbl&gt; 950, 1251, 930, 690, 951, 1079, 1050, 830, 700, 810, ~\n$ prototype_label  &lt;chr&gt; \"straight\", \"straight\", \"curved\", \"curved\", \"cCoM\", \"~\n$ subject_id       &lt;dbl&gt; 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001,~\n$ group            &lt;chr&gt; \"touch\", \"touch\", \"touch\", \"touch\", \"touch\", \"touch\",~\n$ condition        &lt;chr&gt; \"Atypical\", \"Typical\", \"Atypical\", \"Atypical\", \"Typic~\n$ exemplar         &lt;chr&gt; \"eel\", \"rattlesnake\", \"bat\", \"butterfly\", \"hawk\", \"pe~\n$ category_left    &lt;chr&gt; \"fish\", \"amphibian\", \"bird\", \"Insekt\", \"bird\", \"fish\"~\n$ category_right   &lt;chr&gt; \"reptile\", \"reptile\", \"mammal\", \"bird\", \"reptile\", \"b~\n$ category_correct &lt;chr&gt; \"fish\", \"reptile\", \"mammal\", \"Insekt\", \"bird\", \"bird\"~\n$ response         &lt;chr&gt; \"fish\", \"reptile\", \"bird\", \"Insekt\", \"bird\", \"bird\", ~\n$ correct          &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,~\n\n\nHere is a quick explanation of all the different columns:\n\ntrial_id = unique id for individual trials\nMAD = maximal deviation into competitor space\nAUC = area under the curve\nxpos_flips = the amount of horizontal direction changes\nRT = reaction time in ms\nprototype_label = different categories of prototypical movement strategies\nsubject_id = unique id for individual participants\ngroup = groups differ in the response design (click vs. touch)\ncondition = category membership (Typical vs. Atypical)\nexemplar = the concrete animal\ncategory_left = the category displayed on the left\ncategory_right = the category displayed on the right\ncategory_correct= the category that is correct\nresponse = the selected category\ncorrect = whether or not the response matches category_correct"
  },
  {
    "objectID": "practice-sheets/00b-wrangling-plotting.html#selecting-columns",
    "href": "practice-sheets/00b-wrangling-plotting.html#selecting-columns",
    "title": "Wrangling & plotting",
    "section": "Selecting columns",
    "text": "Selecting columns\nFor now, we are only interested in columns RT, group, condition, category_correct, and correct. We can use the select() function of dplyr to get rid of columns we don’t need.\n\n\nToggle code\n# selecting specific columns\ndolphin_selected &lt;-\n  dolphin |&gt;\n  dplyr::select(RT, group, condition, category_correct, correct)\n \n# let's have a look\ndolphin_selected\n\n\n# A tibble: 2,052 x 5\n      RT group condition category_correct correct\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n 1   950 touch Atypical  fish                   1\n 2  1251 touch Typical   reptile                1\n 3   930 touch Atypical  mammal                 0\n 4   690 touch Atypical  Insekt                 1\n 5   951 touch Typical   bird                   1\n 6  1079 touch Atypical  bird                   1\n 7  1050 touch Typical   fish                   1\n 8   830 touch Typical   fish                   1\n 9   700 touch Typical   mammal                 1\n10   810 touch Typical   fish                   1\n# i 2,042 more rows"
  },
  {
    "objectID": "practice-sheets/00b-wrangling-plotting.html#filtering-rows",
    "href": "practice-sheets/00b-wrangling-plotting.html#filtering-rows",
    "title": "Wrangling & plotting",
    "section": "Filtering rows",
    "text": "Filtering rows\nIf we care only about a subset of rows, we can use the filter() function. For example, let’s filter all trials in which the correct category was either a fish or a mammal\n\n\nToggle code\ndolphin_filter1 &lt;-\n  dolphin_selected |&gt; \n  filter(category_correct == \"fish\" | category_correct == \"mammal\")\n  # the | is a logical operator that indicates that either the first expression OR \n  # the second one has to be true\n\ndolphin_filter1\n\n\n# A tibble: 1,296 x 5\n      RT group condition category_correct correct\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n 1   950 touch Atypical  fish                   1\n 2   930 touch Atypical  mammal                 0\n 3  1050 touch Typical   fish                   1\n 4   830 touch Typical   fish                   1\n 5   700 touch Typical   mammal                 1\n 6   810 touch Typical   fish                   1\n 7  1264 touch Typical   mammal                 1\n 8   890 touch Atypical  mammal                 0\n 9  1040 touch Typical   mammal                 1\n10   730 touch Typical   mammal                 1\n# i 1,286 more rows\n\n\nYou can also filter() against particular conditions. For example, let’s filter all rows that do not have bird as their correct category:\n\n\nToggle code\ndolphin_filter2 &lt;-\n  dolphin_selected |&gt; \n  filter(category_correct != \"bird\")\n\ndolphin_filter2\n\n\n# A tibble: 1,728 x 5\n      RT group condition category_correct correct\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n 1   950 touch Atypical  fish                   1\n 2  1251 touch Typical   reptile                1\n 3   930 touch Atypical  mammal                 0\n 4   690 touch Atypical  Insekt                 1\n 5  1050 touch Typical   fish                   1\n 6   830 touch Typical   fish                   1\n 7   700 touch Typical   mammal                 1\n 8   810 touch Typical   fish                   1\n 9  1264 touch Typical   mammal                 1\n10   890 touch Atypical  mammal                 0\n# i 1,718 more rows\n\n\nWe can also filter according to multiple conditions at once, including numeric conditions. Here, we also filter for trials that have correct responses.\n\n\nToggle code\ndolphin_filter3 &lt;-\n  dolphin_selected |&gt; \n  filter(category_correct != \"bird\",\n         correct == 1)\n\ndolphin_filter3\n\n\n# A tibble: 1,602 x 5\n      RT group condition category_correct correct\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n 1   950 touch Atypical  fish                   1\n 2  1251 touch Typical   reptile                1\n 3   690 touch Atypical  Insekt                 1\n 4  1050 touch Typical   fish                   1\n 5   830 touch Typical   fish                   1\n 6   700 touch Typical   mammal                 1\n 7   810 touch Typical   fish                   1\n 8  1264 touch Typical   mammal                 1\n 9  1040 touch Typical   mammal                 1\n10   730 touch Typical   mammal                 1\n# i 1,592 more rows"
  },
  {
    "objectID": "practice-sheets/00b-wrangling-plotting.html#grouping-and-summarizing",
    "href": "practice-sheets/00b-wrangling-plotting.html#grouping-and-summarizing",
    "title": "Wrangling & plotting",
    "section": "Grouping and summarizing",
    "text": "Grouping and summarizing\nWe can also generate summary statistics of certain variables with a combination of group_by() & summarise(). Let’s get the means and standard deviations of the reactions times for each level in the variable condition. We also include the minimum and maximum values for each condition.\n\n\nToggle code\ndolphin_aggregate &lt;-\n  dolphin_filter3 |&gt;\n  group_by(condition) |&gt;\n  summarise(\n    min_RT  = min(RT),\n    mean_RT = mean(RT, na.rm = T),\n    sd_RT   = sd(RT, na.rm = T),\n    max_RT  = max(RT)\n    )\n  # the na.rm = T is an argument that is used to tell R that NAs should be ignored \n  # when calculating the summary statistics\n\n# show the aggregated df\ndolphin_aggregate\n\n\n# A tibble: 2 x 5\n  condition min_RT mean_RT sd_RT max_RT\n  &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Atypical     630   2149. 1840.  19903\n2 Typical      510   1665. 1283.  20685\n\n\nSo we find that atypical categories are responded to slower than typical categories. Makes sense. Identifying a dolphin as a mammal might be difficult because it shares a lot of features with fish.\nWe can group according to many different factors simultaneously, and we can create multiple summary statistics at the same time. Here, we get summary statistics for each combination of all levels in variables condition and group. We use the tidyboot package to get bootstrapped 95% confidence intervalls. (Notice that these are more informative than standard deviations in the sense that they give an upper and lower deviation, not just one number for both directions, which can be misleading when the data is skewed (like reaction times typically are)):\n\n\nToggle code\ndolphin_aggregate2 &lt;-\n  dolphin_filter3 |&gt;\n  group_by(group, condition) |&gt;\n  summarize(\n    lower_CI = tidyboot::ci_lower(RT),\n    mean_RT  = mean(RT, na.rm = T),\n    upper_CI = tidyboot::ci_upper(RT)\n    )\n\n# show the aggregated df\ndolphin_aggregate2\n\n\n# A tibble: 4 x 5\n# Groups:   group [2]\n  group condition lower_CI mean_RT upper_CI\n  &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 click Atypical     1030.   2417.    7311.\n2 click Typical       950    1847.    4698.\n3 touch Atypical      750    1900.    5368.\n4 touch Typical       686.   1486.    3955.\n\n\nWe can see here that the group that needed to click on a response are overall slower than touch responses, but also much more variable in their behavior."
  },
  {
    "objectID": "practice-sheets/00b-wrangling-plotting.html#changing-and-adding-columns",
    "href": "practice-sheets/00b-wrangling-plotting.html#changing-and-adding-columns",
    "title": "Wrangling & plotting",
    "section": "Changing and adding columns",
    "text": "Changing and adding columns\nOften, we are interested in standardized measures because we do not know what a value of 1 means on any given scale. Is 1 a large difference or a small difference? For example when we want to explore the impact of several predictors on the same measurement, we want to know the relative size of a number. To achieve this, we standardize measures by dividing their mean by their respective standard deviations. We will use the scale() function for this and create a new variable in our data frame via mutate().\n(Note that the scale() function creates an object that is of the matrix class. That is fine for the most part but might create issues later on. To avoid any issues, we wrap the scale() function in as.numeric() to store the results as a numeric vector.)\n\n\nToggle code\ndolphin_standardize &lt;-\n  dolphin_selected |&gt;\n  mutate(RT_scale = as.numeric(scale(RT)))\n  \nhead(dolphin_standardize)\n\n\n# A tibble: 6 x 6\n     RT group condition category_correct correct RT_scale\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt;\n1   950 touch Atypical  fish                   1   -0.552\n2  1251 touch Typical   reptile                1   -0.373\n3   930 touch Atypical  mammal                 0   -0.564\n4   690 touch Atypical  Insekt                 1   -0.706\n5   951 touch Typical   bird                   1   -0.551\n6  1079 touch Atypical  bird                   1   -0.475\n\n\nIf we now compare, say atypical and typical categories according to reaction times, we can use the standardized RT ratings. Let’s do all of this in one “pipeline”.\n\n\nToggle code\ndolphin_agg_standardize &lt;- dolphin_selected |&gt;\n  mutate(RT_scale = scale(RT)) |&gt; \n  group_by(condition) |&gt;\n  summarise(mean_RT_scale = mean(RT_scale, na.rm = T))\n  \nhead(dolphin_agg_standardize)\n\n\n# A tibble: 2 x 2\n  condition mean_RT_scale\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Atypical          0.219\n2 Typical          -0.101\n\n\nNow we can see that atypical categories exhibit relatively higher RTs, i.e., more than 0.3 standard deviations higher than for typical categories."
  },
  {
    "objectID": "practice-sheets/00b-wrangling-plotting.html#exercises-for-data-wrangling",
    "href": "practice-sheets/00b-wrangling-plotting.html#exercises-for-data-wrangling",
    "title": "Wrangling & plotting",
    "section": "Exercises for data wrangling",
    "text": "Exercises for data wrangling\n\nExercise 1\n\n\n\n\n\n\nExercise 1a\n\n\n\n\n\nTake the dolphin data set and store a reduced variant of it as dolphin_reduced. The new data frame should contain only the following columns: RT, AUC, group, and exemplar.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_reduced &lt;- dolphin |&gt;\n  select(RT, AUC, group, exemplar)\n\nhead(dolphin_reduced)\n\n\n# A tibble: 6 x 4\n     RT     AUC group exemplar   \n  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      \n1   950  40170. touch eel        \n2  1251  13947  touch rattlesnake\n3   930  84492. touch bat        \n4   690  74084  touch butterfly  \n5   951 223083  touch hawk       \n6  1079 308376  touch penguin    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b\n\n\n\n\n\nWe are for now only interested in those data that have whales as the exemplar. filter() only those rows and store them in a new dataframe called whales_only.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nwhales_only &lt;- dolphin_reduced |&gt; \n  filter(exemplar == \"whale\")\n\nhead(whales_only)\n\n\n# A tibble: 6 x 4\n     RT     AUC group exemplar\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   \n1   760  13498  touch whale   \n2  1990  42404  click whale   \n3  3613 -10167  click whale   \n4  2030 162678  touch whale   \n5  1490  54054  touch whale   \n6  1305  74222. touch whale   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1c\n\n\n\n\n\nNow filter for only those data that have RTs below 1500ms.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nwhales_only2 &lt;- whales_only |&gt; \n  filter(RT &lt; 1500)\n\nhead(whales_only2)\n\n\n# A tibble: 6 x 4\n     RT     AUC group exemplar\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   \n1   760  13498  touch whale   \n2  1490  54054  touch whale   \n3  1305  74222. touch whale   \n4  1350   -643  touch whale   \n5  1040  19426. touch whale   \n6  1141 -44260. click whale   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1d\n\n\n\n\n\nWe don’t like that AUC is unstandardized. Use mutate() to create a new vector that represents scaled AUC values (scaling is achieved by the function scale()).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nwhales_only_scaled &lt;- whales_only2 |&gt; \n  mutate(AUC_scaled = scale(AUC))\n\nhead(whales_only_scaled)\n\n\n# A tibble: 6 x 5\n     RT     AUC group exemplar AUC_scaled[,1]\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;\n1   760  13498  touch whale           -0.456 \n2  1490  54054  touch whale           -0.212 \n3  1305  74222. touch whale           -0.0912\n4  1350   -643  touch whale           -0.541 \n5  1040  19426. touch whale           -0.420 \n6  1141 -44260. click whale           -0.803 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1e\n\n\n\n\n\nCalculate the mean scaled AUC ratings for both both groups.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nwhales_aggregate &lt;- whales_only_scaled |&gt; \n  group_by(group) |&gt; \n  summarise(mean_AUC_scaled = mean(AUC_scaled, na.rm =TRUE))\n\nhead(whales_aggregate)\n\n\n# A tibble: 2 x 2\n  group mean_AUC_scaled\n  &lt;chr&gt;           &lt;dbl&gt;\n1 click           0.798\n2 touch          -0.372\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1f\n\n\n\n\n\nDo all of the above (a-e) in one pipeline.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nwhales_aggregate &lt;- dolphin |&gt;\n  select(RT, AUC, group, exemplar) |&gt; \n  filter(exemplar == \"whale\",\n         RT &lt; 1500) |&gt; \n  mutate(AUC_scaled = scale(AUC)) |&gt; \n  group_by(group) |&gt; \n  summarise(mean_AUC_scaled = mean(AUC_scaled, na.rm =TRUE))\n  \nhead(whales_aggregate)\n\n\n# A tibble: 2 x 2\n  group mean_AUC_scaled\n  &lt;chr&gt;           &lt;dbl&gt;\n1 click           0.798\n2 touch          -0.372\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\nExercise 2a\n\n\n\n\n\nTake the dolphin data set and store a reduced variant of it. The new data frame should contain only the columns condition, group, and xpos_flips, correct. And within the correct vector, we are only interested in the correct trials (= 1). Filter accordingly.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_sub &lt;- dolphin |&gt; \n  select(condition, group, xpos_flips, correct) |&gt; \n  filter(correct == 1)\n\nhead(dolphin_sub)\n\n\n# A tibble: 6 x 4\n  condition group xpos_flips correct\n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1 Atypical  touch          3       1\n2 Typical   touch          1       1\n3 Atypical  touch          0       1\n4 Typical   touch          2       1\n5 Atypical  touch          2       1\n6 Typical   touch          1       1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2b\n\n\n\n\n\nCreate an aggregated data frame that contains the mean xpos_flips value and the standard deviation for group and condition.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_agg &lt;- dolphin_sub |&gt;\n  group_by(group, condition) |&gt; \n  summarise(mean_xpos_flips = mean(xpos_flips, na.rm = TRUE),\n            sd_xpos_flips = sd(xpos_flips, na.rm = TRUE))\n\nhead(dolphin_agg)\n\n\n# A tibble: 4 x 4\n# Groups:   group [2]\n  group condition mean_xpos_flips sd_xpos_flips\n  &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n1 click Atypical            1.33          1.23 \n2 click Typical             0.956         1.05 \n3 touch Atypical            0.797         1.10 \n4 touch Typical             0.572         0.938\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2c\n\n\n\n\n\nUse the rename() function to rename the new vectors for the mean xflips and their standard deviation to xflips_mean and xflips_sd.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin_agg2 &lt;- dolphin_agg |&gt;\n  rename(xflips_mean = mean_xpos_flips,\n         xflips_sd = sd_xpos_flips)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2d\n\n\n\n\n\nDo all of the above (a-c) in one pipeline.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\ndolphin |&gt; \n  select(condition, group, xpos_flips, correct) |&gt; \n  filter(correct == 1) |&gt; \n  group_by(group, condition) |&gt; \n  summarise(mean_xpos_flips = mean(xpos_flips, na.rm = TRUE),\n            sd_xpos_flips = sd(xpos_flips, na.rm = TRUE)) |&gt; \n  rename(xflips_mean = mean_xpos_flips,\n         xflips_sd = sd_xpos_flips)\n\n\n# A tibble: 4 x 4\n# Groups:   group [2]\n  group condition xflips_mean xflips_sd\n  &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n1 click Atypical        1.33      1.23 \n2 click Typical         0.956     1.05 \n3 touch Atypical        0.797     1.10 \n4 touch Typical         0.572     0.938"
  },
  {
    "objectID": "practice-sheets/00b-wrangling-plotting.html#basic-plots",
    "href": "practice-sheets/00b-wrangling-plotting.html#basic-plots",
    "title": "Wrangling & plotting",
    "section": "Basic plots",
    "text": "Basic plots\nNow that we have pre-processed our data set, we are ready to visually explore it. Let’s start very simple. Let’s plot a bar plot. Let’s also add a title to our plot.\n\n\nToggle code\nggplot(dolphin_agg, aes(x = condition, y = mean_RT)) +\n  geom_bar(stat = \"identity\") +\n  ggtitle(\"a bare bar plot\")\n\n\n\n\n\n\n\n\n\nToggle code\n  # stat = \"identity\" takes the number in the dataset as the bar height (as opposed to a 'count')\n\n\nUgh! What an ugly plot, right? But it’s already telling a story: Atypical categories are responded to slower than typical categories. Let’s add a measure of uncertainty, in our case the bootstrapped 95% confidence intervals, as error bars to the plot:\n\n\nToggle code\nggplot(dolphin_agg, aes(x = condition, y = mean_RT)) +\n  geom_bar(stat = \"identity\") + \n  \n  # this is the added layer\n  geom_errorbar(aes(ymin = lower_CI, \n                    ymax = upper_CI), \n                colour = \"black\",\n                linewidth = 0.5) +\n  \n  ggtitle(\"a bare bar plot with error bars\")\n\n\n\n\n\nWe can observe a couple of things here. First, ggplot automatically adjust the axes based on the elements to be plotted unless we tell it not to. Second, the error bars are plotted in front of the bars, i.e. closer to the viewer. This visual ordering reflects the order of layers. We first plotted the bars and THEN the error bars.\nBeyond bar plots, we can create other useful plots types. For example a point plot. Instead of a bar, we plot the mean RT as points.\n\n\nToggle code\nggplot(dolphin_agg, aes(x = condition, y = mean_RT)) +\n  geom_errorbar(aes(ymin = lower_CI, \n                    ymax = upper_CI), \n                colour = \"black\") +  \n  # this is the new geom \n  geom_point() +\n  ggtitle(\"a point plot\")\n\n\n\n\n\n\n\n\n\nOr a line plot that connects the means with a line. For the line plot to work, we need to indicate a group aesthetic, i.e. the group that we want to connect with a line. If you have for example several interacting categories, you need to indicate which groups are supposed to be connected with lines (see below). Because we have only one group here, condition, we set group to 1.\n\n\nToggle code\nggplot(dolphin_agg, aes(x = condition, y = mean_RT, group = 1)) +\n  geom_line() +\n  ggtitle(\"a line plot\")\n\n\n\n\n\n\n\n\n\nYay, we are on a roll. Let’s plot a box plot. Remember the box shows the median (middle vertical line) and the interquartile range (the middle 50% of the data within the box). Note that for the box plot, we do not plot aggregated values, so we need to refer to the entire data set. We also add the aesthetic fill here and set it to the variable condition to color code our boxes.\n\n\nToggle code\n# we changed the dataset referred to\nggplot(dolphin, aes(x = condition, y = RT, fill = condition)) +\n  # this is the new geom \n  geom_boxplot() +\n  ggtitle(\"a box plot\")\n\n\n\n\n\n\n\n\n\nWhile the above plots illustrate one continuous variable (RT) plotted against a categorical variable (condition), we can also plot two continuous variables against each other. For example, we could plot RT against AUC in a scatter plot.\n\n\nToggle code\n# we changed the y aesthetic to `Hardness`\nggplot(dolphin_subset, aes(x = RT, y = AUC)) +\n  geom_point() +\n  ggtitle(\"a scatter plot\")\n\n\n\n\n\n\n\n\n\nFinally, one central plot type for our class. The density plot. It plots on the x-axis a continous value and on the y-axis the “density”” of these values. So high values on the y-axis means a lot of data at the corresponding x-values. The density curve can be outlined with color and filled with fill. To keep the two categories visually distinct, we add an argument to the geom_density() function: alpha. Alpha controls the transparency of the color! We will see a lot of these density plots in our class.\n\n\nToggle code\nggplot(dolphin, aes(x = RT, color = condition, fill = condition)) +\n  geom_density(alpha = 0.5) +\n  ggtitle(\"a density plot\")"
  },
  {
    "objectID": "practice-sheets/00b-wrangling-plotting.html#adjusting-plot-elements",
    "href": "practice-sheets/00b-wrangling-plotting.html#adjusting-plot-elements",
    "title": "Wrangling & plotting",
    "section": "Adjusting plot elements",
    "text": "Adjusting plot elements\nOkay, so we are now already capable of exploring our data visually with a bunch of plots. These plots are exceptionally ugly and of limited communicative value so far. Note that this is perfectly fine during an exploratory phase of data analysis. If we just eye-ball data and we have no trouble interpreting the plots, thats just fine. However, as soon as we want to communicate patterns to others with these graphs, we need to take a little bit more care of its communicative value. Let’s look at ways we can tailor our plots to the needs of an audience.\nLet’s go back to our bar plot and explore whether condition and group has an impact on RT?\n\n\nToggle code\n# First we aggregate RT for group and condition\ndolphin_agg2 &lt;- dolphin_subset |&gt;\n  group_by(group, condition) |&gt;\n  summarise(mean_RT = mean(RT),\n            sd_RT = sd(RT))\n\n# then we plot and color code for condition (note that, for bar plots, the aesthetic of `color` refers to the border of the bar, and `fill` refers to the actual colour of the bar)\n\nggplot(dolphin_agg2, aes(x = condition, y = mean_RT, fill = group)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\nHm… that doesn’t work out, the bars are on top of each other, we need to tell ggplot to position the bars next to each other instead. We do that with position_dogde(). Note that ggplot assigns a default color coding scheme to your plots if you don’t specify it by hand.\n\n\nToggle code\nggplot(dolphin_agg2, aes(x = condition, y = mean_RT, fill = group)) +\n  geom_bar(stat = \"identity\", position = position_dodge())\n\n\n\n\n\n\n\n\n\nAwww much better! Alternatively, we can plot the two categories into separate panels. We achieve this by facetting using the facet_grid() function.\n\n\nToggle code\nggplot(dolphin_agg2, aes(x = group, y = mean_RT, fill = condition)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  # this is the facetting function\n  facet_grid(~ condition)\n\n\n\n\n\n\n\n\n\nOkay. We are getting somewhere. We already learned something again. Apparently, the effect of typiciality on RT is pretty similar across tasks. It does not look like we have an interaction here (more on interactions later).\nNow let’s make these plots ready to communicate information. We add appropriate axes titles. Note that we are using a little hack here: The “” inserts an empty line, creating visual distance from axis title to axis, thus making it easier to read it. Our audience will thank us.\n\n\nToggle code\nggplot(dolphin_agg2, aes(x = group, y = mean_RT, fill = condition)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  facet_grid(~ condition) +\n  # add axis titles\n  xlab(\"\\n task\") +\n    ylab(\"mean response latency in ms\\n\") \n\n\n\n\n\n\n\n\n\nThe same graph as a point / line plot indicated to the audience whether there is an interaction pattern or not. Note that here we do not facet because we actually want the points to be plotted within the same horizontal space. We also have to specify the group aesthetic to tell ggplot which points to connect with lines.\n\n\nToggle code\nggplot(dolphin_agg2, aes(x = group, y = mean_RT, color = condition, group = condition)) +\n  # instead of geom_bar we use geom_point and geom_line\n  geom_point(size = 12) +\n  geom_line(size = 2) +\n  xlab(\"\\n task\") +\n  ylab(\"mean response latency in ms\\n\") \n\n\n\n\n\n\n\n\n\nToggle code\n  # # need to change to the color aesthetic instead of fill\n  # scale_y_continuous(expand = c(0, 0), breaks = (c(0, 500, 1000, 1500, 2000, 2500, 3000)), limits = c(0,3000))\n\n\nThese lines look pretty parallel and don’t indicate a strong interaction pattern. But how do different exemplars differ? Let’s aggregate for individual exemplars first and then create the same plot for the means of all exemplars.\n\n\nToggle code\ndolphin_agg3 &lt;- dolphin_subset |&gt; \n  group_by(exemplar, group, condition) |&gt; \n  summarise(mean_RT = mean(RT, na.rm = TRUE))\n\nggplot(dolphin_agg3, aes(x = group, y = mean_RT, color = condition, group = exemplar)) +\n  # instead of geom_bar we use geom_point and geom_line\n  geom_point(size = 6, alpha = 0.3) +\n  geom_line() +\n  geom_label(aes(label = exemplar)) +\n  xlab(\"\\n task\") +\n  ylab(\"mean response latency in ms\\n\")\n\n\n\n\n\n\n\n\n\nIt looks like “shark” and “rattlesnake” behave very different from their buddies in the typical condition. Interesting! We wouldn’t have noticed if we had only looked at the overall means."
  },
  {
    "objectID": "practice-sheets/00b-wrangling-plotting.html#exercises-for-plotting",
    "href": "practice-sheets/00b-wrangling-plotting.html#exercises-for-plotting",
    "title": "Wrangling & plotting",
    "section": "Exercises for plotting",
    "text": "Exercises for plotting\nTake the scatter plot below as a departure point. It plots AUC (area-under-the-curve) against MAD (maximal absolute deviation).\n\n\nToggle code\nggplot(dolphin, aes(x = MAD, y = AUC)) +\n  geom_point() +\n  ggtitle(\"a scatter plot\")\n\n\n\n\n\n\n\n\n\n\n\nExercise 3a\n\n\n\n\n\n\nChange both the x-axis and the y-axis title to sensible and informative titles.\nChange the plot title to something informative.\nChange the scaling of the x-axis to display only MAD values between -500 and 500\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nggplot(dolphin, aes(x = MAD, y = AUC, \n                    color = group)) +\n  geom_point() +\n  # (1) axes titles\n  xlab(\"\\n maximal absolute deviation\") +\n  ylab(\"area-under-the-curve \\n\") +\n  # (2) change title\n  ggtitle(\"MAD is correlated with AUC\") +\n  # (3) change x-axis (note that certain values are not displayed then. R will spit out a warning)\n  scale_x_continuous(limits = c(-500,500))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3b\n\n\n\n\n\n\nPlot AUC values as a function of group in a density plot (geom_density).\nMake the density curves semi-transparent with the alpha argument\nAdd the mean values for both groups into the density plot as a line.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# (1 - 3)\nggplot(dolphin, aes(x = AUC, color = group, fill = group)) +\n  geom_density(alpha = 0.3) +\n  xlab(\"\\n AUC\")\n\n\n\n\n\n\n\nToggle code\n# (4) aggregate means and add to plot\n\ndolphin_agg &lt;- dolphin |&gt;\n  group_by(group) |&gt;\n  summarise(mean_AUC = mean(AUC, na.rm = TRUE),\n            sd_AUC = sd(AUC, na.rm = TRUE))\n\n# add them to the plot as vertical lines\nggplot(dolphin, aes(x = AUC, color = group, fill = group)) +\n  geom_density(alpha = 0.3) +\n  xlab(\"\\n AUC\") +\n  # since the vertical line refers to dolphin_agg, we need to specify the dataset explicitly \n  geom_vline(data = dolphin_agg, \n             aes(xintercept = mean_AUC, color = group),\n             lty = \"dashed\")"
  },
  {
    "objectID": "practice-sheets/09b-causal-inference.html",
    "href": "practice-sheets/09b-causal-inference.html",
    "title": "Causal inference",
    "section": "",
    "text": "Here is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}"
  },
  {
    "objectID": "practice-sheets/09b-causal-inference.html#case-1-gender",
    "href": "practice-sheets/09b-causal-inference.html#case-1-gender",
    "title": "Causal inference",
    "section": "Case 1: Gender",
    "text": "Case 1: Gender\nIn the first scenario (referred to as “Case 1: Gender as a confound”), the data was collected by the following procedure:\n\n700 participants were recruited, out of which 357 identified as male and 343 identified as female\neach participant decided whether or not to take the drug\nwe observed whether the participant recovered or not (binary outcome)\n\nHere is the data for the first scenario:\n\n\nToggle code\n##################################################\n# set up the data for SP\n##################################################\n\ndata_simpsons_paradox &lt;- tibble(\n  gender = c(\"Male\", \"Male\", \"Female\", \"Female\"),\n  bloodP = c(\"Low\", \"Low\", \"High\", \"High\"),\n  drug   = c(\"Take\", \"Refuse\", \"Take\", \"Refuse\"),\n  k      = c(81, 234, 192, 55),\n  N      = c(87, 270, 263, 80),\n  proportion = k/N\n)\n\ndata_simpsons_paradox |&gt; select(-bloodP)\n\n\n# A tibble: 4 × 5\n  gender drug       k     N proportion\n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 Male   Take      81    87      0.931\n2 Male   Refuse   234   270      0.867\n3 Female Take     192   263      0.730\n4 Female Refuse    55    80      0.688\n\n\nResearch Team 1 bends over this data set and notices that the drug increases recovery for both males and females, as shown in the plot below. Based on this, Research Team 1 concludes that the drug is effective and they recommend its usage.\n\n\nToggle code\ndata_simpsons_paradox |&gt; \n  ggplot(aes(x = drug, y = proportion, group = gender)) +\n  geom_line(size = 1.2, color = project_colors[12]) + \n  geom_point(size = 3, aes(color = gender))"
  },
  {
    "objectID": "practice-sheets/09b-causal-inference.html#case-2-blood-pressure",
    "href": "practice-sheets/09b-causal-inference.html#case-2-blood-pressure",
    "title": "Causal inference",
    "section": "Case 2: Blood pressure",
    "text": "Case 2: Blood pressure\nBut now consider a second scenario. Data was collected by the following process:\n\n700 participants were recruited\neach participant decided whether or not to take the drug\neach participant’s blood pressure is measured and, based on this measurement, the participant is assigned to a high and low blood pressure group (measurement happens after having taken the drug)\nwe observed whether each participant recovered or not\n\nThe data for this scenario look as follows:\n\n\nToggle code\ndata_simpsons_paradox |&gt; select(-gender)\n\n\n# A tibble: 4 × 5\n  bloodP drug       k     N proportion\n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 Low    Take      81    87      0.931\n2 Low    Refuse   234   270      0.867\n3 High   Take     192   263      0.730\n4 High   Refuse    55    80      0.688\n\n\n(Yes, you are right! The numbers are exactly the same as before!)\nResearch Team 2 bends over this data set and notices that the drug decreases recovery rate in the whole population, as shown in the plot below. Based on this, Research Team 2 concludes that the drug is not effective. They do not recommend it for usage.\n\n\nToggle code\ndata_simpsons_paradox |&gt;\n  group_by(drug) |&gt; \n  summarise(proportion = sum(k) / sum(N)) |&gt; \n  ggplot(aes(x = drug, y = proportion)) +\n  geom_line(size = 1.2, aes(group = 1), color = project_colors[12]) +\n  geom_point(size = 3, aes(color = drug))"
  },
  {
    "objectID": "practice-sheets/09b-causal-inference.html#whats-the-paradox",
    "href": "practice-sheets/09b-causal-inference.html#whats-the-paradox",
    "title": "Causal inference",
    "section": "What’s the paradox?",
    "text": "What’s the paradox?\nThe puzzle here is that two research teams have reached opposite conclusions based on data which is at least numerically the exact same. Each team seems, at first glance, to have drawn reasonable conclusions. How is it possible to reach opposite conclusions about whether or not to use a drug, based on the same set of numbers?"
  },
  {
    "objectID": "practice-sheets/09b-causal-inference.html#resolving-the-puzzle-by-causal-analysis",
    "href": "practice-sheets/09b-causal-inference.html#resolving-the-puzzle-by-causal-analysis",
    "title": "Causal inference",
    "section": "Resolving the puzzle by causal analysis",
    "text": "Resolving the puzzle by causal analysis\nYou say: “The data sets are not the same! The numbers are, but in one case we observed gender and in the other we observed blood pressure. That makes a difference, doesn’t it?”\nI say: “Well, okay, but not necessarily. Just having different labels for levels of a categorical variable doesn’t make for a different data set, does it?”\nBut you immediately shoot back: “Maybe, but you also told us about a difference in the data-generating process. There is at least a temporal difference: blood pressure is measured after the treatment, but the level of gender was fixed already before the treatment.”\n“Okay” I say. “So, do you suggest a temporal analysis?”\nYou roll your eyes and after a few more (ridiculous) turns of this conversation, we both agree that, at least conceptually speaking, there is a difference in the plausible causal structure of the involved variables.\nConsider the first case. There are three binary variables involved. Given the temporal sequence of events in the data-generating process, the likely causal relation between the variables is that gender may influence both drug (the decision to take the drug or not) and recovery. Moreover, the drug may have influenced recovery directly.\n\n\n\nCausal structure of scenario 1: gender is a confound\n\n\nNow consider the second scenario. Again, we have three binary variables. But since blood pressure is measured after the treatment, it is not plausible that it could have influenced the decision of whether to take the drug or not. Reversely, it is plausible to assume, i.e., to at least allow for the possibility, that blood pressure was affected by gender and that it may have affected recovery. As before, we also make room for the possibility that drug may affect recovery also directly.\n\n\n\nCausal structure of scenario 2: blood pressure is a mediator"
  },
  {
    "objectID": "practice-sheets/09b-causal-inference.html#case-1-gender-as-a-confound",
    "href": "practice-sheets/09b-causal-inference.html#case-1-gender-as-a-confound",
    "title": "Causal inference",
    "section": "Case 1: Gender as a confound",
    "text": "Case 1: Gender as a confound\nSince the set \\(\\{G\\}\\) statisfies the backdoor criterion for the assumed causal DAG, we know that we can calculate the total causal effect (TCE) by eliminating the do-operator in the conditioning using the adjustment formula:\n\\[\nP(R=r \\mid \\mathit{do}(D=d)) = \\sum_{g \\in \\{0,1\\}} P(R=r \\mid D=d, G=g) \\ P(G=g)\n\\]\nThis means that we need to estimate two probability distributions: the conditional probability of recovery given drug and gender and the (marginal) probability of gender. We can use maximum-likelihood estimates for these by just using the observed frequencies as estimators. For \\(\\mathit{do}(D=1)\\), this yields:\n\\[\n\\begin{align*}\n& P(R=1 \\mid \\mathit{do}(D=1))\n\\\\\n= &   P(R=1 \\mid D=1, G=f) \\ P(G=f) + P(R=1 \\mid D=1, G=m) \\ P(G=m)\n\\\\\n= &   \\frac{192}{263} \\ \\frac{343}{700}\n    + \\frac{81}{87}  \\ \\frac{357}{700}\n\\\\\n= & 0.8325462\n\\end{align*}\n\\]\nFor \\(\\mathit{do}(D=0)\\), we get:\n\\[\n\\begin{align*}\n& P(R=1 \\mid \\mathit{do}(D=0))\n\\\\\n= &   P(R=1 \\mid D=0, G=f) \\ P(G=f) + P(R=1 \\mid D=0, G=m) \\ P(G=m)\n\\\\\n= &   \\frac{55}{80} \\ \\frac{343}{700}\n    + \\frac{234}{270}  \\ \\frac{357}{700}\n\\\\\n= & 0.778875\n\\end{align*}\n\\] So, an ML-estimate of the TCE would be:\n\n\nToggle code\nMLE_case1 &lt;- 192/263 * 343/700 + 81/87 * 357/700 -\n  (55/80 * 343/700 + 234/270 * 357/700)\nMLE_case1\n\n\n[1] 0.05367122\n\n\nThis suggest that the drug is effectively increasing expected recovery, but we do not have a measure of uncertainty of this estimate. We don’t know if we should consider this convincing evidence to recommend wide-spread adoption. That’s why we need (something like) Bayesian estimation eventually. But let’s first look at the second scenario."
  },
  {
    "objectID": "practice-sheets/09b-causal-inference.html#case-2-blood-pressure-as-a-mediator",
    "href": "practice-sheets/09b-causal-inference.html#case-2-blood-pressure-as-a-mediator",
    "title": "Causal inference",
    "section": "Case 2: Blood pressure as a mediator",
    "text": "Case 2: Blood pressure as a mediator\nFor the causal graph assumed for the second scenario, the do-intervention reduces to the conditional probability:\n\\[\nP\\left(R=1 \\mid \\mathit{do}(D=d)\\right)\n= P\\left(R = 1 \\mid D=d \\right)\n\\]\nFor \\(\\mathit{do}(D=1)\\), this yields:\n\\[\nP(R=1 \\mid \\mathit{do}(D=1)) = P(R=1 \\mid D=1) = \\frac{273}{350} = 0.78\n\\]\nFor \\(\\mathit{do}(D=0)\\), we get:\n\\[\nP(R=1 \\mid \\mathit{do}(D=0)) = P(R=1 \\mid D=0) = \\frac{289}{350} = 0.8257143\n\\]\nThe ML-esimtate of the TCE is therefore:\n\n\nToggle code\nMLE_case2 &lt;- 273/350 - 289/350\nMLE_case2\n\n\n[1] -0.04571429\n\n\nThis differs in sign from the previous estimate, and we might conclude that administering the drug is, overall, not beneficial. Yet, again, we have no uncertainty quantification regarding this estimate."
  },
  {
    "objectID": "practice-sheets/09b-causal-inference.html#some-data-wrangling",
    "href": "practice-sheets/09b-causal-inference.html#some-data-wrangling",
    "title": "Causal inference",
    "section": "Some data wrangling",
    "text": "Some data wrangling\nFor subsequent analysis (especially when generating predictive samples), it helps to have the data in long format. The uncount() function is a great tool for this.\n\n\nToggle code\n# cast into long format\ndata_SP_long &lt;- rbind(\n  data_simpsons_paradox |&gt; uncount(k) |&gt; \n    mutate(recover = TRUE)  |&gt; select(-N, -proportion),\n  data_simpsons_paradox |&gt; uncount(N-k) |&gt; \n    mutate(recover = FALSE) |&gt; select(-N, -proportion, -k)\n)\ndata_SP_long\n\n\n# A tibble: 700 × 4\n   gender bloodP drug  recover\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;lgl&gt;  \n 1 Male   Low    Take  TRUE   \n 2 Male   Low    Take  TRUE   \n 3 Male   Low    Take  TRUE   \n 4 Male   Low    Take  TRUE   \n 5 Male   Low    Take  TRUE   \n 6 Male   Low    Take  TRUE   \n 7 Male   Low    Take  TRUE   \n 8 Male   Low    Take  TRUE   \n 9 Male   Low    Take  TRUE   \n10 Male   Low    Take  TRUE   \n# ℹ 690 more rows"
  },
  {
    "objectID": "practice-sheets/09b-causal-inference.html#case-1-gender-as-a-confound-1",
    "href": "practice-sheets/09b-causal-inference.html#case-1-gender-as-a-confound-1",
    "title": "Causal inference",
    "section": "Case 1: Gender as a confound",
    "text": "Case 1: Gender as a confound\nGiven the causal structure assumed for scenario 1, we can calculate the effects of the relevant do-intervention as:\n\\[\nP(R=1 \\mid \\mathit{do}(D=d)) = \\sum_{g \\in \\{0,1\\}} P(R=1 \\mid D=d, G=g) \\ P(G=g)\n\\]\nTherefore, we need to do three things:\n\nWe estimate \\(P(R=1 \\mid D=d, G=g)\\). This can be done with a logistic regression model, regressing \\(R\\) on \\(D\\) and \\(G\\).\nWe estimate \\(P(G)\\), which we can do with an intercept-only logistic regression model.\nWe calculate the TCE based on samples from the posterior predictive distributions of these models.\n\n\nStep 1: Intercept-only model for gender\nHere is an intercept-only logistic regression model for gender:\n\n\nToggle code\nniter = 2000\n\nfit_SP_GonIntercept &lt;- brm(\n  formula = gender ~ 1,\n  data    = data_SP_long,\n  family  = bernoulli(link = \"logit\"),\n  iter    = niter\n)\n\n\nEach sample from the posterior of the Intercept parameter represents (a guess of) the log-odds of the Male category. The posterior over the proportion of male participants can therefore be retrieved and plotted as follows (the yellow line shows the observed frequency):\n\n\nToggle code\nlogistic &lt;- function(x) {\n  1 / (1 + exp(-x))\n}\n\nposterior_SP_GonIntercept &lt;- tidybayes::tidy_draws(fit_SP_GonIntercept) |&gt; \n  mutate(prop_male = logistic(b_Intercept)) |&gt; \n  select(prop_male)\n\nposterior_SP_GonIntercept |&gt; \n  ggplot(aes(x = prop_male)) + \n  tidybayes::stat_halfeye() +\n  geom_vline(aes(xintercept = 357/700), color = project_colors[3]) +\n  xlab(\"proportion males\") + \n  ylab(\"posterior density\")\n\n\n\n\n\n\n\nStep 2: Regressing \\(R\\) against \\(G\\) and \\(D\\)\nNext, we regress recover on drug and gender.\n\n\nToggle code\nfit_SP_RonGD &lt;- brm(\n  formula = recover ~ gender * drug,\n  data    = data_SP_long,\n  family  = bernoulli(link = \"logit\"),\n  iter    = niter\n)\n\n\n\n\nStep 3: Compute the posterior expectations and the TCE\nIn a third step, we draw posterior predictive samples for the model from step 2, based on posterior predictive samples for the model from step 1, while manually setting drug to Take and Refuse.\nFirst, we get posterior predictive samples of gender from the model from step 1. Notice that these are just samples of Male and Female, for which we will generate predictions based on the the second model.\n\n\nToggle code\npostPred_gender &lt;- tidybayes::predicted_draws(\n  object  = fit_SP_GonIntercept,\n  newdata = tibble(Intercept = 1),\n  value   = \"gender\",\n  ndraws  = niter * 2\n  ) |&gt; \n  ungroup() |&gt; \n  mutate(gender = ifelse(gender, \"Male\", \"Female\")) |&gt; \n  select(gender)\n\n# NB: in this case we could also have gotten this via: \n# rbinom(n=4000, p=rbeta(4000, 315+1, 700-315+1), size = 700)\n\npostPred_gender\n\n\n# A tibble: 4,000 × 1\n   gender\n   &lt;chr&gt; \n 1 Female\n 2 Female\n 3 Female\n 4 Female\n 5 Female\n 6 Female\n 7 Male  \n 8 Female\n 9 Female\n10 Female\n# ℹ 3,990 more rows\n\n\nBased on these ‘sampled individuals’ we generate the prediction of the second model (predicting the a posteriori expected recovery, given gender and whether to take the drug or not).\n\n\nToggle code\n# posterior predictive samples for D=1\nposterior_DrugTaken &lt;- tidybayes::epred_draws(\n  object  = fit_SP_RonGD,\n  newdata = postPred_gender |&gt; mutate(drug = \"Take\"),\n  value   = \"taken\",\n  ndraws  = niter * 2\n) |&gt; ungroup() |&gt; \n  select(taken)\n\n# posterior predictive samples for D=0\nposterior_DrugRefused &lt;- tidybayes::epred_draws(\n  object  = fit_SP_RonGD,\n  newdata = postPred_gender |&gt; mutate(drug = \"Refuse\"),\n  value   = \"refused\",\n  ndraws  = niter * 2\n) |&gt; ungroup() |&gt; \n  select(refused)\n\n\nTo calculate the TCE we look at the difference in predicted recovery rate:\n\n\nToggle code\nCE_post &lt;- cbind(posterior_DrugTaken, posterior_DrugRefused) |&gt; \n  mutate(causal_effect = taken - refused) \n\nrbind(\n  aida::summarize_sample_vector(CE_post$taken, \"drug_taken\"),\n  aida::summarize_sample_vector(CE_post$refused, \"drug_refused\"),\n  aida::summarize_sample_vector(CE_post$causal_effect, \"causal_effect\")\n)\n\n\n# A tibble: 3 × 4\n  Parameter      `|95%`   mean `95%|`\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 drug_taken     0.691  0.832   0.972\n2 drug_refused   0.616  0.778   0.905\n3 causal_effect -0.0456 0.0537  0.143\n\n\nThis yields a point estimate (Bayesian mean posterior) and the usual uncertainty quantification in terms of credible intervals etc. In this case, we would not be compelled to conclude that the causal effect is substantial as the posterior estimate for this effect clearly encompasses non-negligible mass for the range of negative values. The orange line show the maximum-likelihood estimate of the causal effect.\n\n\nToggle code\nCE_post |&gt; \n  ggplot(aes(x = causal_effect)) +\n  tidybayes::stat_halfeye() +\n  geom_vline(aes(xintercept = MLE_case1), color = project_colors[3])\n\n\n\n\n\n\n\n\n\n\n\nExercise 1: Alternative calculation of causal effect estimate\n\n\n\n\n\nThe last plot looks a bit ragged. This is because we approximate \\(P(G)\\) by actual samples of levels Male and Female. There is an alternative, though. Both approaches are correct. But they differ slightly in logic and execution, so you may benefit from having seen both.\nIn this new approach you do this:\n\nApproximate \\(P(G)\\) by taking samples from the expected value of \\(P(G=M)\\). Use the function tidybayes::epred_draws() for this.\nSample expected values of \\(P(R=1 \\mid G, D)\\) for all combinations of levels of \\(G\\) and \\(D\\).\nWeigh the predicted recovery rates from step 2 with the corresponding predictions of \\(P(G)\\) from step 1.\nCompute the causal effect from this.\n\nCompute the ususal summary statistics (posterior mean, credible interval) and plot the posterior for the estimated causal effect.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# get epred samples for intercept-only model:\npostPred_maleProportion &lt;- tidybayes::epred_draws(\n  fit_SP_GonIntercept, \n  newdata = tibble(Intercept = 1),\n  value = \"maleProp\",\n  ndraws  = niter * 2\n  ) |&gt; ungroup() |&gt; \n  select(.draw, maleProp)\n\n#  get epred samples for the R ~ D, G model\n#   for all combinations of D and G\nposterior_DrugTaken &lt;- tidybayes::epred_draws(\n  object  = fit_SP_RonGD,\n  newdata = tibble(gender = c(\"Male\", \"Male\", \"Female\", \"Female\"),\n                   drug   = c(\"Take\", \"Refuse\", \"Take\", \"Refuse\")),\n  value   = \"recovery\",\n  ndraws  = niter * 2\n) |&gt; ungroup() |&gt; \n  select(.draw, gender, drug, recovery)\n\n# weigh and compute the causal effect\nCE_post &lt;- posterior_DrugTaken |&gt; full_join( postPred_maleProportion) |&gt; \n  mutate(weights = ifelse(gender == \"Male\", maleProp, 1-maleProp)) |&gt; \n  group_by(`.draw`, drug) |&gt; \n  summarize(predRecover = sum(recovery * weights)) |&gt; \n  pivot_wider(names_from = drug, values_from = predRecover) |&gt; \n  mutate(causal_effect = Take - Refuse) \n\n# produce summary statistics\nrbind(\n  aida::summarize_sample_vector(CE_post$Take, \"drug_taken\"),\n  aida::summarize_sample_vector(CE_post$Refuse, \"drug_refused\"),\n  aida::summarize_sample_vector(CE_post$causal_effect, \"causal_effect\")\n)\n\n\n# A tibble: 3 × 4\n  Parameter       `|95%`   mean `95%|`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 drug_taken     0.793   0.832   0.870\n2 drug_refused   0.723   0.778   0.828\n3 causal_effect -0.00972 0.0537  0.120\n\n\nToggle code\n# plot the relevant posterior\nCE_post |&gt; \n  ggplot(aes(x = causal_effect)) +\n  tidybayes::stat_halfeye()"
  },
  {
    "objectID": "practice-sheets/09b-causal-inference.html#case-2-blood-pressure-as-a-mediator-1",
    "href": "practice-sheets/09b-causal-inference.html#case-2-blood-pressure-as-a-mediator-1",
    "title": "Causal inference",
    "section": "Case 2: Blood pressure as a mediator",
    "text": "Case 2: Blood pressure as a mediator\nFor the second case, blood pressure as a mediator, the calculations are much easier. We just need to estimate \\(P(R \\mid D, B)\\), so a single regression model will do.\n\n\nToggle code\nfit_SP_RonBD &lt;- brms::brm(\n  formula = recover ~ drug,\n  data    = data_SP_long,\n  family  = bernoulli(link = \"logit\"),\n  iter    = niter\n)\n\n\nThe coefficients of a logistic regression model relate to log-odds. Using the faintr package and the logistic transformation, we can calculate samples of the causal effect as follows:\n\n\nToggle code\nposterior_DrugTaken &lt;- \n  faintr::extract_cell_draws(fit_SP_RonBD) |&gt; \n  pull(Take) |&gt; \n  logistic()\n\nposterior_DrugRefused &lt;- \n  faintr::extract_cell_draws(fit_SP_RonBD)|&gt; \n  pull(Refuse) |&gt; \n  logistic()\n\nposterior_causalEffect &lt;- \n  posterior_DrugTaken - posterior_DrugRefused\n\nrbind(\n  aida::summarize_sample_vector(posterior_DrugTaken, \"drug_taken\"),\n  aida::summarize_sample_vector(posterior_DrugRefused, \"drug_refused\"),\n  aida::summarize_sample_vector(posterior_causalEffect, \"causal_effect\")\n)\n\n\n# A tibble: 3 × 4\n  Parameter     `|95%`    mean `95%|`\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 drug_taken     0.739  0.780  0.825 \n2 drug_refused   0.784  0.825  0.864 \n3 causal_effect -0.104 -0.0454 0.0101\n\n\nHere is a density plot of the posterior samples. The orange line show the maximum-likelihood estimate of the causal effect.\n\n\nToggle code\ntibble(CE = posterior_causalEffect) |&gt; \n  ggplot(aes(x = CE)) +\n  tidybayes::stat_halfeye() +\n  geom_vline(aes(xintercept = MLE_case2), color = project_colors[3])\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Using poterior predictives to estimate causal effects\n\n\n\n\n\nThe last section computed estimates of the relevant causal effect directly from the samples for model coefficients. This works well for logistic regression, but in other cases it may be more convenient to use samples from the posterior predictive distribution of the R ~ D, G model, similar to what we did in the first scenario.\nSo, use tidybayes::epred_draws() to get estimates of the causal effect.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# get posterior predictive samples & compute causal effect\nCE_post2 &lt;- tidybayes::epred_draws(\n  fit_SP_RonBD,\n  newdata = tibble(drug   = c(\"Take\", \"Refuse\")),\n  ndraws  = niter * 2\n) |&gt; \n  pivot_wider(id_cols = `.draw`, names_from = drug, values_from = `.epred`) |&gt; \n  mutate(causal_effect = Take - Refuse)\n\n# produce summary statistics\nrbind(\n  aida::summarize_sample_vector(CE_post2$Take, \"drug_taken\"),\n  aida::summarize_sample_vector(CE_post2$Refuse, \"drug_refused\"),\n  aida::summarize_sample_vector(CE_post2$causal_effect, \"causal_effect\")\n)\n\n\n# A tibble: 3 × 4\n  Parameter     `|95%`    mean `95%|`\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 drug_taken     0.739  0.780  0.825 \n2 drug_refused   0.784  0.825  0.864 \n3 causal_effect -0.104 -0.0454 0.0101\n\n\nToggle code\n# plot posterior\nCE_post2 |&gt; \n  ggplot(aes(x = causal_effect)) +\n  tidybayes::stat_halfeye() +\n  geom_vline(aes(xintercept = MLE_case2), color = project_colors[3])"
  },
  {
    "objectID": "practice-sheets/10b-GAMs.html",
    "href": "practice-sheets/10b-GAMs.html",
    "title": "Generalized additive models in brms",
    "section": "",
    "text": "This tutorial provides both a conceptual and a practical introduction to fitting generalized additive models (GAMs) in brms. GAMs approximate wiggly curves by “smoothed splines”. The central idea to internalize here is that we can think of smoothed splines as a random effect. This is, indeed, how brms deals with GAMs.\nThe material here leans heavily on T.J. Mahr’s excellent tutorial.\n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nData set and mission statement\nLet’s start with a common example for introductions to GAMs, the non-linear relationship between time (after impact) and acceleration (of the head of a motorcycle driver). The data is from the MASS package and it looks like this:\n\n\nToggle code\n# motorcycle data\ndata_motor &lt;- MASS::mcycle |&gt; \n  tibble() |&gt; \n  unique()\n\n# plot the data\ndata_motor |&gt; \n  ggplot(aes(x = times, y = accel)) +\n  geom_smooth(color = project_colors[1]) +\n  geom_point()\n\n\n\n\n\nNotice that the plotting function geom_smooth already provides us with a smoothing line to fit the central tendency of this this “wiggly data”. But this one is not very good. We can probably do better.\nThe goals for the remainder of this tutorial are:\n\nTo understand how a wiggly curve can be constructed from a set of “atomic wiggles”.\nTo see the parallel between (i) fitting weights over “atomic wiggles”, with a penalty against excessive wiggliness, and fitting (ii) group-level effects.\nTo be able to run a Bayesian regression model with penalized smoothing splines using brms.\n\n\n\nWiggly curves as mixtures over “atomic wiggles”\nLet’s play a game! It’s called “make a wiggle”. Here’s how it’s played: I give you a bunch of “atomic wiggles”, you adjust some weights, and out pops … TADA! … a smoothing spline.\nOr, for those dull at heart, in technical terms, I give you a set of basis functions. This set is also called basis. Then you build intuitions about how, by adjusting numerical weights for the basis functions, we can flexibly approximate non-linear data.\nThe following code gives you a function get_basis() which takes as input a sequence of observations \\(x\\) for which we would like to construct a non-linear line. The output is a set of “atomic wiggles” (see plot below). The function also takes an integer value k as input, which is the number of “atomic wiggles” that you get.\nDo not worry about the details of this function, better to directly look at its output. But if you must know: this function uses the gam function from the mgcv package to construct (indirectly, through the model matrix internally built up by mgcv::gam) k cubic regression splines. The brms package uses the function mgcv:s which is called here in the model formula as well (more on this below).\n\n\nToggle code\n# create a set of /k/ splines for vector /x/\nget_basis &lt;- function(x, k = 20){\n  # fit a dummy GAM\n  fit_gam &lt;- mgcv::gam(\n    formula = y ~ s(times, bs = \"cr\", k = 20),\n    data    = tibble(y = 1, times = x))\n  # extract model matrix\n  model_matrix &lt;- model.matrix(fit_gam)\n  # wrangle to long tibble\n  some_curves &lt;- model_matrix |&gt; \n    as_tibble() |&gt; \n    mutate(x = x) |&gt; \n    pivot_longer(cols = -x) |&gt; \n    mutate(\n      name = str_replace(name, \"s\\\\(times\\\\).\", \"curve_\"))\n  return(some_curves)\n}\n\n\nLet’s construct a basis then and plot it:\n\n\nToggle code\nn_x = 1000\nx = seq(0,60, length.out = n_x)\n\nsome_curves &lt;- get_basis(x)\n\n# plot the basis\nsome_curves |&gt; \n  ggplot(aes(x = x, y = value, color = name, group = name)) +\n  geom_line(size=1.5) + \n  scale_colour_manual(values = c(project_colors, project_colors)) +\n  theme(legend.position=\"none\") +\n  ylab(\"\") + ggtitle(\"your basis\") +\n  xlab(\"\")\n\n\n\n\n\nTo play “make a wiggle” you also need a vector of weights, one for each basis function. Here is a function which, given a weight vector, computes the weighted average over all basis functions for a smooth prediction.\n\n\nToggle code\nmake_a_wiggle &lt;- function(some_curves, weights, data = NULL) {\n  \n  your_wiggly_line &lt;- some_curves |&gt; \n    mutate(\n      weight = rep(weights, n_x),\n      weighted_value = value * weight\n      ) |&gt; \n    group_by(x) |&gt; \n    summarize(wiggly_line = sum(weighted_value)) |&gt; \n    ungroup()\n  \n  your_wiggle_plot &lt;- your_wiggly_line |&gt; \n    ggplot(aes(x = x, y = wiggly_line)) + \n    geom_line(color = project_colors[1], size=1.5) + \n    ggtitle(\"your wiggly line\") +\n    xlab(\"x\") + ylab(\"\")\n    \n  your_wiggle_plot\n}\n\n\nAnd here are some examples:\n\n\nToggle code\nweights &lt;- c(0,2,-1,1,1,2,1,5,5,-7,1,1,1,10,1,1,1,1,1,1)\nmake_a_wiggle(some_curves, weights)\n\n\n\n\n\n\n\nToggle code\nweights &lt;- c(-1,0.5,-2,-1,0,3,4,3,2,-1,-5,1,1,1,1,2,2,2,2,2)\nmake_a_wiggle(some_curves, weights)\n\n\n\n\n\nAnd a not so wiggly wiggle:\n\n\nToggle code\nweights &lt;- c(0,1,10,rep(0, times = 17))\nmake_a_wiggle(some_curves, weights)\n\n\n\n\n\n\n\n\n\n\n\nExercise 1: Try a manual fit\n\n\n\n\n\nPlay around with weights and try to find a constellation that approximates the shape of the motorcycle data in data_motor that we plotted above.\n\n\n\n\n\nPenalizing wiggliness and group-level effects\nWe saw how a vector of weights allows us to approximate non-linear relationship, once we have a set of elementary basis functions. To fit a non-linear regression model, the data should inform us about which vector of weights to use. So, essentially, we can just use the basis functions as linear predictors. There’s a lot of them, so that’s going to be ugly, but let’s do it:\n\n\nToggle code\nmotor_basis &lt;- get_basis(data_motor$times, nrow(data_motor)) |&gt; \n  mutate (accel = rep(data_motor$accel, each = 20)) |&gt; \n  rename(times = x) |&gt; \n  dplyr::filter(name != \"(Intercept)\") |&gt; \n  pivot_wider(id_cols = c(\"times\", \"accel\"), names_from = name, values_from = value, values_fn = mean)\n\nfit_motor_FE &lt;- brms::brm(\n  formula = accel ~ times + curve_1 + curve_2 + curve_3 + curve_4 + curve_5 + curve_6 + curve_7 + curve_8 + curve_9 + curve_10 +\n    curve_11 + curve_12 + curve_13 + curve_14 + curve_15 + curve_16 + curve_17 + curve_18 + curve_19,\n  data    = motor_basis,\n  prior   = prior(normal(0,50))\n)\n\n\nThis really only works if we specify some constraints via the priors for the coefficients! (Try a fit with improper priors!)\nHere is a plot of the posterior linear predictor:\n\n\nToggle code\nplot_post_linPred &lt;- function(fit, data) {\n  postPred &lt;- data |&gt; \n    tidybayes::add_epred_draws(fit) |&gt; \n    group_by(times,accel) |&gt; \n    summarize(\n      lower  = tidybayes::hdi(.epred)[1],\n      mean   = mean(.epred),\n      higher = tidybayes::hdi(.epred)[2]\n    )\n  \n  postPred |&gt; \n    ggplot(aes(x = times,  y = accel)) +\n    geom_line(aes(y = mean), color = project_colors[1], size = 2) +\n    geom_ribbon(aes(ymin = lower, ymax = higher), \n                fill = project_colors[6], alpha = 0.5) +\n    geom_point(size = 2)\n}\n\nplot_post_linPred(fit_motor_FE, motor_basis)\n\n\n\n\n\nThat looks like it’s on the right track, but it’s not wiggly enough.\n\n\n\n\n\n\nExercise 2: Increasing smoothness of the predictor\n\n\n\n\n\nWhat could we do to make the linear predictor more curvy?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe bigger the fixed effect coefficients, the more the basis functions can affect the linear predictor. Currently, there are rather strong priors on the coefficients (A Gaussian with standard deviation 10). If we make these priors wider, we should see a much more curvy fit.\n\n\nToggle code\nfit_motor_FE_wider &lt;- brms::brm(\n  formula = accel ~ times + curve_1 + curve_2 + curve_3 + curve_4 + curve_5 + curve_6 + curve_7 + curve_8 + curve_9 + curve_10 +\n    curve_11 + curve_12 + curve_13 + curve_14 + curve_15 + curve_16 + curve_17 + curve_18 + curve_19,\n  data    = motor_basis,\n  prior   = prior(normal(0,500))\n)\n\nplot_post_linPred(fit_motor_FE_wider, motor_basis)\n\n\n\n\n\n\n\n\n\n\n\nTaking stock so far, all of this shows that:\n\nBy adjusting weights of basis functions, we can get smoothed curves.\nThese weights can be learned from the data (visual results look good).\nBUT: the degree of smoothness is determined by the prior on coefficients.\n\nThe latter problem is worrisome because we do not want to manually adjust an important parameter like that. Ideally, the smoothness of the curve should be dictated by the data itself. Otherwise, we might overfit or remain too linear, so to speak.\nNotice that the parameter to tweak is the standard deviation of the normal distribution of the prior over coefficients. The smaller this is, the more linear a curve we predict. The bigger the more smoothed the fit. So, we want a model that let’s the data decide what the standard deviation is supposed to be for additive offsets to what is otherwise a vanilla regression model. But, hey, that’s exactly what group-level effects do, too!\nConcretely, we want a predictor \\(\\eta\\) which consists of the usual linear regression part \\(X \\beta\\) with an additional part \\(Z b\\), which specifies how much to deviate from the vanilla linear prediction. In the case at hand, the matrix \\(Z\\) is, essentially, the basis (rows are for each observation of times, columns correspond to the basis functions). We also want the coefficient’s \\(b\\) to be close to zero but with unknown variance, which is to be inferred from the data. The overal structure of this model is therefore:\n\\[\n\\begin{align*}\n\\eta &= X \\beta + Z b \\\\\nb & \\sim \\mathcal{N}(o, \\sigma_b) \\\\\n\\sigma_b & \\sim \\dots \\text{some prior} \\dots\n\\end{align*}\n\\] And that is exactly the structure of a group-level / random-effects model. Random effects are supposed to be small, where what counts as small enough is to be determined by the data. Likewise, weights of basis functions are suppose to be small and, likewise, what counts as small is not to be fixed by hand but governed by the needs of the data. (Here we are again: we should think of group-level effects as putting up an (elastic) fence and letting the sheep graze where they want; a gentle anything-goes within a flexible margin.)\nIndeed, brms implements GAMs as, essentially, multi-level models, even though the touch-and-feel maybe different.\n\n\nImplementing GAMs in brms\nTo run a basic GAM in brms, just wrap the predictor to be smoothed in s(), which is brms-wrapper around the mgcv function of the same name.\n\n\nToggle code\nfit_motor &lt;- \n  brms::brm(\n    formula = accel ~ s(times, bs = \"cr\", k = 20),\n    data    = data_motor\n  )\n\n\nHere is the posterior linear predictor for this model, which looks fine:\n\n\nToggle code\nplot_post_linPred(fit_motor, data_motor)\n\n\n\n\n\nIt is interesting to interpret the summary:\n\n\nToggle code\n# interpret this\nsummary(fit_motor)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: accel ~ s(times, bs = \"cr\", k = 20) \n   Data: data_motor (Number of observations: 132) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmooth Terms: \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(stimes_1)     4.95      1.25     3.12     7.93 1.01      810     1187\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   -25.67      2.00   -29.73   -21.73 1.00     4500     2689\nstimes_1      1.66      0.33     1.02     2.31 1.00     4324     2713\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    22.84      1.48    20.13    25.89 1.00     4728     2960\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe “Smooth Term” sds(stimes_1) is essentially the standard deviation \\(\\sigma\\) in the equations above. (The “sigma” is the usual standard deviation of the Gaussian likelihood function.) The intercept is the usual intercept, and the term “stimes_1” is the normal linear regressions slope. But these are so distorted by the penalized smooths in which they are wrapped that is is not prudent to draw strong conclusions from them (in my understanding; though I am not certain for it).\n\n\nExercises\n\n\n\n\n\n\nExercise 3: Posterior predictives\n\n\n\n\n\nThe previous plot showed ribbons for the 95% HDI for the linear predictor. Make a similar plot for the posterior predictive distribution. Before you code, think about how you expect the plot to differ from the previous one (different curve, more / less uncertainty …).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe expect the central tendency to be similar (albeit a bit more noisy, given sampling inaccuracy), but most of all the ribbons will be broader, given that we quantify uncertaint about where the data points would fall, not about where the central tendency is.\n\n\nToggle code\npostPred &lt;- data_motor |&gt; \n  tidybayes::add_predicted_draws(fit_motor) |&gt; \n  group_by(times,accel) |&gt; \n  summarize(\n    lower  = tidybayes::hdi(.prediction)[1],\n    mean   = mean(.prediction),\n    higher = tidybayes::hdi(.prediction)[2]\n  )\n\npostPred |&gt; \n  ggplot(aes(x = times,  y = accel)) +\n  geom_line(aes(y = mean), color = project_colors[1], size = 2) +\n  geom_ribbon(aes(ymin = lower, ymax = higher), \n              fill = project_colors[7], alpha = 0.3) +\n  geom_point(size = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 4: Posterior predictive check\n\n\n\n\n\nConsult visual posterior predictive checks for the GAM. Does it look alright? Anything systematicaly amiss?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA simple PPC suggests that the overall distribution of the data is not matched. This could be due to the large uncertainty for early time points, which is not borne out by the data. This explains why the PPCs are flatter, less pronounced.\n\n\nToggle code\npp_check(fit_motor, ndraws = 40)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5: A distributional GAM\n\n\n\n\n\nGiven the problem above, it makes sense to try a distributional model, regressing the “sigma” parameter of the likelihood function on times. Decide whether it’s better to plot the posterior for the linear predictor or the posterior predictives. Interpret what you see.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere is the model fit:\n\n\nToggle code\nfit_motor_distributional &lt;- \n  brms::brm(\n    formula = brms::bf(accel ~ s(times, bs = \"cr\", k = 20),\n                       sigma ~ times),\n    data    = data_motor\n  )\n\n\nWe should consult the posterior predictive because that is where the difference in spread around the linear predictor will show, not in the estimates of the linear predictor.\n\n\nToggle code\npostPred &lt;- data_motor |&gt; \n  tidybayes::add_predicted_draws(fit_motor_distributional) |&gt; \n  group_by(times,accel) |&gt; \n  summarize(\n    lower  = tidybayes::hdi(.prediction)[1],\n    mean   = mean(.prediction),\n    higher = tidybayes::hdi(.prediction)[2]\n  )\n\npostPred |&gt; \n  ggplot(aes(x = times,  y = accel)) +\n  geom_line(aes(y = mean), color = project_colors[1], size = 2) +\n  geom_ribbon(aes(ymin = lower, ymax = higher), \n              fill = project_colors[7], alpha = 0.4) +\n  geom_point(size = 2)\n\n\n\n\n\nWe see a tighter credible interval for early time points, but the change to the non-distributional model is not very large.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6: A GAM for World Temperature\n\n\n\n\n\nRun a Bayesian GAM for the World Temperature data (aida::data_WorldTemp). Plot the expected posterior predictor of central tendency together with the data. Can you use information from the posterior over model parameters to address the question of whether there is a trend towards higher measurements as time progresses? Compare the conclusions you draw from this linear model to those you would draw from a naive linear model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst run the models.\n\n\nToggle code\nfit_temp_splines &lt;- \n  brms::brm(\n    formula = avg_temp ~ s(year, bs = \"cr\", k=20),\n    data    = aida::data_WorldTemp\n  )\n\nfit_temp_vanilla &lt;- \n  brms::brm(\n    formula = avg_temp ~ year,\n    data    = aida::data_WorldTemp\n  )\n\n\nHere is the posterior predicted central tendency for the GAM (w/ credible intervals):\n\n\nToggle code\npostPred &lt;- aida::data_WorldTemp |&gt; \n  tidybayes::add_predicted_draws(fit_temp_splines) |&gt; \n  group_by(avg_temp, year) |&gt; \n  summarize(\n    lower  = tidybayes::hdi(.prediction)[1],\n    mean   = mean(.prediction),\n    higher = tidybayes::hdi(.prediction)[2]\n  )\n\npostPred |&gt; \n  ggplot(aes(x = year,  y = avg_temp)) +\n  geom_line(aes(y = mean), color = project_colors[1], size = 2) +\n  geom_ribbon(aes(ymin = lower, ymax = higher), \n              fill = project_colors[6], alpha = 0.4) +\n  geom_point(size = 2)\n\n\n\n\n\nWe can compare the summaries of both models:\n\n\nToggle code\ntidybayes::summarise_draws(fit_temp_splines)[1:4,]\n\n\n# A tibble: 4 × 10\n  variable     mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;       &lt;num&gt;  &lt;num&gt;   &lt;num&gt;   &lt;num&gt;  &lt;num&gt;  &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 b_Interce… 8.31   8.31   0.0200  0.0195  8.28   8.35    1.00    5089.    3026.\n2 bs_syear_1 0.122  0.122  0.00499 0.00500 0.114  0.130   1.00    4876.    3011.\n3 sds_syear… 0.0571 0.0542 0.0192  0.0177  0.0318 0.0930  1.00     665.    1211.\n4 sigma      0.326  0.325  0.0144  0.0145  0.303  0.350   1.00    4001.    3232.\n\n\nToggle code\ntidybayes::summarise_draws(fit_temp_vanilla)[1:4,]\n\n\n# A tibble: 4 × 10\n  variable        mean   median      sd     mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;          &lt;num&gt;    &lt;num&gt;   &lt;num&gt;   &lt;num&gt;    &lt;num&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 b_Intercept -3.52    -3.52    6.23e-1 6.25e-1 -4.53    -2.48     1.00    4763.\n2 b_year       0.00628  0.00628 3.31e-4 3.33e-4  0.00573  0.00681  1.00    4762.\n3 sigma        0.406    0.405   1.79e-2 1.80e-2  0.378    0.437    1.00    1346.\n4 lprior      -3.16    -3.16    1.55e-3 1.55e-3 -3.16    -3.16     1.00    1349.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\n\nThe population-level slope parameter represents the unsmoothed linear predictor line for the splines model, but it is difficult to interpret (because we have to strip off the smoothing splines, so to speak). There seems to be an indication of a positive linear effect underneath the smooth, but I, personally, would not base firm conclusions on this.\n\n\n\n\n\n\nreplace talk of “linear predictor” with central tendency"
  },
  {
    "objectID": "practice-sheets/03c-multi-membership.html",
    "href": "practice-sheets/03c-multi-membership.html",
    "title": "Multi-membership models",
    "section": "",
    "text": "Multi-membership models are useful in cases where group-level effects are plausible but elements do not necessarily belong just to a single group. For example, if we want to include group-level effects for the native language (of participants in an experiment), some individuals may have more than one. Individuals with more than one native language would belong to multiple groups, whence the term multi-membership models. Moreover, there are cases where membership in a group is a matter of degree: if a grouping of individuals is by “country of residence”, for example, some people might spend variable amounts of time in different countries, and our model may want to account for that.\n\nPreamble\nHere is code to load (and if necessary, install) required packages, and to set some global options (for plotting and efficient fitting of Bayesian models).\n\n\nToggle code\n# install packages from CRAN (unless installed)\npckgs_needed &lt;- c(\n  \"tidyverse\",\n  \"brms\",\n  \"rstan\",\n  \"rstanarm\",\n  \"remotes\",\n  \"tidybayes\",\n  \"bridgesampling\",\n  \"shinystan\",\n  \"mgcv\"\n)\npckgs_installed &lt;- installed.packages()[,\"Package\"]\npckgs_2_install &lt;- pckgs_needed[!(pckgs_needed %in% pckgs_installed)]\nif(length(pckgs_2_install)) {\n  install.packages(pckgs_2_install)\n} \n\n# install additional packages from GitHub (unless installed)\nif (! \"aida\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/aida-package\")\n}\nif (! \"faintr\" %in% pckgs_installed) {\n  remotes::install_github(\"michael-franke/faintr\")\n}\nif (! \"cspplot\" %in% pckgs_installed) {\n  remotes::install_github(\"CogSciPrag/cspplot\")\n}\n\n# load the required packages\nx &lt;- lapply(pckgs_needed, library, character.only = TRUE)\nlibrary(aida)\nlibrary(faintr)\nlibrary(cspplot)\n\n# these options help Stan run faster\noptions(mc.cores = parallel::detectCores())\n\n# use the CSP-theme for plotting\ntheme_set(theme_csp())\n\n# global color scheme from CSP\nproject_colors = cspplot::list_colors() |&gt; pull(hex)\n# names(project_colors) &lt;- cspplot::list_colors() |&gt; pull(name)\n\n# setting theme colors globally\nscale_colour_discrete &lt;- function(...) {\n  scale_colour_manual(..., values = project_colors)\n}\nscale_fill_discrete &lt;- function(...) {\n   scale_fill_manual(..., values = project_colors)\n}\n\n\n\n\nMulti-membership group-level effects\nTo understand multi-membership models, consider first a normal (non-multi-membership) group-level modeling approach. Say that each observation \\(i\\) belongs to exactly one of \\(k\\) categories \\(c(i) \\in \\{1, \\dots, k\\}\\). A group-level intercept consists of a vector of additive offsets \\(\\vec{u} = \\langle u_{1}, \\dots, u_{k} \\rangle\\), one adjustment \\(u_{i}\\) for each category \\(i\\). The linear predictor for observation \\(i\\) is computed by adding the appropriate group-level offset to the population-level intercept, like so:\n\\[\n\\eta_i = \\beta_0 + u_{c(i)} + \\dots\n\\]\nInstead of an indexing function \\(c(i)\\), which returns the category index for each observation \\(i\\), this can also be written using a one-hot vector encoding. Let, \\(\\vec{\\delta}_i\\) be a vector of length \\(k\\) which contains only zeros, except in position \\(c(i)\\), where it contains a 1. Then we can equivalently write the equation for the linear predictor term as:\n\\[\n\\eta_i = \\beta_0 + \\vec{u} \\cdot \\vec{\\delta}_i + \\dots\n\\]\nFor example, if there are only four categories, and if observation \\(i\\) belongs to the third category (\\(c(i)=3\\)), we have \\(\\delta_i = \\langle 0,0,1,0 \\rangle\\), so that the dot product \\(\\vec{u} \\cdot \\vec{\\delta}_i\\) will just return the value of \\(\\vec{u}\\) at position 3.\nFrom here, it’s just one step further to a multi-membership model. Instead of a one-hot vector encoding for group-membership, consider a vector of (normalized) weights \\(\\vec{w}_i\\) given the relative degree to which observation \\(i\\) belongs to each category. The linear predictor is then:\n\\[\n\\eta_i = \\beta_0 + \\vec{u} \\cdot \\vec{w}_i + \\dots\n\\]\nand that’s all to it. Similar considerations apply to random slopes.\nImportantly, the weights \\(\\vec{w}_i\\) are given (from observation). (I am not aware that brms allows to estimate the weights as well. If that would be a requirement, directly coding the model in Stan might be necessary.)\n\n\nSimulated data set of multi-membership effects\nTo explore how a multi-membership model can recover the true effects of weighted membership, we use a simulated data set based on a vanilla linear regression (with one predictor term). The true population-level parameters are:\n\n\nToggle code\n# true parameters (population-level)\npop_Intercept = 3\npop_Slope     = 2\npop_SD        = 1\n\n\nFor group-level effects, assume that there are five groups. Each has a true additive offset (random intercept) as follows:\n\n\nToggle code\n# true parameters (group-level):\n# - there are five groups\n# - random intercepts for each group\n#   -&gt; sequence -0.8 -0.4  0.0  0.4  0.8\ngroup_Intercepts = seq(from = -0.8, to = 0.8, length.out = 5)\n\n\nHere are samples from a model, in which each individual observation (each row) can belong to variable degree to one of two groups.\n\n\nToggle code\nn_samples = 2000\ndata_multiMember &lt;- \n  tibble(\n    x     = rnorm(n_samples), \n    g1    = sample(1:5, n_samples, TRUE), \n    g2    = sample(1:5, n_samples, TRUE),\n    y_pop = rnorm(n_samples, mean = pop_Slope * x + pop_Intercept),\n    w1    = rbeta(n_samples, 1, 1),\n    w2    = 1 - w1,\n    RE    = group_Intercepts[g1] * w1 + group_Intercepts[g2] * w2,\n    y     = y_pop + RE\n  )\n\n\nThis plot shows the simulated data, where the points in blue show the actual observations, and the points in yellow show the data before applying the group-level effects.\n\n\nToggle code\n# unperturbed data in yellow, observed data in blue \ndata_multiMember |&gt; \n  ggplot(aes(x,y)) + \n  geom_point(aes(x = x ,y = y_pop), color = project_colors[3], alpha = 0.5) +\n  geom_point(color = project_colors[1], alpha = 0.5)\n\n\n\n\n\nTo run a multi-membership model in brms, there is special syntax for the group-level effects. Writing + (1 | mm(g1, g2)) indicates random intercepts for multi-membership as indicated by the vectors g1 and g2. Without further information, this is interpreted as belonging equally to each group (weights 0.5 for each, if we have two possible group memberships). If membership is weighted, as indicated in our data set in columns w1 and w2, this can be expressed as: + ( 1 | mm(g1, g2, weights = cbind(w1, w2)). (NB: the weights argument expects a matrix, so that we need to include cbind() here.)\n\n\nToggle code\n# multi-membership model with two members per group and equal weights\nfit_mm &lt;- \n  brms::brm(\n    formula = y ~ x + ( 1 | mm(g1, g2, weights = cbind(w1, w2))), \n    data    = data_multiMember,\n    control = list(adapt_delta = 0.99)\n    )\n\n\nDid the model recover the true group-level intercepts? It did, check it:\n\n\nToggle code\ntidybayes::summarise_draws(fit_mm) |&gt; \n  filter(grepl(\"r_\",variable)) |&gt; \n  select(variable, q5, mean, q95)\n\n\n# A tibble: 5 × 4\n  variable                   q5    mean    q95\n  &lt;chr&gt;                   &lt;num&gt;   &lt;num&gt;  &lt;num&gt;\n1 r_mmg1g2[1,Intercept] -1.63   -0.863  -0.143\n2 r_mmg1g2[2,Intercept] -1.14   -0.378   0.352\n3 r_mmg1g2[3,Intercept] -0.836  -0.0612  0.678\n4 r_mmg1g2[4,Intercept] -0.301   0.459   1.20 \n5 r_mmg1g2[5,Intercept]  0.0557  0.815   1.53 \n\n\n\n\nExercises\n\n\n\n\n\n\nExercise 1a: Biased group membership\n\n\n\n\n\nConstruct a new data set (using the previous code), in which there is a (mild) bias in group membership. Concretely, add bias to the sampling of variables g1 and g2, making it more likely for g1 to be a lower rather than higher indexed group, and reversely for g2. Also, add a bias in the weights towards g1 by changing the shape parameters of the Beta distribution. This should induce an overall bias towards random intercepts that lower the linear predictor.\nPlot the new data using the previous plotting code.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nn_samples = 2000\ndata_multiMember &lt;- \n  tibble(\n    x     = rnorm(n_samples), \n    g1    = sample(1:5, n_samples, TRUE, prob = 5:1), \n    g2    = sample(1:5, n_samples, TRUE, prob = 1:5),\n    y_pop = rnorm(n_samples, mean = pop_Slope * x + pop_Intercept),\n    w1    = rbeta(n_samples, 4, 1),\n    w2    = 1 - w1,\n    RE    = group_Intercepts[g1] * w1 + group_Intercepts[g2] * w2,\n    y     = y_pop + RE\n  )\n\n\n\n\nToggle code\n# unperturbed data in yellow, observed data in blue \ndata_multiMember |&gt; \n  ggplot(aes(x,y)) + \n  geom_point(aes(x = x ,y = y_pop), color = project_colors[3], alpha = 0.5) +\n  geom_point(color = project_colors[1], alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1b: Check parameter recovery\n\n\n\n\n\nFit a multi-membership model to the new data and check if group-level parameters are recoverable still. How do you interpret the results: did the model recover the true parameters or not?\n[If you want: Think about what influences recoverability.]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\n# multi-membership model with two members per group and equal weights\nfit_mm &lt;- \n  brms::brm(\n    formula = y ~ x + ( 1 | mm(g1, g2, weights = cbind(w1, w2)) ), \n    data    = data_multiMember,\n    control = list(adapt_delta = 0.99)\n    )\ntidybayes::summarise_draws(fit_mm) |&gt; \n  filter(grepl(\"r_\",variable)) |&gt; \n  select(variable, q5, mean, q95)\n\n\n# A tibble: 5 × 4\n  variable                   q5    mean    q95\n  &lt;chr&gt;                   &lt;num&gt;   &lt;num&gt;  &lt;num&gt;\n1 r_mmg1g2[1,Intercept] -1.47   -0.785  -0.123\n2 r_mmg1g2[2,Intercept] -1.11   -0.436   0.228\n3 r_mmg1g2[3,Intercept] -0.754  -0.0790  0.591\n4 r_mmg1g2[4,Intercept] -0.187   0.491   1.14 \n5 r_mmg1g2[5,Intercept]  0.0763  0.769   1.43 \n\n\nThe recovery is not as immaculate as without the bias, but it is still okay.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1c: Compare multi-membership model to vanilla\n\n\n\n\n\nRun a vanilla regression model for the biased data, i.e., without group-level effects. Compare the models based on a simple posterior predictive check with pp_check(). What do you conclude from this?\nCompare the models based on their posterior predicitive adequacy, e.g., using LOO model comparison (if you can; maybe you have not learned about this yet). Interpret the results.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nToggle code\nfit_vanilla &lt;- \n  brms::brm(\n    formula = y ~ x, \n    data    = data_multiMember\n  )\n\npp_check(fit_mm)\n\n\n\n\n\nToggle code\npp_check(fit_vanilla)\n\n\n\n\n\nToggle code\nloo_compare(loo(fit_mm), loo(fit_vanilla))\n\n\n            elpd_diff se_diff\nfit_mm         0.0       0.0 \nfit_vanilla -168.2      17.2 \n\n\nThe visual PPC does not distinguish between these models (because it is sensitive only to the overal distribution of the response variable).\nThe LOO-based model comparison is more informative: The vanilla model appears to be substantially worse.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\nCan you think of an example from your own line of work where multi-membership group-level effects might be plausible, possibly even mandatory?"
  },
  {
    "objectID": "03-hierarchical-models.html",
    "href": "03-hierarchical-models.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This unit introduces hierarchical regression models (also known as mixed models, or random-effect models).\n\nThere are also tutorials covering multi-membership group-level effects and an alternative motivation for using multi-level modeling which is not “breaking the wrong independence assumptions”."
  },
  {
    "objectID": "06-model-comparison.html",
    "href": "06-model-comparison.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "The three main things we can do with models are:\n\ninferring (credible) parameter values (on the assumption that the model we use is good (enough))\nmaking predictions (from an ex ante (a priori) or ex post (a posteriori)) point of view\n\npredictions can also be used for model checking (also known as model criticism)\n\ncomparing which of several models is better (in some sense of “better”)\n\nWe have so far looked only at the first two “pillars of Bayesian data analysis”. In this unit we look at the third, model comparison.\nModel comparison is deeply related to the second point: making predictions. We compare models based on their ability to predict data well enough. But not exclusively! We might also rely on other aspects, such as whether a model makes fewer spurious assumptions, i.e., is simpler, more economical or more parsimonious.\nAs usual, there is not one criterion for model comparison that everybody unanimously agrees to as the best. As usual, this is likely because “goodness of a model” is a multi-dimensional concept. What counts as a good model for science (knowledge gain; theoretical understanding) need not be the same as for engineering or application (getting high-quality predictions in the most efficient manner).\nThis unit therefore centers on two important tools for Bayesian model comparison that lie at opposite ends of a continuous spectrum, namely Bayes factors and leave-one-out (LOO) cross validation. Bayes factors take the most extreme point of view of ex ante predictions: we compare models without any updating on the relevant data. LOO-CV, on the other hand, take the (almost) most extreme point of view of ex post predictions, comparing models that are trained on all of the relevant data, except one single data observations.\nIn the practical session, we explore how these different perspectives can give rise to opposite results. This is not a puzzle or paradox, and maybe not even something to quarrel about. It is a natural reflex of comparing the same objects based on a different task, namely making predictions before training, or after."
  },
  {
    "objectID": "01-basics.html",
    "href": "01-basics.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This chapter introduces the basics of Bayesian linear regression modeling.\n\n\nIt contains tutorials covering:\n\nsimple linear regression models (with WebPPL),\nsimple linear regression models (with brms),\nhow to inspect and set priors and how to obtain prior and predictive samples with brms."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This site provides material for an intermediate level course on Bayesian linear regression modeling. The course presupposes some prior exposure to statistics and some acquaintance with R."
  },
  {
    "objectID": "index.html#intended-audience",
    "href": "index.html#intended-audience",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Intended audience",
    "text": "Intended audience\nThis course is designed for people who have completed a first, introductory course on data analysis, which has conveyed roughly the following:\n\nbasic knowledge of R and, ideally, the tidyverse\nbasic familiarity with Bayesian reasoning (prior, likelihood, posterior)\nsome prior exposure to regression modeling (Bayesian or otherwise)"
  },
  {
    "objectID": "index.html#scope",
    "href": "index.html#scope",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Scope",
    "text": "Scope\nThe aim of this course is to increase students’ overview over topics relevant for intermediate to advanced Bayesian regression modeling. The course focuses on Bayesian multi-level generalized linear models as implemented in the brms package. It covers, among other things, the following theoretical and practical aspects:\n\nprior and posterior model checking\nMCMC methods (HMC diagnostics)\ngeneralized linear models (ordered logit, multinomial, Poisson, Beta …)\ndistributional and non-linear models (GAMs, Gaussian processes)\nmodel comparison (Bayes factors, cross-validation)"
  },
  {
    "objectID": "index.html#additional-material",
    "href": "index.html#additional-material",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Additional material",
    "text": "Additional material\nThis course contains a short tutorial on Bayesian modeling, and a brief tutorial on wrangling and plotting in the tidyverse.\nFor more background, a companion to this course, is the introductory webbook“An introduction to Data Analysis”, which covers R, tidyverse, Bayesian and, eventually, also frequentist statistics. This course presupposes roughly the content covered in Chapters 2–9 and 12–13.\nThere is also a cheat sheet on BRMS for reference."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nPart of the hands-on material (wrangling, plotting, simple regression modeling) was used in a previous course, co-taught with Timo Roettger. My gratitude for his permission to build on it here. The tutorial on contrast coding was first authored by Polina Tsvilodub. The initial exercises for building Bayesian intuitions using WebPPL were inspired by and elaborate on material first created by Michael Henry Tessler in this webbook."
  },
  {
    "objectID": "05-MCMC.html",
    "href": "05-MCMC.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "The success of Bayesian statistics is in large part the fruit of very clever algorithms and efficient implementations for drawing samples from complex, high-dimensional posterior distributions. This unit covers:\n\nMarkov Chain Monte Carlo methods, in particular:\n\nsimple Metropolis-Hastings and\nHamiltonian Monte Carlo\n\ncommon notions and diagnostics for assessing the quality of MCMC samples, such as:\n\n\\(\\hat{R}\\)\nautocorrelation\neffective sample size\ntraceplots\ndivergent transitions\n\ncontrol parameters for brms model fits\n\nWe also take a peak at the Stan programming language."
  },
  {
    "objectID": "02-categorical-predictors.html",
    "href": "02-categorical-predictors.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This chapter covers treatment of categorical predictors in linear regression, using contrast coding. We learn how to set contrasts, interpret results and use samples to retrieve information on contrasts that we did not encode in the contrast scheme.\n\n\nThere is a theoretical tutorial on the topic, as well as an tutorial with practical exercises.\nThere is also a sheet with additional exercises (simple linear regression with metric and categorical variables)."
  },
  {
    "objectID": "09-causal-inference.html",
    "href": "09-causal-inference.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This unit introduces the basics of causal inference, following the approach of Judea Pearl. The material presented here draws heavily on this excellent primer. What we add to this is the Bayesian-regression perspective. We show how to quantify uncertainty of estimates of causal effects, using Bayesian regression modeling.\nThere is a short exercise script on stochastic dependence and causal relation, and a tutorial script on causal inference."
  },
  {
    "objectID": "04-GLM.html",
    "href": "04-GLM.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This unit introduces generalized linear models, mixture models and distributional models."
  },
  {
    "objectID": "00-background.html",
    "href": "00-background.html",
    "title": "Bayesian Regression: Theory & Practice",
    "section": "",
    "text": "This section provides a quick recap of fundamental concepts of Bayesian Data Analysis and the technical work flow this course uses.\nIn the first part, we explore how to think like a computational Bayesian, which is to think in terms of data-generating models and sampling processes. To get more familiar with the mode of being, there are examples using the probabilistic programming language WebPPL.\nThe second part introduces basics of data wrangling (in the tidyverse) and plotting (with ggplot2)."
  }
]